{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Search-Augmented ACE: PUCT Q-Estimator Ablation\n\n**Hypothesis**: Tree search (PUCT) over ACE-style playbooks can outperform greedy sequential evolution, but the Q estimator determines whether the tree actually explores or degenerates into a chain.\n\n**Four conditions** (same total LLM budget):\n1. **Greedy ACE** \u2014 sequential generate \u2192 reflect \u2192 curate (baseline)\n2. **PUCT-Mean** \u2014 Q = mean reward. Standard. Biased by early results.\n3. **PUCT-EMA** \u2014 Q = exponential moving average (\u03b1=0.4). Reacts to recent performance.\n4. **PUCT-Bayesian** \u2014 Q = Beta posterior mean. Shrinks toward 0.5 with low data, most exploratory.\n\n**Key fix**: Progressive widening \u2014 only expand a node after it accumulates `\u2265 ceil(visits^0.5)` children. This forces branching instead of chaining.\n\n**Setup**: Qwen2.5-7B-Instruct via vLLM on A100, 50 GSM8K problems."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install vllm openai datasets matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import signal\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch vLLM server in background\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "VLLM_PORT = 8000\n",
    "\n",
    "vllm_proc = subprocess.Popen(\n",
    "    [\n",
    "        \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
    "        \"--model\", MODEL_NAME,\n",
    "        \"--port\", str(VLLM_PORT),\n",
    "        \"--max-model-len\", \"4096\",\n",
    "        \"--gpu-memory-utilization\", \"0.85\",\n",
    "        \"--dtype\", \"auto\",\n",
    "    ],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    ")\n",
    "print(f\"vLLM server PID: {vllm_proc.pid}\")\n",
    "\n",
    "# Wait for server to be ready\n",
    "client = OpenAI(base_url=f\"http://localhost:{VLLM_PORT}/v1\", api_key=\"dummy\")\n",
    "for attempt in range(60):\n",
    "    try:\n",
    "        client.models.list()\n",
    "        print(f\"vLLM ready after {attempt + 1}s\")\n",
    "        break\n",
    "    except Exception:\n",
    "        time.sleep(1)\n",
    "else:\n",
    "    raise RuntimeError(\"vLLM server did not start within 60s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GSM8K Data Loading & Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset\n\nds = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n\ndef extract_gsm8k_answer(answer_text: str) -> str:\n    \"\"\"Extract numeric answer from GSM8K '#### <number>' format.\"\"\"\n    match = re.search(r\"####\\s*(-?[\\d,]+\\.?\\d*)\", answer_text)\n    if match:\n        return match.group(1).replace(\",\", \"\").strip()\n    # Fallback: last number in text\n    nums = re.findall(r\"-?[\\d,]+\\.?\\d*\", answer_text)\n    if nums:\n        return nums[-1].replace(\",\", \"\")\n    return \"\"\n\nproblems = []\nfor item in ds:\n    problems.append({\n        \"question\": item[\"question\"],\n        \"answer\": extract_gsm8k_answer(item[\"answer\"]),\n        \"full_answer\": item[\"answer\"],\n    })\n\n# Use first 50 problems, shuffled deterministically\nrng = random.Random(SEED)\nrng.shuffle(problems)\nproblems = problems[:50]\nprint(f\"Loaded {len(problems)} GSM8K problems\")\nprint(f\"Example: Q='{problems[0]['question'][:80]}...' A={problems[0]['answer']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core Components: Playbook, Generator, Reflector, Curator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Playbook representation ---\n",
    "\n",
    "@dataclass\n",
    "class Bullet:\n",
    "    id: str\n",
    "    section: str  # STRATEGIES, COMMON_MISTAKES, SOLUTION_PATTERNS\n",
    "    content: str\n",
    "    helpful: int = 0\n",
    "    harmful: int = 0\n",
    "\n",
    "    def to_str(self) -> str:\n",
    "        return f\"[{self.id}] helpful={self.helpful} harmful={self.harmful} :: {self.content}\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Playbook:\n",
    "    bullets: List[Bullet] = field(default_factory=list)\n",
    "    _next_id: int = 1\n",
    "\n",
    "    def add(self, section: str, content: str) -> str:\n",
    "        prefix = {\"STRATEGIES\": \"str\", \"COMMON_MISTAKES\": \"err\", \"SOLUTION_PATTERNS\": \"sol\"}.get(section, \"gen\")\n",
    "        bid = f\"{prefix}-{self._next_id:05d}\"\n",
    "        self._next_id += 1\n",
    "        self.bullets.append(Bullet(id=bid, section=section, content=content))\n",
    "        return bid\n",
    "\n",
    "    def remove(self, bid: str):\n",
    "        self.bullets = [b for b in self.bullets if b.id != bid]\n",
    "\n",
    "    def update(self, bid: str, content: str):\n",
    "        for b in self.bullets:\n",
    "            if b.id == bid:\n",
    "                b.content = content\n",
    "                return\n",
    "\n",
    "    def tag(self, bid: str, label: str):\n",
    "        for b in self.bullets:\n",
    "            if b.id == bid:\n",
    "                if label == \"helpful\":\n",
    "                    b.helpful += 1\n",
    "                elif label == \"harmful\":\n",
    "                    b.harmful += 1\n",
    "\n",
    "    def to_str(self) -> str:\n",
    "        sections = defaultdict(list)\n",
    "        for b in self.bullets:\n",
    "            sections[b.section].append(b.to_str())\n",
    "        parts = []\n",
    "        for sec in [\"STRATEGIES\", \"COMMON_MISTAKES\", \"SOLUTION_PATTERNS\"]:\n",
    "            if sections[sec]:\n",
    "                parts.append(f\"## {sec}\")\n",
    "                parts.extend(sections[sec])\n",
    "        return \"\\n\".join(parts) if parts else \"(empty playbook)\"\n",
    "\n",
    "    def copy(self) -> \"Playbook\":\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return len(self.bullets)\n",
    "\n",
    "\n",
    "def make_initial_playbook() -> Playbook:\n",
    "    pb = Playbook()\n",
    "    pb.add(\"STRATEGIES\", \"Break word problems into step-by-step arithmetic.\")\n",
    "    pb.add(\"STRATEGIES\", \"Identify what quantity the question asks for before computing.\")\n",
    "    pb.add(\"COMMON_MISTAKES\", \"Watch for unit conversions (hours to minutes, etc).\")\n",
    "    return pb\n",
    "\n",
    "print(make_initial_playbook().to_str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM call wrapper ---\n",
    "\n",
    "call_counter = defaultdict(int)  # track calls by role\n",
    "\n",
    "def llm_call(system: str, user: str, role: str = \"generate\", temperature: float = 0.7, max_tokens: int = 1024) -> str:\n",
    "    \"\"\"Single LLM call via vLLM OpenAI-compatible API.\"\"\"\n",
    "    call_counter[role] += 1\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": user},\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"LLM call failed ({role}): {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def reset_call_counter():\n",
    "    global call_counter\n",
    "    call_counter = defaultdict(int)\n",
    "\n",
    "def get_call_counts() -> Dict[str, int]:\n",
    "    return dict(call_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Generator ---\n\ndef generate(question: str, playbook: Playbook) -> Tuple[str, List[str], str]:\n    \"\"\"\n    Generate a solution to a math problem using the playbook.\n    Returns (extracted_answer, bullets_used, raw_response).\n    \"\"\"\n    pb_text = playbook.to_str()\n    system = (\n        \"You are a math problem solver. Use the playbook strategies below to help solve the problem.\\n\"\n        \"When you use a specific strategy, mention its ID (e.g., [str-00001]).\\n\"\n        \"Show your work step-by-step, then give the final numeric answer on its own line as: #### <number>\\n\\n\"\n        f\"PLAYBOOK:\\n{pb_text}\"\n    )\n    user = f\"Solve this problem:\\n{question}\"\n    raw = llm_call(system, user, role=\"generate\")\n\n    # Extract answer\n    answer = \"\"\n    m = re.search(r\"####\\s*(-?[\\d,]+\\.?\\d*)\", raw)\n    if m:\n        answer = m.group(1).replace(\",\", \"\").strip()\n    else:\n        nums = re.findall(r\"-?[\\d,]+\\.?\\d*\", raw)\n        if nums:\n            answer = nums[-1].replace(\",\", \"\")\n\n    # Extract bullet references\n    bullets_used = re.findall(r\"\\[(\\w+-\\d+)\\]\", raw)\n\n    return answer, bullets_used, raw"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Answer comparison ---\n",
    "\n",
    "def answers_match(predicted: str, ground_truth: str) -> bool:\n",
    "    \"\"\"Compare numeric answers with tolerance.\"\"\"\n",
    "    try:\n",
    "        p = float(predicted.replace(\",\", \"\"))\n",
    "        g = float(ground_truth.replace(\",\", \"\"))\n",
    "        return abs(p - g) < 1e-3\n",
    "    except (ValueError, TypeError):\n",
    "        return predicted.strip() == ground_truth.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reflector ---\n",
    "\n",
    "def reflect(question: str, raw_response: str, predicted: str, ground_truth: str,\n",
    "            bullets_used: List[str], playbook: Playbook) -> Tuple[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Reflect on the solution attempt.\n",
    "    Returns (reflection_text, bullet_tags) where bullet_tags maps bullet_id -> 'helpful'|'harmful'|'neutral'.\n",
    "    \"\"\"\n",
    "    correct = answers_match(predicted, ground_truth)\n",
    "    feedback = \"CORRECT\" if correct else f\"INCORRECT (predicted {predicted}, expected {ground_truth})\"\n",
    "\n",
    "    bullets_text = \"\"\n",
    "    for b in playbook.bullets:\n",
    "        if b.id in bullets_used:\n",
    "            bullets_text += f\"  {b.to_str()}\\n\"\n",
    "\n",
    "    system = (\n",
    "        \"You are a math reasoning analyst. Analyze whether the solution approach was correct \"\n",
    "        \"and whether the playbook strategies used were helpful or harmful.\\n\"\n",
    "        \"For each bullet ID used, output a JSON line: {\\\"id\\\": \\\"str-00001\\\", \\\"tag\\\": \\\"helpful\\\"}\\n\"\n",
    "        \"Tags must be one of: helpful, harmful, neutral.\\n\"\n",
    "        \"End with a brief reflection paragraph.\"\n",
    "    )\n",
    "    user = (\n",
    "        f\"Problem: {question}\\n\\n\"\n",
    "        f\"Solution attempt:\\n{raw_response}\\n\\n\"\n",
    "        f\"Result: {feedback}\\n\\n\"\n",
    "        f\"Playbook bullets used:\\n{bullets_text}\"\n",
    "    )\n",
    "    raw = llm_call(system, user, role=\"reflect\", temperature=0.3)\n",
    "\n",
    "    # Parse bullet tags\n",
    "    tags = {}\n",
    "    for m in re.finditer(r'\"id\"\\s*:\\s*\"([^\"]+)\".*?\"tag\"\\s*:\\s*\"(helpful|harmful|neutral)\"', raw):\n",
    "        tags[m.group(1)] = m.group(2)\n",
    "\n",
    "    # If no tags parsed but we know correctness, apply heuristic\n",
    "    if not tags:\n",
    "        for bid in bullets_used:\n",
    "            tags[bid] = \"helpful\" if correct else \"neutral\"\n",
    "\n",
    "    return raw, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Curator ---\n",
    "\n",
    "MAX_BULLETS = 20\n",
    "\n",
    "def curate(playbook: Playbook, reflection: str, question: str) -> Playbook:\n",
    "    \"\"\"\n",
    "    Curate the playbook based on reflection.\n",
    "    Returns a new (copied) playbook with operations applied.\n",
    "    \"\"\"\n",
    "    pb = playbook.copy()\n",
    "    pb_text = pb.to_str()\n",
    "\n",
    "    system = (\n",
    "        \"You are a playbook curator for math problem solving. Based on the reflection, \"\n",
    "        \"propose operations to improve the playbook.\\n\"\n",
    "        \"Output a JSON array of operations:\\n\"\n",
    "        '[{\"op\": \"ADD\", \"section\": \"STRATEGIES\", \"content\": \"new insight\"},\\n'\n",
    "        ' {\"op\": \"UPDATE\", \"id\": \"str-00001\", \"content\": \"refined text\"},\\n'\n",
    "        ' {\"op\": \"DELETE\", \"id\": \"err-00002\"}]\\n'\n",
    "        f\"Sections: STRATEGIES, COMMON_MISTAKES, SOLUTION_PATTERNS\\n\"\n",
    "        f\"Max bullets: {MAX_BULLETS}. Current: {pb.size}.\\n\"\n",
    "        \"Only propose operations that are clearly supported by the reflection. Keep it minimal.\"\n",
    "    )\n",
    "    user = (\n",
    "        f\"Current playbook:\\n{pb_text}\\n\\n\"\n",
    "        f\"Problem context: {question[:200]}\\n\\n\"\n",
    "        f\"Reflection:\\n{reflection}\"\n",
    "    )\n",
    "    raw = llm_call(system, user, role=\"curate\", temperature=0.3)\n",
    "\n",
    "    # Parse operations from JSON array\n",
    "    ops = []\n",
    "    # Try to find JSON array in response\n",
    "    json_match = re.search(r'\\[\\s*\\{.*?\\}\\s*\\]', raw, re.DOTALL)\n",
    "    if json_match:\n",
    "        try:\n",
    "            ops = json.loads(json_match.group())\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "    # Apply operations\n",
    "    for op in ops:\n",
    "        try:\n",
    "            if op.get(\"op\") == \"ADD\" and pb.size < MAX_BULLETS:\n",
    "                pb.add(op.get(\"section\", \"STRATEGIES\"), op.get(\"content\", \"\"))\n",
    "            elif op.get(\"op\") == \"UPDATE\" and op.get(\"id\"):\n",
    "                pb.update(op[\"id\"], op.get(\"content\", \"\"))\n",
    "            elif op.get(\"op\") == \"DELETE\" and op.get(\"id\"):\n",
    "                pb.remove(op[\"id\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Safety: if curator emptied the playbook, reset\n",
    "    if pb.size == 0:\n",
    "        pb = make_initial_playbook()\n",
    "        pb._next_id = playbook._next_id\n",
    "\n",
    "    return pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Search Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Shared tracking ---\n",
    "\n",
    "@dataclass\n",
    "class RunLog:\n",
    "    \"\"\"Tracks per-problem results for a single condition.\"\"\"\n",
    "    correct: List[bool] = field(default_factory=list)\n",
    "    playbook_sizes: List[int] = field(default_factory=list)\n",
    "    call_counts: Dict[str, int] = field(default_factory=dict)\n",
    "    final_playbook: Optional[Playbook] = None\n",
    "\n",
    "    @property\n",
    "    def running_accuracy(self) -> List[float]:\n",
    "        acc = []\n",
    "        total = 0\n",
    "        for i, c in enumerate(self.correct):\n",
    "            total += int(c)\n",
    "            acc.append(total / (i + 1))\n",
    "        return acc\n",
    "\n",
    "    @property\n",
    "    def final_accuracy(self) -> float:\n",
    "        if not self.correct:\n",
    "            return 0.0\n",
    "        # Accuracy on last 20 problems\n",
    "        tail = self.correct[-20:]\n",
    "        return sum(tail) / len(tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Strategy 1: Greedy ACE ---\n",
    "\n",
    "CURATE_EVERY = 5\n",
    "\n",
    "def run_greedy(problems: List[dict]) -> RunLog:\n",
    "    \"\"\"Sequential generate \u2192 reflect \u2192 curate loop.\"\"\"\n",
    "    reset_call_counter()\n",
    "    log = RunLog()\n",
    "    pb = make_initial_playbook()\n",
    "    reflections_buffer = []\n",
    "\n",
    "    for i, prob in enumerate(problems):\n",
    "        answer, bullets_used, raw = generate(prob[\"question\"], pb)\n",
    "        correct = answers_match(answer, prob[\"answer\"])\n",
    "        log.correct.append(correct)\n",
    "        log.playbook_sizes.append(pb.size)\n",
    "\n",
    "        # Reflect\n",
    "        reflection, tags = reflect(\n",
    "            prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, pb\n",
    "        )\n",
    "        for bid, label in tags.items():\n",
    "            pb.tag(bid, label)\n",
    "        reflections_buffer.append((reflection, prob[\"question\"]))\n",
    "\n",
    "        # Curate periodically\n",
    "        if (i + 1) % CURATE_EVERY == 0 and reflections_buffer:\n",
    "            # Use most recent reflection\n",
    "            ref_text, ref_q = reflections_buffer[-1]\n",
    "            pb = curate(pb, ref_text, ref_q)\n",
    "            reflections_buffer = []\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            acc = sum(log.correct) / len(log.correct)\n",
    "            print(f\"  Greedy [{i+1}/{len(problems)}] acc={acc:.2%} bullets={pb.size}\")\n",
    "\n",
    "    log.call_counts = get_call_counts()\n",
    "    log.final_playbook = pb\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Strategy 2: PUCT-ACE with Q-estimator variants ---\n\n@dataclass\nclass MCTSNode:\n    playbook: Playbook\n    parent: Optional[\"MCTSNode\"] = None\n    children: List[\"MCTSNode\"] = field(default_factory=list)\n    visits: int = 0\n    reward_history: List[float] = field(default_factory=list)\n    results: List[bool] = field(default_factory=list)\n\n    def q_value(self, mode: str = \"mean\") -> float:\n        if not self.reward_history:\n            return 0.5  # optimistic prior for unvisited nodes\n        if mode == \"mean\":\n            return sum(self.reward_history) / len(self.reward_history)\n        elif mode == \"ema\":\n            # Exponential moving average \u2014 recent batches matter more\n            alpha = 0.4\n            q = self.reward_history[0]\n            for r in self.reward_history[1:]:\n                q = alpha * r + (1 - alpha) * q\n            return q\n        elif mode == \"bayesian\":\n            # Beta(successes+1, failures+1) posterior mean\n            # Shrinks toward 0.5 with few observations \u2014 maximally exploratory\n            s = sum(self.reward_history)\n            n = len(self.reward_history)\n            return (s + 1) / (n + 2)\n        return 0.5\n\n\ndef puct_select(node: MCTSNode, c_puct: float = 1.5, q_mode: str = \"mean\") -> MCTSNode:\n    \"\"\"Select best child via PUCT, recurse to leaf.\"\"\"\n    if not node.children:\n        return node\n    n_parent = node.visits\n    best, best_score = None, -float(\"inf\")\n    for child in node.children:\n        prior = 1.0 / len(node.children)\n        exploit = child.q_value(q_mode)\n        explore = c_puct * prior * math.sqrt(n_parent) / (1 + child.visits)\n        score = exploit + explore\n        if score > best_score:\n            best_score = score\n            best = child\n    return puct_select(best, c_puct, q_mode)\n\n\ndef backprop(node: MCTSNode, reward: float):\n    \"\"\"Backpropagate reward up the tree.\"\"\"\n    while node is not None:\n        node.visits += 1\n        node.reward_history.append(reward)\n        node = node.parent\n\n\ndef should_expand(node: MCTSNode, pw_alpha: float = 0.5) -> bool:\n    \"\"\"Progressive widening: expand only when node needs more children.\n    Branching factor grows as ceil(visits^alpha). With alpha=0.5:\n      visits=1 \u2192 1 child allowed\n      visits=4 \u2192 2 children\n      visits=9 \u2192 3 children\n    This forces the tree to RE-EVALUATE existing nodes before branching.\n    \"\"\"\n    max_children = max(1, math.ceil(node.visits ** pw_alpha))\n    return len(node.children) < max_children\n\n\ndef run_puct(problems: List[dict], batch_size: int = 3, c_puct: float = 1.5,\n             q_mode: str = \"mean\") -> RunLog:\n    \"\"\"PUCT tree search over playbook versions with progressive widening.\"\"\"\n    reset_call_counter()\n    log = RunLog()\n    root = MCTSNode(playbook=make_initial_playbook())\n    prob_idx = 0\n\n    while prob_idx < len(problems):\n        # Select leaf via PUCT\n        leaf = puct_select(root, c_puct, q_mode)\n\n        # Evaluate on a batch using a COPY (don't mutate tree node)\n        eval_pb = leaf.playbook.copy()\n        batch_end = min(prob_idx + batch_size, len(problems))\n        batch = problems[prob_idx:batch_end]\n        batch_correct = []\n        last_reflection = \"\"\n        last_question = \"\"\n\n        for prob in batch:\n            answer, bullets_used, raw = generate(prob[\"question\"], eval_pb)\n            correct = answers_match(answer, prob[\"answer\"])\n            batch_correct.append(correct)\n            log.correct.append(correct)\n            log.playbook_sizes.append(eval_pb.size)\n\n            reflection, tags = reflect(\n                prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, eval_pb\n            )\n            for bid, label in tags.items():\n                eval_pb.tag(bid, label)\n            last_reflection = reflection\n            last_question = prob[\"question\"]\n\n        prob_idx = batch_end\n        reward = sum(batch_correct) / len(batch_correct)\n        leaf.results.extend(batch_correct)\n\n        # Progressive widening: only expand if this node needs more children\n        if should_expand(leaf):\n            new_pb = curate(eval_pb, last_reflection, last_question)\n            child = MCTSNode(playbook=new_pb, parent=leaf)\n            leaf.children.append(child)\n            backprop(child, reward)\n        else:\n            # Just backprop to re-score existing node (no curate call)\n            backprop(leaf, reward)\n\n        if prob_idx % 10 <= batch_size:\n            acc = sum(log.correct) / len(log.correct)\n            depth = _tree_depth(root)\n            size = _tree_size(root)\n            branches = _branch_count(root)\n            print(f\"  PUCT-{q_mode} [{prob_idx}/{len(problems)}] acc={acc:.2%} \"\n                  f\"depth={depth} nodes={size} branches={branches}\")\n\n    log.call_counts = get_call_counts()\n    best_leaf = _best_leaf(root, q_mode)\n    log.final_playbook = best_leaf.playbook\n    return log\n\n\ndef _tree_depth(node: MCTSNode) -> int:\n    if not node.children:\n        return 0\n    return 1 + max(_tree_depth(c) for c in node.children)\n\n\ndef _tree_size(node: MCTSNode) -> int:\n    return 1 + sum(_tree_size(c) for c in node.children)\n\n\ndef _branch_count(node: MCTSNode) -> int:\n    \"\"\"Count nodes with >1 child (actual branching points).\"\"\"\n    count = 1 if len(node.children) > 1 else 0\n    return count + sum(_branch_count(c) for c in node.children)\n\n\ndef _best_leaf(node: MCTSNode, q_mode: str = \"mean\") -> MCTSNode:\n    if not node.children:\n        return node\n    best = max(node.children, key=lambda c: c.q_value(q_mode) if c.visits > 0 else -1)\n    return _best_leaf(best, q_mode)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"Running Greedy ACE (baseline)...\")\nprint(\"=\" * 60)\ngreedy_log = run_greedy(problems)\nprint(f\"Greedy done. Final acc (last 20): {greedy_log.final_accuracy:.2%}\")\nprint(f\"Calls: {greedy_log.call_counts}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "puct_logs = {}\nfor q_mode in [\"mean\", \"ema\", \"bayesian\"]:\n    print(\"=\" * 60)\n    print(f\"Running PUCT-{q_mode.upper()}...\")\n    print(\"=\" * 60)\n    log = run_puct(problems, q_mode=q_mode)\n    puct_logs[q_mode] = log\n    print(f\"PUCT-{q_mode} done. Final acc (last 20): {log.final_accuracy:.2%}\")\n    print(f\"Calls: {log.call_counts}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analysis & Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\"Greedy ACE\": greedy_log}\n",
    "for q_mode, log in puct_logs.items():\n",
    "    results[f\"PUCT-{q_mode.upper()}\"] = log\n",
    "\n",
    "COLORS = {\n",
    "    \"Greedy ACE\": \"#1f77b4\",\n",
    "    \"PUCT-MEAN\": \"#ff7f0e\",\n",
    "    \"PUCT-EMA\": \"#2ca02c\",\n",
    "    \"PUCT-BAYESIAN\": \"#d62728\",\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle(\"PUCT Q-Estimator Ablation on ACE Playbook Search (GSM8K)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# --- (a) Running accuracy curves ---\n",
    "ax = axes[0, 0]\n",
    "for name, log in results.items():\n",
    "    acc = log.running_accuracy\n",
    "    ax.plot(range(1, len(acc) + 1), acc, label=name, color=COLORS[name], linewidth=2)\n",
    "ax.set_xlabel(\"Problems Solved\")\n",
    "ax.set_ylabel(\"Running Accuracy\")\n",
    "ax.set_title(\"(a) Running Accuracy Over Time\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# --- (b) Final accuracy bar chart with bootstrap CI ---\n",
    "ax = axes[0, 1]\n",
    "\n",
    "def bootstrap_ci(data, n_boot=1000, ci=0.95):\n",
    "    data = np.array(data, dtype=float)\n",
    "    means = [np.mean(np.random.choice(data, size=len(data), replace=True)) for _ in range(n_boot)]\n",
    "    lo = np.percentile(means, (1 - ci) / 2 * 100)\n",
    "    hi = np.percentile(means, (1 + ci) / 2 * 100)\n",
    "    return np.mean(data), lo, hi\n",
    "\n",
    "names = list(results.keys())\n",
    "means, lows, highs = [], [], []\n",
    "for name in names:\n",
    "    tail = results[name].correct[-20:]\n",
    "    m, lo, hi = bootstrap_ci(tail)\n",
    "    means.append(m)\n",
    "    lows.append(m - lo)\n",
    "    highs.append(hi - m)\n",
    "\n",
    "bars = ax.bar(names, means, color=[COLORS[n] for n in names], yerr=[lows, highs], capsize=5)\n",
    "ax.set_ylabel(\"Accuracy (last 20 problems)\")\n",
    "ax.set_title(\"(b) Final Accuracy Comparison\")\n",
    "ax.set_ylim(0, 1)\n",
    "ax.tick_params(axis='x', rotation=15)\n",
    "for bar, m in zip(bars, means):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f\"{m:.1%}\",\n",
    "            ha=\"center\", fontsize=9)\n",
    "\n",
    "# --- (c) Playbook size over time ---\n",
    "ax = axes[1, 0]\n",
    "for name, log in results.items():\n",
    "    sizes = log.playbook_sizes\n",
    "    ax.plot(range(1, len(sizes) + 1), sizes, label=name, color=COLORS[name], linewidth=2)\n",
    "ax.set_xlabel(\"Problems Solved\")\n",
    "ax.set_ylabel(\"Playbook Bullets\")\n",
    "ax.set_title(\"(c) Playbook Size Over Time\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- (d) Cost breakdown ---\n",
    "ax = axes[1, 1]\n",
    "roles = [\"generate\", \"reflect\", \"curate\"]\n",
    "x = np.arange(len(names))\n",
    "width = 0.2\n",
    "for i, role in enumerate(roles):\n",
    "    counts = [results[n].call_counts.get(role, 0) for n in names]\n",
    "    ax.bar(x + i * width, counts, width, label=role.capitalize())\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(names, fontsize=8, rotation=15)\n",
    "ax.set_ylabel(\"LLM Calls\")\n",
    "ax.set_title(\"(d) LLM Call Breakdown\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"search_ace_results.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Plots saved to search_ace_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Statistical comparison ---\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Statistical Comparison (Bootstrap 95% CI on accuracy difference vs Greedy)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def bootstrap_diff_ci(a, b, n_boot=2000):\n",
    "    a, b = np.array(a, dtype=float), np.array(b, dtype=float)\n",
    "    diffs = []\n",
    "    for _ in range(n_boot):\n",
    "        ia = np.random.choice(len(a), len(a), replace=True)\n",
    "        ib = np.random.choice(len(b), len(b), replace=True)\n",
    "        diffs.append(np.mean(b[ib]) - np.mean(a[ia]))\n",
    "    lo, hi = np.percentile(diffs, [2.5, 97.5])\n",
    "    return np.mean(diffs), lo, hi\n",
    "\n",
    "greedy_tail = greedy_log.correct[-20:]\n",
    "\n",
    "for q_mode in [\"mean\", \"ema\", \"bayesian\"]:\n",
    "    name = f\"PUCT-{q_mode.upper()}\"\n",
    "    tail = puct_logs[q_mode].correct[-20:]\n",
    "    mean_diff, lo, hi = bootstrap_diff_ci(greedy_tail, tail)\n",
    "    sig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\n",
    "    print(f\"{name} vs Greedy: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")\n",
    "\n",
    "# Pairwise among PUCT variants\n",
    "print()\n",
    "for a_mode, b_mode in [(\"mean\", \"ema\"), (\"mean\", \"bayesian\"), (\"ema\", \"bayesian\")]:\n",
    "    a_tail = puct_logs[a_mode].correct[-20:]\n",
    "    b_tail = puct_logs[b_mode].correct[-20:]\n",
    "    mean_diff, lo, hi = bootstrap_diff_ci(a_tail, b_tail)\n",
    "    sig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\n",
    "    print(f\"PUCT-{b_mode.upper()} vs PUCT-{a_mode.upper()}: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Summary\n",
    "\n",
    "### Experiment\n",
    "- **Dataset**: GSM8K (50 problems, shuffled)\n",
    "- **Model**: Qwen2.5-7B-Instruct (local via vLLM)\n",
    "- **Conditions**: Greedy ACE, PUCT-Mean, PUCT-EMA, PUCT-Bayesian\n",
    "\n",
    "### What changed from v1\n",
    "- **Removed ES-ACE**: Too slow (4x budget even after round-robin fix). Population-based search needs efficiency improvements (MAP-Elites, crossover) before it's viable.\n",
    "- **Fixed PUCT chain degeneration**: Added progressive widening (`ceil(visits^0.5)` children). Nodes must be re-evaluated before branching, forcing actual tree structure.\n",
    "- **Q-estimator ablation**: The original PUCT used mean Q, which couldn't distinguish nodes when accuracy was uniformly high (~90%). Three variants test whether different Q estimators produce meaningfully different tree behavior.\n",
    "\n",
    "### Q-Estimator Properties\n",
    "| Estimator | Formula | Exploration Bias | When It Helps |\n",
    "|-----------|---------|-----------------|---------------|\n",
    "| **Mean** | `sum(r) / n` | Neutral | Stable environments, many visits |\n",
    "| **EMA** | `alpha * r_new + (1-alpha) * q_old` | Recency | Non-stationary playbooks (curator changes things) |\n",
    "| **Bayesian** | `(s+1)/(n+2)` | Toward 0.5 | Low data (shrinks extreme Q with few visits) |\n",
    "\n",
    "### Key Questions\n",
    "1. Does progressive widening produce actual branching (branches > 0)?\n",
    "2. Which Q estimator yields the best final accuracy?\n",
    "3. Does any PUCT variant beat Greedy, or does the tree overhead hurt at 50 problems?\n",
    "\n",
    "### Limitations\n",
    "- 50 problems is small; need 200+ to see exploration pay off\n",
    "- Uniform PUCT prior (no LLM-based prior yet)\n",
    "- Single seed; should repeat with multiple seeds\n",
    "- Progressive widening alpha=0.5 is a hyperparameter; not tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cleanup: kill vLLM server ---\n",
    "try:\n",
    "    os.kill(vllm_proc.pid, signal.SIGTERM)\n",
    "    print(f\"Killed vLLM server (PID {vllm_proc.pid})\")\n",
    "except ProcessLookupError:\n",
    "    print(\"vLLM server already stopped\")"
   ]
  }
 ]
}