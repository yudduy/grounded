{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search-Augmented ACE: MCTS/ES for Math Reasoning\n",
    "\n",
    "**Hypothesis**: Tree search (PUCT) and evolutionary strategies over ACE-style playbooks outperform greedy sequential playbook evolution on GSM8K math reasoning.\n",
    "\n",
    "**Three conditions** (same total LLM budget):\n",
    "1. **Greedy ACE** — sequential generate → reflect → curate\n",
    "2. **PUCT-ACE** — tree search over playbook versions\n",
    "3. **ES-ACE** — evolutionary strategy with population of playbooks\n",
    "\n",
    "**Setup**: Qwen2.5-7B-Instruct via vLLM on A100, 50 GSM8K problems, ~500 total LLM calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install vllm openai datasets matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import signal\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch vLLM server in background\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "VLLM_PORT = 8000\n",
    "\n",
    "vllm_proc = subprocess.Popen(\n",
    "    [\n",
    "        \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
    "        \"--model\", MODEL_NAME,\n",
    "        \"--port\", str(VLLM_PORT),\n",
    "        \"--max-model-len\", \"4096\",\n",
    "        \"--gpu-memory-utilization\", \"0.85\",\n",
    "        \"--dtype\", \"auto\",\n",
    "    ],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    ")\n",
    "print(f\"vLLM server PID: {vllm_proc.pid}\")\n",
    "\n",
    "# Wait for server to be ready\n",
    "client = OpenAI(base_url=f\"http://localhost:{VLLM_PORT}/v1\", api_key=\"dummy\")\n",
    "for attempt in range(60):\n",
    "    try:\n",
    "        client.models.list()\n",
    "        print(f\"vLLM ready after {attempt + 1}s\")\n",
    "        break\n",
    "    except Exception:\n",
    "        time.sleep(1)\n",
    "else:\n",
    "    raise RuntimeError(\"vLLM server did not start within 60s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GSM8K Data Loading & Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset\n\nds = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n\ndef extract_gsm8k_answer(answer_text: str) -> str:\n    \"\"\"Extract numeric answer from GSM8K '#### <number>' format.\"\"\"\n    match = re.search(r\"####\\s*(-?[\\d,]+\\.?\\d*)\", answer_text)\n    if match:\n        return match.group(1).replace(\",\", \"\").strip()\n    # Fallback: last number in text\n    nums = re.findall(r\"-?[\\d,]+\\.?\\d*\", answer_text)\n    if nums:\n        return nums[-1].replace(\",\", \"\")\n    return \"\"\n\nproblems = []\nfor item in ds:\n    problems.append({\n        \"question\": item[\"question\"],\n        \"answer\": extract_gsm8k_answer(item[\"answer\"]),\n        \"full_answer\": item[\"answer\"],\n    })\n\n# Use first 50 problems, shuffled deterministically\nrng = random.Random(SEED)\nrng.shuffle(problems)\nproblems = problems[:50]\nprint(f\"Loaded {len(problems)} GSM8K problems\")\nprint(f\"Example: Q='{problems[0]['question'][:80]}...' A={problems[0]['answer']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core Components: Playbook, Generator, Reflector, Curator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Playbook representation ---\n",
    "\n",
    "@dataclass\n",
    "class Bullet:\n",
    "    id: str\n",
    "    section: str  # STRATEGIES, COMMON_MISTAKES, SOLUTION_PATTERNS\n",
    "    content: str\n",
    "    helpful: int = 0\n",
    "    harmful: int = 0\n",
    "\n",
    "    def to_str(self) -> str:\n",
    "        return f\"[{self.id}] helpful={self.helpful} harmful={self.harmful} :: {self.content}\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Playbook:\n",
    "    bullets: List[Bullet] = field(default_factory=list)\n",
    "    _next_id: int = 1\n",
    "\n",
    "    def add(self, section: str, content: str) -> str:\n",
    "        prefix = {\"STRATEGIES\": \"str\", \"COMMON_MISTAKES\": \"err\", \"SOLUTION_PATTERNS\": \"sol\"}.get(section, \"gen\")\n",
    "        bid = f\"{prefix}-{self._next_id:05d}\"\n",
    "        self._next_id += 1\n",
    "        self.bullets.append(Bullet(id=bid, section=section, content=content))\n",
    "        return bid\n",
    "\n",
    "    def remove(self, bid: str):\n",
    "        self.bullets = [b for b in self.bullets if b.id != bid]\n",
    "\n",
    "    def update(self, bid: str, content: str):\n",
    "        for b in self.bullets:\n",
    "            if b.id == bid:\n",
    "                b.content = content\n",
    "                return\n",
    "\n",
    "    def tag(self, bid: str, label: str):\n",
    "        for b in self.bullets:\n",
    "            if b.id == bid:\n",
    "                if label == \"helpful\":\n",
    "                    b.helpful += 1\n",
    "                elif label == \"harmful\":\n",
    "                    b.harmful += 1\n",
    "\n",
    "    def to_str(self) -> str:\n",
    "        sections = defaultdict(list)\n",
    "        for b in self.bullets:\n",
    "            sections[b.section].append(b.to_str())\n",
    "        parts = []\n",
    "        for sec in [\"STRATEGIES\", \"COMMON_MISTAKES\", \"SOLUTION_PATTERNS\"]:\n",
    "            if sections[sec]:\n",
    "                parts.append(f\"## {sec}\")\n",
    "                parts.extend(sections[sec])\n",
    "        return \"\\n\".join(parts) if parts else \"(empty playbook)\"\n",
    "\n",
    "    def copy(self) -> \"Playbook\":\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return len(self.bullets)\n",
    "\n",
    "\n",
    "def make_initial_playbook() -> Playbook:\n",
    "    pb = Playbook()\n",
    "    pb.add(\"STRATEGIES\", \"Break word problems into step-by-step arithmetic.\")\n",
    "    pb.add(\"STRATEGIES\", \"Identify what quantity the question asks for before computing.\")\n",
    "    pb.add(\"COMMON_MISTAKES\", \"Watch for unit conversions (hours to minutes, etc).\")\n",
    "    return pb\n",
    "\n",
    "print(make_initial_playbook().to_str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM call wrapper ---\n",
    "\n",
    "call_counter = defaultdict(int)  # track calls by role\n",
    "\n",
    "def llm_call(system: str, user: str, role: str = \"generate\", temperature: float = 0.7, max_tokens: int = 1024) -> str:\n",
    "    \"\"\"Single LLM call via vLLM OpenAI-compatible API.\"\"\"\n",
    "    call_counter[role] += 1\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": user},\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"LLM call failed ({role}): {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def reset_call_counter():\n",
    "    global call_counter\n",
    "    call_counter = defaultdict(int)\n",
    "\n",
    "def get_call_counts() -> Dict[str, int]:\n",
    "    return dict(call_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Generator ---\n\ndef generate(question: str, playbook: Playbook) -> Tuple[str, List[str], str]:\n    \"\"\"\n    Generate a solution to a math problem using the playbook.\n    Returns (extracted_answer, bullets_used, raw_response).\n    \"\"\"\n    pb_text = playbook.to_str()\n    system = (\n        \"You are a math problem solver. Use the playbook strategies below to help solve the problem.\\n\"\n        \"When you use a specific strategy, mention its ID (e.g., [str-00001]).\\n\"\n        \"Show your work step-by-step, then give the final numeric answer on its own line as: #### <number>\\n\\n\"\n        f\"PLAYBOOK:\\n{pb_text}\"\n    )\n    user = f\"Solve this problem:\\n{question}\"\n    raw = llm_call(system, user, role=\"generate\")\n\n    # Extract answer\n    answer = \"\"\n    m = re.search(r\"####\\s*(-?[\\d,]+\\.?\\d*)\", raw)\n    if m:\n        answer = m.group(1).replace(\",\", \"\").strip()\n    else:\n        nums = re.findall(r\"-?[\\d,]+\\.?\\d*\", raw)\n        if nums:\n            answer = nums[-1].replace(\",\", \"\")\n\n    # Extract bullet references\n    bullets_used = re.findall(r\"\\[(\\w+-\\d+)\\]\", raw)\n\n    return answer, bullets_used, raw"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Answer comparison ---\n",
    "\n",
    "def answers_match(predicted: str, ground_truth: str) -> bool:\n",
    "    \"\"\"Compare numeric answers with tolerance.\"\"\"\n",
    "    try:\n",
    "        p = float(predicted.replace(\",\", \"\"))\n",
    "        g = float(ground_truth.replace(\",\", \"\"))\n",
    "        return abs(p - g) < 1e-3\n",
    "    except (ValueError, TypeError):\n",
    "        return predicted.strip() == ground_truth.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reflector ---\n",
    "\n",
    "def reflect(question: str, raw_response: str, predicted: str, ground_truth: str,\n",
    "            bullets_used: List[str], playbook: Playbook) -> Tuple[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Reflect on the solution attempt.\n",
    "    Returns (reflection_text, bullet_tags) where bullet_tags maps bullet_id -> 'helpful'|'harmful'|'neutral'.\n",
    "    \"\"\"\n",
    "    correct = answers_match(predicted, ground_truth)\n",
    "    feedback = \"CORRECT\" if correct else f\"INCORRECT (predicted {predicted}, expected {ground_truth})\"\n",
    "\n",
    "    bullets_text = \"\"\n",
    "    for b in playbook.bullets:\n",
    "        if b.id in bullets_used:\n",
    "            bullets_text += f\"  {b.to_str()}\\n\"\n",
    "\n",
    "    system = (\n",
    "        \"You are a math reasoning analyst. Analyze whether the solution approach was correct \"\n",
    "        \"and whether the playbook strategies used were helpful or harmful.\\n\"\n",
    "        \"For each bullet ID used, output a JSON line: {\\\"id\\\": \\\"str-00001\\\", \\\"tag\\\": \\\"helpful\\\"}\\n\"\n",
    "        \"Tags must be one of: helpful, harmful, neutral.\\n\"\n",
    "        \"End with a brief reflection paragraph.\"\n",
    "    )\n",
    "    user = (\n",
    "        f\"Problem: {question}\\n\\n\"\n",
    "        f\"Solution attempt:\\n{raw_response}\\n\\n\"\n",
    "        f\"Result: {feedback}\\n\\n\"\n",
    "        f\"Playbook bullets used:\\n{bullets_text}\"\n",
    "    )\n",
    "    raw = llm_call(system, user, role=\"reflect\", temperature=0.3)\n",
    "\n",
    "    # Parse bullet tags\n",
    "    tags = {}\n",
    "    for m in re.finditer(r'\"id\"\\s*:\\s*\"([^\"]+)\".*?\"tag\"\\s*:\\s*\"(helpful|harmful|neutral)\"', raw):\n",
    "        tags[m.group(1)] = m.group(2)\n",
    "\n",
    "    # If no tags parsed but we know correctness, apply heuristic\n",
    "    if not tags:\n",
    "        for bid in bullets_used:\n",
    "            tags[bid] = \"helpful\" if correct else \"neutral\"\n",
    "\n",
    "    return raw, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Curator ---\n",
    "\n",
    "MAX_BULLETS = 20\n",
    "\n",
    "def curate(playbook: Playbook, reflection: str, question: str) -> Playbook:\n",
    "    \"\"\"\n",
    "    Curate the playbook based on reflection.\n",
    "    Returns a new (copied) playbook with operations applied.\n",
    "    \"\"\"\n",
    "    pb = playbook.copy()\n",
    "    pb_text = pb.to_str()\n",
    "\n",
    "    system = (\n",
    "        \"You are a playbook curator for math problem solving. Based on the reflection, \"\n",
    "        \"propose operations to improve the playbook.\\n\"\n",
    "        \"Output a JSON array of operations:\\n\"\n",
    "        '[{\"op\": \"ADD\", \"section\": \"STRATEGIES\", \"content\": \"new insight\"},\\n'\n",
    "        ' {\"op\": \"UPDATE\", \"id\": \"str-00001\", \"content\": \"refined text\"},\\n'\n",
    "        ' {\"op\": \"DELETE\", \"id\": \"err-00002\"}]\\n'\n",
    "        f\"Sections: STRATEGIES, COMMON_MISTAKES, SOLUTION_PATTERNS\\n\"\n",
    "        f\"Max bullets: {MAX_BULLETS}. Current: {pb.size}.\\n\"\n",
    "        \"Only propose operations that are clearly supported by the reflection. Keep it minimal.\"\n",
    "    )\n",
    "    user = (\n",
    "        f\"Current playbook:\\n{pb_text}\\n\\n\"\n",
    "        f\"Problem context: {question[:200]}\\n\\n\"\n",
    "        f\"Reflection:\\n{reflection}\"\n",
    "    )\n",
    "    raw = llm_call(system, user, role=\"curate\", temperature=0.3)\n",
    "\n",
    "    # Parse operations from JSON array\n",
    "    ops = []\n",
    "    # Try to find JSON array in response\n",
    "    json_match = re.search(r'\\[\\s*\\{.*?\\}\\s*\\]', raw, re.DOTALL)\n",
    "    if json_match:\n",
    "        try:\n",
    "            ops = json.loads(json_match.group())\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "    # Apply operations\n",
    "    for op in ops:\n",
    "        try:\n",
    "            if op.get(\"op\") == \"ADD\" and pb.size < MAX_BULLETS:\n",
    "                pb.add(op.get(\"section\", \"STRATEGIES\"), op.get(\"content\", \"\"))\n",
    "            elif op.get(\"op\") == \"UPDATE\" and op.get(\"id\"):\n",
    "                pb.update(op[\"id\"], op.get(\"content\", \"\"))\n",
    "            elif op.get(\"op\") == \"DELETE\" and op.get(\"id\"):\n",
    "                pb.remove(op[\"id\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Safety: if curator emptied the playbook, reset\n",
    "    if pb.size == 0:\n",
    "        pb = make_initial_playbook()\n",
    "        pb._next_id = playbook._next_id\n",
    "\n",
    "    return pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Search Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Shared tracking ---\n",
    "\n",
    "@dataclass\n",
    "class RunLog:\n",
    "    \"\"\"Tracks per-problem results for a single condition.\"\"\"\n",
    "    correct: List[bool] = field(default_factory=list)\n",
    "    playbook_sizes: List[int] = field(default_factory=list)\n",
    "    call_counts: Dict[str, int] = field(default_factory=dict)\n",
    "    final_playbook: Optional[Playbook] = None\n",
    "\n",
    "    @property\n",
    "    def running_accuracy(self) -> List[float]:\n",
    "        acc = []\n",
    "        total = 0\n",
    "        for i, c in enumerate(self.correct):\n",
    "            total += int(c)\n",
    "            acc.append(total / (i + 1))\n",
    "        return acc\n",
    "\n",
    "    @property\n",
    "    def final_accuracy(self) -> float:\n",
    "        if not self.correct:\n",
    "            return 0.0\n",
    "        # Accuracy on last 20 problems\n",
    "        tail = self.correct[-20:]\n",
    "        return sum(tail) / len(tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Strategy 1: Greedy ACE ---\n",
    "\n",
    "CURATE_EVERY = 5\n",
    "\n",
    "def run_greedy(problems: List[dict]) -> RunLog:\n",
    "    \"\"\"Sequential generate → reflect → curate loop.\"\"\"\n",
    "    reset_call_counter()\n",
    "    log = RunLog()\n",
    "    pb = make_initial_playbook()\n",
    "    reflections_buffer = []\n",
    "\n",
    "    for i, prob in enumerate(problems):\n",
    "        answer, bullets_used, raw = generate(prob[\"question\"], pb)\n",
    "        correct = answers_match(answer, prob[\"answer\"])\n",
    "        log.correct.append(correct)\n",
    "        log.playbook_sizes.append(pb.size)\n",
    "\n",
    "        # Reflect\n",
    "        reflection, tags = reflect(\n",
    "            prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, pb\n",
    "        )\n",
    "        for bid, label in tags.items():\n",
    "            pb.tag(bid, label)\n",
    "        reflections_buffer.append((reflection, prob[\"question\"]))\n",
    "\n",
    "        # Curate periodically\n",
    "        if (i + 1) % CURATE_EVERY == 0 and reflections_buffer:\n",
    "            # Use most recent reflection\n",
    "            ref_text, ref_q = reflections_buffer[-1]\n",
    "            pb = curate(pb, ref_text, ref_q)\n",
    "            reflections_buffer = []\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            acc = sum(log.correct) / len(log.correct)\n",
    "            print(f\"  Greedy [{i+1}/{len(problems)}] acc={acc:.2%} bullets={pb.size}\")\n",
    "\n",
    "    log.call_counts = get_call_counts()\n",
    "    log.final_playbook = pb\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Strategy 2: PUCT-ACE ---\n\n@dataclass\nclass MCTSNode:\n    playbook: Playbook\n    parent: Optional[\"MCTSNode\"] = None\n    children: List[\"MCTSNode\"] = field(default_factory=list)\n    visits: int = 0\n    total_reward: float = 0.0\n    results: List[bool] = field(default_factory=list)\n\n    @property\n    def q_value(self) -> float:\n        if self.visits == 0:\n            return 0.5  # optimistic prior for unvisited nodes\n        return self.total_reward / self.visits\n\n\ndef puct_select(node: MCTSNode, c_puct: float = 1.0) -> MCTSNode:\n    \"\"\"Select best child via PUCT, recurse to leaf.\"\"\"\n    if not node.children:\n        return node\n    n_parent = node.visits\n    best, best_score = None, -float(\"inf\")\n    for child in node.children:\n        prior = 1.0 / len(node.children)\n        exploit = child.q_value\n        explore = c_puct * prior * math.sqrt(n_parent) / (1 + child.visits)\n        score = exploit + explore\n        if score > best_score:\n            best_score = score\n            best = child\n    return puct_select(best, c_puct)\n\n\ndef backprop(node: MCTSNode, reward: float):\n    \"\"\"Backpropagate reward up the tree.\"\"\"\n    while node is not None:\n        node.visits += 1\n        node.total_reward += reward\n        node = node.parent\n\n\ndef run_puct(problems: List[dict], batch_size: int = 3, c_puct: float = 1.0) -> RunLog:\n    \"\"\"PUCT tree search over playbook versions.\"\"\"\n    reset_call_counter()\n    log = RunLog()\n    root = MCTSNode(playbook=make_initial_playbook())\n    prob_idx = 0\n\n    while prob_idx < len(problems):\n        # Select leaf via PUCT\n        leaf = puct_select(root, c_puct)\n\n        # Evaluate on a batch using a COPY of the leaf's playbook (don't mutate tree node)\n        eval_pb = leaf.playbook.copy()\n        batch_end = min(prob_idx + batch_size, len(problems))\n        batch = problems[prob_idx:batch_end]\n        batch_correct = []\n        last_reflection = \"\"\n        last_question = \"\"\n\n        for prob in batch:\n            answer, bullets_used, raw = generate(prob[\"question\"], eval_pb)\n            correct = answers_match(answer, prob[\"answer\"])\n            batch_correct.append(correct)\n            log.correct.append(correct)\n            log.playbook_sizes.append(eval_pb.size)\n\n            reflection, tags = reflect(\n                prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, eval_pb\n            )\n            for bid, label in tags.items():\n                eval_pb.tag(bid, label)\n            last_reflection = reflection\n            last_question = prob[\"question\"]\n\n        prob_idx = batch_end\n        reward = sum(batch_correct) / len(batch_correct)\n        leaf.results.extend(batch_correct)\n\n        # Expand: curate to create child node with updated playbook\n        new_pb = curate(eval_pb, last_reflection, last_question)\n        child = MCTSNode(playbook=new_pb, parent=leaf)\n        leaf.children.append(child)\n\n        # Backprop from the CHILD (so leaf gets visit count, enabling revisits)\n        backprop(child, reward)\n\n        if prob_idx % 10 <= batch_size:\n            acc = sum(log.correct) / len(log.correct)\n            print(f\"  PUCT [{prob_idx}/{len(problems)}] acc={acc:.2%} tree_depth={_tree_depth(root)} nodes={_tree_size(root)}\")\n\n    log.call_counts = get_call_counts()\n    best_leaf = _best_leaf(root)\n    log.final_playbook = best_leaf.playbook\n    return log\n\n\ndef _tree_depth(node: MCTSNode) -> int:\n    if not node.children:\n        return 0\n    return 1 + max(_tree_depth(c) for c in node.children)\n\n\ndef _tree_size(node: MCTSNode) -> int:\n    return 1 + sum(_tree_size(c) for c in node.children)\n\n\ndef _best_leaf(node: MCTSNode) -> MCTSNode:\n    if not node.children:\n        return node\n    best = max(node.children, key=lambda c: c.q_value if c.visits > 0 else -1)\n    return _best_leaf(best)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Strategy 3: ES-ACE ---\n\nPOP_SIZE = 4\nTOP_K = 2  # tournament selection: keep top 2\n\ndef run_es(problems: List[dict], batch_size: int = 3) -> RunLog:\n    \"\"\"Evolutionary strategy with population of playbooks.\n\n    Budget control: each generation, we evaluate ONE problem per population member\n    (round-robin assignment), so total generate calls ≈ total problems (same as Greedy).\n    After POP_SIZE problems, we do selection + mutation (curate).\n    \"\"\"\n    reset_call_counter()\n    log = RunLog()\n    population = [make_initial_playbook() for _ in range(POP_SIZE)]\n    prob_idx = 0\n    generation = 0\n\n    while prob_idx < len(problems):\n        # Each member gets one problem (round-robin)\n        scores = [0.0] * POP_SIZE\n        member_results = [[] for _ in range(POP_SIZE)]\n        all_reflections = []\n        problems_this_gen = []\n\n        for member_idx in range(POP_SIZE):\n            if prob_idx >= len(problems):\n                break\n            prob = problems[prob_idx]\n            problems_this_gen.append(prob)\n            pb = population[member_idx]\n\n            answer, bullets_used, raw = generate(prob[\"question\"], pb)\n            correct = answers_match(answer, prob[\"answer\"])\n            member_results[member_idx].append(correct)\n            log.correct.append(correct)\n            log.playbook_sizes.append(pb.size)\n\n            reflection, tags = reflect(\n                prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, pb\n            )\n            for bid, label in tags.items():\n                pb.tag(bid, label)\n            all_reflections.append((member_idx, reflection, prob[\"question\"]))\n\n            scores[member_idx] = 1.0 if correct else 0.0\n            prob_idx += 1\n\n        generation += 1\n\n        # Selection: keep top-K by score (break ties by helpful count)\n        ranked = sorted(range(POP_SIZE), key=lambda i: (\n            scores[i], sum(b.helpful - b.harmful for b in population[i].bullets)\n        ), reverse=True)\n        survivors = [population[i].copy() for i in ranked[:TOP_K]]\n\n        # Mutation: curate each survivor to fill population\n        new_pop = []\n        for surv_idx, surv in enumerate(survivors):\n            new_pop.append(surv)\n            orig_idx = ranked[surv_idx]\n            ref_candidates = [(r, q) for mi, r, q in all_reflections if mi == orig_idx]\n            if ref_candidates:\n                ref_text, ref_q = ref_candidates[-1]\n                mutant = curate(surv, ref_text, ref_q)\n                new_pop.append(mutant)\n            else:\n                new_pop.append(surv.copy())\n\n        population = new_pop[:POP_SIZE]\n\n        if prob_idx % 10 <= POP_SIZE:\n            acc = sum(log.correct) / len(log.correct) if log.correct else 0\n            print(f\"  ES [{prob_idx}/{len(problems)}] gen={generation} acc={acc:.2%}\")\n\n    log.call_counts = get_call_counts()\n    best_idx = max(range(len(population)),\n                   key=lambda i: sum(b.helpful - b.harmful for b in population[i].bullets))\n    log.final_playbook = population[best_idx]\n    return log"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Running Greedy ACE...\")\n",
    "print(\"=\" * 60)\n",
    "greedy_log = run_greedy(problems)\n",
    "print(f\"Greedy done. Final acc (last 20): {greedy_log.final_accuracy:.2%}\")\n",
    "print(f\"Calls: {greedy_log.call_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Running PUCT-ACE...\")\n",
    "print(\"=\" * 60)\n",
    "puct_log = run_puct(problems)\n",
    "print(f\"PUCT done. Final acc (last 20): {puct_log.final_accuracy:.2%}\")\n",
    "print(f\"Calls: {puct_log.call_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Running ES-ACE...\")\n",
    "print(\"=\" * 60)\n",
    "es_log = run_es(problems)\n",
    "print(f\"ES done. Final acc (last 20): {es_log.final_accuracy:.2%}\")\n",
    "print(f\"Calls: {es_log.call_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analysis & Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"Greedy ACE\": greedy_log,\n",
    "    \"PUCT-ACE\": puct_log,\n",
    "    \"ES-ACE\": es_log,\n",
    "}\n",
    "\n",
    "COLORS = {\"Greedy ACE\": \"#1f77b4\", \"PUCT-ACE\": \"#ff7f0e\", \"ES-ACE\": \"#2ca02c\"}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle(\"Search-Augmented ACE: GSM8K Math Reasoning\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# --- (a) Running accuracy curves ---\n",
    "ax = axes[0, 0]\n",
    "for name, log in results.items():\n",
    "    acc = log.running_accuracy\n",
    "    ax.plot(range(1, len(acc) + 1), acc, label=name, color=COLORS[name], linewidth=2)\n",
    "ax.set_xlabel(\"Problems Solved\")\n",
    "ax.set_ylabel(\"Running Accuracy\")\n",
    "ax.set_title(\"(a) Running Accuracy Over Time\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# --- (b) Final accuracy bar chart with bootstrap CI ---\n",
    "ax = axes[0, 1]\n",
    "\n",
    "def bootstrap_ci(data, n_boot=1000, ci=0.95):\n",
    "    \"\"\"Bootstrap confidence interval for mean.\"\"\"\n",
    "    data = np.array(data, dtype=float)\n",
    "    means = [np.mean(np.random.choice(data, size=len(data), replace=True)) for _ in range(n_boot)]\n",
    "    lo = np.percentile(means, (1 - ci) / 2 * 100)\n",
    "    hi = np.percentile(means, (1 + ci) / 2 * 100)\n",
    "    return np.mean(data), lo, hi\n",
    "\n",
    "names = list(results.keys())\n",
    "means, lows, highs = [], [], []\n",
    "for name in names:\n",
    "    tail = results[name].correct[-20:]\n",
    "    m, lo, hi = bootstrap_ci(tail)\n",
    "    means.append(m)\n",
    "    lows.append(m - lo)\n",
    "    highs.append(hi - m)\n",
    "\n",
    "bars = ax.bar(names, means, color=[COLORS[n] for n in names], yerr=[lows, highs], capsize=5)\n",
    "ax.set_ylabel(\"Accuracy (last 20 problems)\")\n",
    "ax.set_title(\"(b) Final Accuracy Comparison\")\n",
    "ax.set_ylim(0, 1)\n",
    "for bar, m in zip(bars, means):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f\"{m:.1%}\",\n",
    "            ha=\"center\", fontsize=10)\n",
    "\n",
    "# --- (c) Playbook size over time ---\n",
    "ax = axes[1, 0]\n",
    "for name, log in results.items():\n",
    "    sizes = log.playbook_sizes\n",
    "    ax.plot(range(1, len(sizes) + 1), sizes, label=name, color=COLORS[name], linewidth=2)\n",
    "ax.set_xlabel(\"Problems Solved\")\n",
    "ax.set_ylabel(\"Playbook Bullets\")\n",
    "ax.set_title(\"(c) Playbook Size Over Time\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- (d) Cost breakdown ---\n",
    "ax = axes[1, 1]\n",
    "roles = [\"generate\", \"reflect\", \"curate\"]\n",
    "x = np.arange(len(names))\n",
    "width = 0.25\n",
    "for i, role in enumerate(roles):\n",
    "    counts = [results[n].call_counts.get(role, 0) for n in names]\n",
    "    ax.bar(x + i * width, counts, width, label=role.capitalize())\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(names)\n",
    "ax.set_ylabel(\"LLM Calls\")\n",
    "ax.set_title(\"(d) LLM Call Breakdown\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"search_ace_results.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Plots saved to search_ace_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Statistical comparison ---\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Statistical Comparison (Bootstrap 95% CI on accuracy difference)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def bootstrap_diff_ci(a, b, n_boot=2000):\n",
    "    a, b = np.array(a, dtype=float), np.array(b, dtype=float)\n",
    "    diffs = []\n",
    "    for _ in range(n_boot):\n",
    "        ia = np.random.choice(len(a), len(a), replace=True)\n",
    "        ib = np.random.choice(len(b), len(b), replace=True)\n",
    "        diffs.append(np.mean(b[ib]) - np.mean(a[ia]))\n",
    "    lo, hi = np.percentile(diffs, [2.5, 97.5])\n",
    "    return np.mean(diffs), lo, hi\n",
    "\n",
    "greedy_tail = greedy_log.correct[-20:]\n",
    "puct_tail = puct_log.correct[-20:]\n",
    "es_tail = es_log.correct[-20:]\n",
    "\n",
    "for name, tail in [(\"PUCT-ACE\", puct_tail), (\"ES-ACE\", es_tail)]:\n",
    "    mean_diff, lo, hi = bootstrap_diff_ci(greedy_tail, tail)\n",
    "    sig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\n",
    "    print(f\"{name} vs Greedy: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")\n",
    "\n",
    "mean_diff, lo, hi = bootstrap_diff_ci(puct_tail, es_tail)\n",
    "sig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\n",
    "print(f\"ES-ACE vs PUCT-ACE: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Results Summary\n\n### Experiment\n- **Dataset**: GSM8K (50 problems, shuffled)\n- **Model**: Qwen2.5-7B-Instruct (local via vLLM)\n- **Conditions**: Greedy ACE, PUCT-ACE (c=1.0), ES-ACE (pop=4)\n\n### Budget Control\nAll conditions use ~50 generate + ~50 reflect calls. Curate calls vary slightly:\n- **Greedy**: curate every 5 problems → 10 curate calls\n- **PUCT**: curate per batch of 3 → ~17 curate calls\n- **ES**: curate per generation of 4 → ~13 curate calls (top-2 survivors × 1 mutant each)\n\n### Key Questions\n1. Does PUCT-guided playbook search outperform greedy sequential evolution?\n2. Does evolutionary diversity (ES) help vs. single-trajectory (Greedy)?\n3. What is the cost overhead of search vs. accuracy gain?\n\n### Interpretation\nSee plots above. The running accuracy curves (a) show learning dynamics. The bar chart (b) shows final performance with confidence intervals. If PUCT or ES significantly outperforms Greedy on the last 20 problems, this validates the hypothesis that search over playbook space improves ACE.\n\n### Limitations\n- 50 problems is small; larger eval needed for publication\n- Uniform PUCT prior (no LLM-based prior P(s,a) yet)\n- Single seed; should repeat with multiple seeds\n- ES round-robin means each member sees different problems (not directly comparable per-problem)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cleanup: kill vLLM server ---\n",
    "try:\n",
    "    os.kill(vllm_proc.pid, signal.SIGTERM)\n",
    "    print(f\"Killed vLLM server (PID {vllm_proc.pid})\")\n",
    "except ProcessLookupError:\n",
    "    print(\"vLLM server already stopped\")"
   ]
  }
 ]
}