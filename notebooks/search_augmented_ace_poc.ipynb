{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search-Augmented ACE: Search Strategy Comparison\n",
    "\n",
    "**Hypothesis**: Different search strategies over ACE-style playbook space trade off exploration, exploitation, and efficiency differently. We compare six conditions spanning no-evolution baselines through flat bandits to tree search.\n",
    "\n",
    "**Six conditions** (matched LLM budget ~100-120 calls):\n",
    "1. **Majority Vote** — no evolution, 2 samples per problem, majority answer (null hypothesis)\n",
    "2. **Greedy ACE** — sequential generate → reflect → curate (standard ACE baseline)\n",
    "3. **Thompson Sampling** — flat bandit over a pool of curated playbook variants\n",
    "4. **PUCT-Mean** — tree search, Q = mean reward\n",
    "5. **PUCT-EMA** — tree search, Q = exponential moving average (α=0.4)\n",
    "6. **PUCT-Bayesian** — tree search, Q = Beta posterior mean\n",
    "\n",
    "**Setup**: Qwen2.5-7B-Instruct via vLLM on A100, 50 GSM8K problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install vllm openai datasets matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import signal\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch vLLM server in background\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "VLLM_PORT = 8000\n",
    "\n",
    "vllm_proc = subprocess.Popen(\n",
    "    [\n",
    "        \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
    "        \"--model\", MODEL_NAME,\n",
    "        \"--port\", str(VLLM_PORT),\n",
    "        \"--max-model-len\", \"4096\",\n",
    "        \"--gpu-memory-utilization\", \"0.85\",\n",
    "        \"--dtype\", \"auto\",\n",
    "    ],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    ")\n",
    "print(f\"vLLM server PID: {vllm_proc.pid}\")\n",
    "\n",
    "# Wait for server to be ready\n",
    "client = OpenAI(base_url=f\"http://localhost:{VLLM_PORT}/v1\", api_key=\"dummy\")\n",
    "for attempt in range(60):\n",
    "    try:\n",
    "        client.models.list()\n",
    "        print(f\"vLLM ready after {attempt + 1}s\")\n",
    "        break\n",
    "    except Exception:\n",
    "        time.sleep(1)\n",
    "else:\n",
    "    raise RuntimeError(\"vLLM server did not start within 60s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GSM8K Data Loading & Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset\n\nds = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n\ndef extract_gsm8k_answer(answer_text: str) -> str:\n    \"\"\"Extract numeric answer from GSM8K '#### <number>' format.\"\"\"\n    match = re.search(r\"####\\s*(-?[\\d,]+\\.?\\d*)\", answer_text)\n    if match:\n        return match.group(1).replace(\",\", \"\").strip()\n    # Fallback: last number in text\n    nums = re.findall(r\"-?[\\d,]+\\.?\\d*\", answer_text)\n    if nums:\n        return nums[-1].replace(\",\", \"\")\n    return \"\"\n\nproblems = []\nfor item in ds:\n    problems.append({\n        \"question\": item[\"question\"],\n        \"answer\": extract_gsm8k_answer(item[\"answer\"]),\n        \"full_answer\": item[\"answer\"],\n    })\n\n# Use first 50 problems, shuffled deterministically\nrng = random.Random(SEED)\nrng.shuffle(problems)\nproblems = problems[:50]\nprint(f\"Loaded {len(problems)} GSM8K problems\")\nprint(f\"Example: Q='{problems[0]['question'][:80]}...' A={problems[0]['answer']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core Components: Playbook, Generator, Reflector, Curator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Playbook representation ---\n",
    "\n",
    "@dataclass\n",
    "class Bullet:\n",
    "    id: str\n",
    "    section: str  # STRATEGIES, COMMON_MISTAKES, SOLUTION_PATTERNS\n",
    "    content: str\n",
    "    helpful: int = 0\n",
    "    harmful: int = 0\n",
    "\n",
    "    def to_str(self) -> str:\n",
    "        return f\"[{self.id}] helpful={self.helpful} harmful={self.harmful} :: {self.content}\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Playbook:\n",
    "    bullets: List[Bullet] = field(default_factory=list)\n",
    "    _next_id: int = 1\n",
    "\n",
    "    def add(self, section: str, content: str) -> str:\n",
    "        prefix = {\"STRATEGIES\": \"str\", \"COMMON_MISTAKES\": \"err\", \"SOLUTION_PATTERNS\": \"sol\"}.get(section, \"gen\")\n",
    "        bid = f\"{prefix}-{self._next_id:05d}\"\n",
    "        self._next_id += 1\n",
    "        self.bullets.append(Bullet(id=bid, section=section, content=content))\n",
    "        return bid\n",
    "\n",
    "    def remove(self, bid: str):\n",
    "        self.bullets = [b for b in self.bullets if b.id != bid]\n",
    "\n",
    "    def update(self, bid: str, content: str):\n",
    "        for b in self.bullets:\n",
    "            if b.id == bid:\n",
    "                b.content = content\n",
    "                return\n",
    "\n",
    "    def tag(self, bid: str, label: str):\n",
    "        for b in self.bullets:\n",
    "            if b.id == bid:\n",
    "                if label == \"helpful\":\n",
    "                    b.helpful += 1\n",
    "                elif label == \"harmful\":\n",
    "                    b.harmful += 1\n",
    "\n",
    "    def to_str(self) -> str:\n",
    "        sections = defaultdict(list)\n",
    "        for b in self.bullets:\n",
    "            sections[b.section].append(b.to_str())\n",
    "        parts = []\n",
    "        for sec in [\"STRATEGIES\", \"COMMON_MISTAKES\", \"SOLUTION_PATTERNS\"]:\n",
    "            if sections[sec]:\n",
    "                parts.append(f\"## {sec}\")\n",
    "                parts.extend(sections[sec])\n",
    "        return \"\\n\".join(parts) if parts else \"(empty playbook)\"\n",
    "\n",
    "    def copy(self) -> \"Playbook\":\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return len(self.bullets)\n",
    "\n",
    "\n",
    "def make_initial_playbook() -> Playbook:\n",
    "    pb = Playbook()\n",
    "    pb.add(\"STRATEGIES\", \"Break word problems into step-by-step arithmetic.\")\n",
    "    pb.add(\"STRATEGIES\", \"Identify what quantity the question asks for before computing.\")\n",
    "    pb.add(\"COMMON_MISTAKES\", \"Watch for unit conversions (hours to minutes, etc).\")\n",
    "    return pb\n",
    "\n",
    "print(make_initial_playbook().to_str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM call wrapper ---\n",
    "\n",
    "call_counter = defaultdict(int)  # track calls by role\n",
    "\n",
    "def llm_call(system: str, user: str, role: str = \"generate\", temperature: float = 0.7, max_tokens: int = 1024) -> str:\n",
    "    \"\"\"Single LLM call via vLLM OpenAI-compatible API.\"\"\"\n",
    "    call_counter[role] += 1\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": user},\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"LLM call failed ({role}): {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def reset_call_counter():\n",
    "    global call_counter\n",
    "    call_counter = defaultdict(int)\n",
    "\n",
    "def get_call_counts() -> Dict[str, int]:\n",
    "    return dict(call_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Generator ---\n\ndef generate(question: str, playbook: Playbook) -> Tuple[str, List[str], str]:\n    \"\"\"\n    Generate a solution to a math problem using the playbook.\n    Returns (extracted_answer, bullets_used, raw_response).\n    \"\"\"\n    pb_text = playbook.to_str()\n    system = (\n        \"You are a math problem solver. Use the playbook strategies below to help solve the problem.\\n\"\n        \"When you use a specific strategy, mention its ID (e.g., [str-00001]).\\n\"\n        \"Show your work step-by-step, then give the final numeric answer on its own line as: #### <number>\\n\\n\"\n        f\"PLAYBOOK:\\n{pb_text}\"\n    )\n    user = f\"Solve this problem:\\n{question}\"\n    raw = llm_call(system, user, role=\"generate\")\n\n    # Extract answer\n    answer = \"\"\n    m = re.search(r\"####\\s*(-?[\\d,]+\\.?\\d*)\", raw)\n    if m:\n        answer = m.group(1).replace(\",\", \"\").strip()\n    else:\n        nums = re.findall(r\"-?[\\d,]+\\.?\\d*\", raw)\n        if nums:\n            answer = nums[-1].replace(\",\", \"\")\n\n    # Extract bullet references\n    bullets_used = re.findall(r\"\\[(\\w+-\\d+)\\]\", raw)\n\n    return answer, bullets_used, raw"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Answer comparison ---\n",
    "\n",
    "def answers_match(predicted: str, ground_truth: str) -> bool:\n",
    "    \"\"\"Compare numeric answers with tolerance.\"\"\"\n",
    "    try:\n",
    "        p = float(predicted.replace(\",\", \"\"))\n",
    "        g = float(ground_truth.replace(\",\", \"\"))\n",
    "        return abs(p - g) < 1e-3\n",
    "    except (ValueError, TypeError):\n",
    "        return predicted.strip() == ground_truth.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reflector ---\n",
    "\n",
    "def reflect(question: str, raw_response: str, predicted: str, ground_truth: str,\n",
    "            bullets_used: List[str], playbook: Playbook) -> Tuple[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Reflect on the solution attempt.\n",
    "    Returns (reflection_text, bullet_tags) where bullet_tags maps bullet_id -> 'helpful'|'harmful'|'neutral'.\n",
    "    \"\"\"\n",
    "    correct = answers_match(predicted, ground_truth)\n",
    "    feedback = \"CORRECT\" if correct else f\"INCORRECT (predicted {predicted}, expected {ground_truth})\"\n",
    "\n",
    "    bullets_text = \"\"\n",
    "    for b in playbook.bullets:\n",
    "        if b.id in bullets_used:\n",
    "            bullets_text += f\"  {b.to_str()}\\n\"\n",
    "\n",
    "    system = (\n",
    "        \"You are a math reasoning analyst. Analyze whether the solution approach was correct \"\n",
    "        \"and whether the playbook strategies used were helpful or harmful.\\n\"\n",
    "        \"For each bullet ID used, output a JSON line: {\\\"id\\\": \\\"str-00001\\\", \\\"tag\\\": \\\"helpful\\\"}\\n\"\n",
    "        \"Tags must be one of: helpful, harmful, neutral.\\n\"\n",
    "        \"End with a brief reflection paragraph.\"\n",
    "    )\n",
    "    user = (\n",
    "        f\"Problem: {question}\\n\\n\"\n",
    "        f\"Solution attempt:\\n{raw_response}\\n\\n\"\n",
    "        f\"Result: {feedback}\\n\\n\"\n",
    "        f\"Playbook bullets used:\\n{bullets_text}\"\n",
    "    )\n",
    "    raw = llm_call(system, user, role=\"reflect\", temperature=0.3)\n",
    "\n",
    "    # Parse bullet tags\n",
    "    tags = {}\n",
    "    for m in re.finditer(r'\"id\"\\s*:\\s*\"([^\"]+)\".*?\"tag\"\\s*:\\s*\"(helpful|harmful|neutral)\"', raw):\n",
    "        tags[m.group(1)] = m.group(2)\n",
    "\n",
    "    # If no tags parsed but we know correctness, apply heuristic\n",
    "    if not tags:\n",
    "        for bid in bullets_used:\n",
    "            tags[bid] = \"helpful\" if correct else \"neutral\"\n",
    "\n",
    "    return raw, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Curator ---\n",
    "\n",
    "MAX_BULLETS = 20\n",
    "\n",
    "def curate(playbook: Playbook, reflection: str, question: str) -> Playbook:\n",
    "    \"\"\"\n",
    "    Curate the playbook based on reflection.\n",
    "    Returns a new (copied) playbook with operations applied.\n",
    "    \"\"\"\n",
    "    pb = playbook.copy()\n",
    "    pb_text = pb.to_str()\n",
    "\n",
    "    system = (\n",
    "        \"You are a playbook curator for math problem solving. Based on the reflection, \"\n",
    "        \"propose operations to improve the playbook.\\n\"\n",
    "        \"Output a JSON array of operations:\\n\"\n",
    "        '[{\"op\": \"ADD\", \"section\": \"STRATEGIES\", \"content\": \"new insight\"},\\n'\n",
    "        ' {\"op\": \"UPDATE\", \"id\": \"str-00001\", \"content\": \"refined text\"},\\n'\n",
    "        ' {\"op\": \"DELETE\", \"id\": \"err-00002\"}]\\n'\n",
    "        f\"Sections: STRATEGIES, COMMON_MISTAKES, SOLUTION_PATTERNS\\n\"\n",
    "        f\"Max bullets: {MAX_BULLETS}. Current: {pb.size}.\\n\"\n",
    "        \"Only propose operations that are clearly supported by the reflection. Keep it minimal.\"\n",
    "    )\n",
    "    user = (\n",
    "        f\"Current playbook:\\n{pb_text}\\n\\n\"\n",
    "        f\"Problem context: {question[:200]}\\n\\n\"\n",
    "        f\"Reflection:\\n{reflection}\"\n",
    "    )\n",
    "    raw = llm_call(system, user, role=\"curate\", temperature=0.3)\n",
    "\n",
    "    # Parse operations from JSON array\n",
    "    ops = []\n",
    "    # Try to find JSON array in response\n",
    "    json_match = re.search(r'\\[\\s*\\{.*?\\}\\s*\\]', raw, re.DOTALL)\n",
    "    if json_match:\n",
    "        try:\n",
    "            ops = json.loads(json_match.group())\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "    # Apply operations\n",
    "    for op in ops:\n",
    "        try:\n",
    "            if op.get(\"op\") == \"ADD\" and pb.size < MAX_BULLETS:\n",
    "                pb.add(op.get(\"section\", \"STRATEGIES\"), op.get(\"content\", \"\"))\n",
    "            elif op.get(\"op\") == \"UPDATE\" and op.get(\"id\"):\n",
    "                pb.update(op[\"id\"], op.get(\"content\", \"\"))\n",
    "            elif op.get(\"op\") == \"DELETE\" and op.get(\"id\"):\n",
    "                pb.remove(op[\"id\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Safety: if curator emptied the playbook, reset\n",
    "    if pb.size == 0:\n",
    "        pb = make_initial_playbook()\n",
    "        pb._next_id = playbook._next_id\n",
    "\n",
    "    return pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Search Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Shared tracking ---\n",
    "\n",
    "@dataclass\n",
    "class RunLog:\n",
    "    \"\"\"Tracks per-problem results for a single condition.\"\"\"\n",
    "    correct: List[bool] = field(default_factory=list)\n",
    "    playbook_sizes: List[int] = field(default_factory=list)\n",
    "    call_counts: Dict[str, int] = field(default_factory=dict)\n",
    "    final_playbook: Optional[Playbook] = None\n",
    "\n",
    "    @property\n",
    "    def running_accuracy(self) -> List[float]:\n",
    "        acc = []\n",
    "        total = 0\n",
    "        for i, c in enumerate(self.correct):\n",
    "            total += int(c)\n",
    "            acc.append(total / (i + 1))\n",
    "        return acc\n",
    "\n",
    "    @property\n",
    "    def final_accuracy(self) -> float:\n",
    "        if not self.correct:\n",
    "            return 0.0\n",
    "        # Accuracy on last 20 problems\n",
    "        tail = self.correct[-20:]\n",
    "        return sum(tail) / len(tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Strategy 1: Greedy ACE ---\n",
    "\n",
    "CURATE_EVERY = 5\n",
    "\n",
    "def run_greedy(problems: List[dict]) -> RunLog:\n",
    "    \"\"\"Sequential generate → reflect → curate loop.\"\"\"\n",
    "    reset_call_counter()\n",
    "    log = RunLog()\n",
    "    pb = make_initial_playbook()\n",
    "    reflections_buffer = []\n",
    "\n",
    "    for i, prob in enumerate(problems):\n",
    "        answer, bullets_used, raw = generate(prob[\"question\"], pb)\n",
    "        correct = answers_match(answer, prob[\"answer\"])\n",
    "        log.correct.append(correct)\n",
    "        log.playbook_sizes.append(pb.size)\n",
    "\n",
    "        # Reflect\n",
    "        reflection, tags = reflect(\n",
    "            prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, pb\n",
    "        )\n",
    "        for bid, label in tags.items():\n",
    "            pb.tag(bid, label)\n",
    "        reflections_buffer.append((reflection, prob[\"question\"]))\n",
    "\n",
    "        # Curate periodically\n",
    "        if (i + 1) % CURATE_EVERY == 0 and reflections_buffer:\n",
    "            # Use most recent reflection\n",
    "            ref_text, ref_q = reflections_buffer[-1]\n",
    "            pb = curate(pb, ref_text, ref_q)\n",
    "            reflections_buffer = []\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            acc = sum(log.correct) / len(log.correct)\n",
    "            print(f\"  Greedy [{i+1}/{len(problems)}] acc={acc:.2%} bullets={pb.size}\")\n",
    "\n",
    "    log.call_counts = get_call_counts()\n",
    "    log.final_playbook = pb\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Strategy 2: PUCT-ACE with Q-estimator variants ---\n",
    "\n",
    "@dataclass\n",
    "class MCTSNode:\n",
    "    playbook: Playbook\n",
    "    parent: Optional[\"MCTSNode\"] = None\n",
    "    children: List[\"MCTSNode\"] = field(default_factory=list)\n",
    "    visits: int = 0\n",
    "    reward_history: List[float] = field(default_factory=list)\n",
    "    results: List[bool] = field(default_factory=list)\n",
    "\n",
    "    def q_value(self, mode: str = \"mean\") -> float:\n",
    "        if not self.reward_history:\n",
    "            return 0.5  # optimistic prior for unvisited nodes\n",
    "        if mode == \"mean\":\n",
    "            return sum(self.reward_history) / len(self.reward_history)\n",
    "        elif mode == \"ema\":\n",
    "            # Exponential moving average — recent batches matter more\n",
    "            alpha = 0.4\n",
    "            q = self.reward_history[0]\n",
    "            for r in self.reward_history[1:]:\n",
    "                q = alpha * r + (1 - alpha) * q\n",
    "            return q\n",
    "        elif mode == \"bayesian\":\n",
    "            # Beta(successes+1, failures+1) posterior mean\n",
    "            # Shrinks toward 0.5 with few observations — maximally exploratory\n",
    "            s = sum(self.reward_history)\n",
    "            n = len(self.reward_history)\n",
    "            return (s + 1) / (n + 2)\n",
    "        return 0.5\n",
    "\n",
    "\n",
    "def puct_select(node: MCTSNode, c_puct: float = 1.5, q_mode: str = \"mean\") -> MCTSNode:\n",
    "    \"\"\"Select best child via PUCT, recurse to leaf.\"\"\"\n",
    "    if not node.children:\n",
    "        return node\n",
    "    n_parent = node.visits\n",
    "    best, best_score = None, -float(\"inf\")\n",
    "    for child in node.children:\n",
    "        prior = 1.0 / len(node.children)\n",
    "        exploit = child.q_value(q_mode)\n",
    "        explore = c_puct * prior * math.sqrt(n_parent) / (1 + child.visits)\n",
    "        score = exploit + explore\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best = child\n",
    "    return puct_select(best, c_puct, q_mode)\n",
    "\n",
    "\n",
    "def backprop(node: MCTSNode, reward: float):\n",
    "    \"\"\"Backpropagate reward up the tree.\"\"\"\n",
    "    while node is not None:\n",
    "        node.visits += 1\n",
    "        node.reward_history.append(reward)\n",
    "        node = node.parent\n",
    "\n",
    "\n",
    "def should_expand(node: MCTSNode, pw_alpha: float = 0.5) -> bool:\n",
    "    \"\"\"Progressive widening: expand only when node needs more children.\n",
    "    Branching factor grows as ceil(visits^alpha). With alpha=0.5:\n",
    "      visits=1 → 1 child allowed\n",
    "      visits=4 → 2 children\n",
    "      visits=9 → 3 children\n",
    "    This forces the tree to RE-EVALUATE existing nodes before branching.\n",
    "    \"\"\"\n",
    "    max_children = max(1, math.ceil(node.visits ** pw_alpha))\n",
    "    return len(node.children) < max_children\n",
    "\n",
    "\n",
    "def run_puct(problems: List[dict], batch_size: int = 3, c_puct: float = 1.5,\n",
    "             q_mode: str = \"mean\") -> RunLog:\n",
    "    \"\"\"PUCT tree search over playbook versions with progressive widening.\"\"\"\n",
    "    reset_call_counter()\n",
    "    log = RunLog()\n",
    "    root = MCTSNode(playbook=make_initial_playbook())\n",
    "    prob_idx = 0\n",
    "\n",
    "    while prob_idx < len(problems):\n",
    "        # Select leaf via PUCT\n",
    "        leaf = puct_select(root, c_puct, q_mode)\n",
    "\n",
    "        # Evaluate on a batch using a COPY (don't mutate tree node)\n",
    "        eval_pb = leaf.playbook.copy()\n",
    "        batch_end = min(prob_idx + batch_size, len(problems))\n",
    "        batch = problems[prob_idx:batch_end]\n",
    "        batch_correct = []\n",
    "        last_reflection = \"\"\n",
    "        last_question = \"\"\n",
    "\n",
    "        for prob in batch:\n",
    "            answer, bullets_used, raw = generate(prob[\"question\"], eval_pb)\n",
    "            correct = answers_match(answer, prob[\"answer\"])\n",
    "            batch_correct.append(correct)\n",
    "            log.correct.append(correct)\n",
    "            log.playbook_sizes.append(eval_pb.size)\n",
    "\n",
    "            reflection, tags = reflect(\n",
    "                prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, eval_pb\n",
    "            )\n",
    "            for bid, label in tags.items():\n",
    "                eval_pb.tag(bid, label)\n",
    "            last_reflection = reflection\n",
    "            last_question = prob[\"question\"]\n",
    "\n",
    "        prob_idx = batch_end\n",
    "        reward = sum(batch_correct) / len(batch_correct)\n",
    "        leaf.results.extend(batch_correct)\n",
    "\n",
    "        # Progressive widening: only expand if this node needs more children\n",
    "        if should_expand(leaf):\n",
    "            new_pb = curate(eval_pb, last_reflection, last_question)\n",
    "            child = MCTSNode(playbook=new_pb, parent=leaf)\n",
    "            leaf.children.append(child)\n",
    "\n",
    "        # Always backprop to LEAF (the node whose playbook was actually evaluated).\n",
    "        # New children start with optimistic Q=0.5 prior until they're evaluated.\n",
    "        backprop(leaf, reward)\n",
    "\n",
    "        if prob_idx % 10 <= batch_size:\n",
    "            acc = sum(log.correct) / len(log.correct)\n",
    "            depth = _tree_depth(root)\n",
    "            size = _tree_size(root)\n",
    "            branches = _branch_count(root)\n",
    "            print(f\"  PUCT-{q_mode} [{prob_idx}/{len(problems)}] acc={acc:.2%} \"\n",
    "                  f\"depth={depth} nodes={size} branches={branches}\")\n",
    "\n",
    "    log.call_counts = get_call_counts()\n",
    "    best_leaf = _best_leaf(root, q_mode)\n",
    "    log.final_playbook = best_leaf.playbook\n",
    "    return log\n",
    "\n",
    "\n",
    "def _tree_depth(node: MCTSNode) -> int:\n",
    "    if not node.children:\n",
    "        return 0\n",
    "    return 1 + max(_tree_depth(c) for c in node.children)\n",
    "\n",
    "\n",
    "def _tree_size(node: MCTSNode) -> int:\n",
    "    return 1 + sum(_tree_size(c) for c in node.children)\n",
    "\n",
    "\n",
    "def _branch_count(node: MCTSNode) -> int:\n",
    "    \"\"\"Count nodes with >1 child (actual branching points).\"\"\"\n",
    "    count = 1 if len(node.children) > 1 else 0\n",
    "    return count + sum(_branch_count(c) for c in node.children)\n",
    "\n",
    "\n",
    "def _best_leaf(node: MCTSNode, q_mode: str = \"mean\") -> MCTSNode:\n",
    "    if not node.children:\n",
    "        return node\n",
    "    best = max(node.children, key=lambda c: c.q_value(q_mode) if c.visits > 0 else -1)\n",
    "    return _best_leaf(best, q_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Strategy 3: Thompson Sampling over Playbook Pool ---\n\nTHOMPSON_SEED_PROBLEMS = 6  # problems used to generate initial reflections\nTHOMPSON_N_VARIANTS = 5     # playbook variants to create (+ 1 base = 6 total)\n\ndef run_thompson(problems: List[dict]) -> RunLog:\n    \"\"\"Flat bandit: maintain Beta posteriors per playbook variant, sample to select.\n\n    Phase 1 (seed): Solve first 6 problems with base playbook, collect reflections,\n                    curate 5 variants from different reflections.\n    Phase 2 (bandit): For each remaining problem, Thompson-sample which playbook to use.\n                     Reflect after each problem to update bullet tags on chosen playbook.\n    \n    Budget: 50 generate + 50 reflect + 5 curate = 105 calls.\n    (Seed phase: 6 generate + 6 reflect. Bandit phase: 44 generate + 44 reflect.)\n    \"\"\"\n    reset_call_counter()\n    log = RunLog()\n\n    base_pb = make_initial_playbook()\n    pool = [base_pb]\n\n    # --- Phase 1: Seed — solve problems with base, collect reflections for curation ---\n    seed = problems[:THOMPSON_SEED_PROBLEMS]\n    remaining = problems[THOMPSON_SEED_PROBLEMS:]\n    reflections = []\n\n    for prob in seed:\n        answer, bullets_used, raw = generate(prob[\"question\"], base_pb)\n        correct = answers_match(answer, prob[\"answer\"])\n        log.correct.append(correct)\n        log.playbook_sizes.append(base_pb.size)\n        # Reflect to get material for curation\n        reflection, tags = reflect(\n            prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, base_pb\n        )\n        for bid, label in tags.items():\n            base_pb.tag(bid, label)\n        reflections.append((reflection, prob[\"question\"]))\n\n    # Create variants from different reflections (diversity)\n    for ref_text, ref_q in reflections[:THOMPSON_N_VARIANTS]:\n        variant = curate(base_pb, ref_text, ref_q)\n        pool.append(variant)\n\n    print(f\"  Thompson: created {len(pool)} playbook variants from {len(seed)} seed problems\")\n\n    # --- Phase 2: Thompson Sampling ---\n    # Beta(alpha, beta) per variant — uniform prior\n    alphas = [1.0] * len(pool)\n    betas_param = [1.0] * len(pool)\n\n    for i, prob in enumerate(remaining):\n        # Sample from each Beta posterior, pick the highest\n        ts_samples = [np.random.beta(a, b) for a, b in zip(alphas, betas_param)]\n        chosen = int(np.argmax(ts_samples))\n        pb = pool[chosen]\n\n        answer, bullets_used, raw = generate(prob[\"question\"], pb)\n        correct = answers_match(answer, prob[\"answer\"])\n        log.correct.append(correct)\n        log.playbook_sizes.append(pb.size)\n\n        # Reflect and update tags on the chosen playbook\n        reflection, tags = reflect(\n            prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, pb\n        )\n        for bid, label in tags.items():\n            pb.tag(bid, label)\n\n        # Update Beta posterior for the chosen variant\n        if correct:\n            alphas[chosen] += 1.0\n        else:\n            betas_param[chosen] += 1.0\n\n        if (THOMPSON_SEED_PROBLEMS + i + 1) % 10 == 0:\n            acc = sum(log.correct) / len(log.correct)\n            # Show allocation stats\n            pulls = [int(a + b - 2) for a, b in zip(alphas, betas_param)]\n            best_var = int(np.argmax([a / (a + b) for a, b in zip(alphas, betas_param)]))\n            print(f\"  Thompson [{THOMPSON_SEED_PROBLEMS + i + 1}/{len(problems)}] \"\n                  f\"acc={acc:.2%} pulls={pulls} best=variant-{best_var}\")\n\n    log.call_counts = get_call_counts()\n    # Best variant by posterior mean\n    best_idx = int(np.argmax([a / (a + b) for a, b in zip(alphas, betas_param)]))\n    log.final_playbook = pool[best_idx]\n    return log"
  },
  {
   "cell_type": "code",
   "source": "# --- Strategy 3b: Dynamic Thompson Sampling (pool grows over time) ---\n\nTHOMPSON_DYN_SEED_PROBLEMS = 6\nTHOMPSON_DYN_N_VARIANTS = 5\nARM_ADD_INTERVAL = 10  # curate a new arm every N bandit-phase problems\n\ndef run_thompson_dynamic(problems: List[dict]) -> RunLog:\n    \"\"\"Thompson Sampling with dynamic arm addition.\n\n    Like run_thompson, but every ARM_ADD_INTERVAL problems during the bandit phase,\n    curate a NEW playbook variant from the current best arm's playbook and add it\n    to the pool with a fresh Beta(1,1) prior. This lets the pool evolve over time.\n\n    Inspired by OPTS (arXiv:2503.01163): bandit-based strategy selection benefits\n    from periodic pool mutation. The best arm's playbook has accumulated useful\n    bullet tags — curating from it produces better variants than seed-based curation.\n\n    Budget breakdown:\n      Phase 1 (seed): 6 generate + 6 reflect + 5 curate = 17 calls\n      Phase 2 (bandit): 44 generate + 44 reflect = 88 calls\n      Dynamic additions: ~4 curate calls (every 10 problems from 44 remaining)\n      Total: ~109 calls\n    \"\"\"\n    reset_call_counter()\n    log = RunLog()\n\n    base_pb = make_initial_playbook()\n    pool = [base_pb]\n\n    # --- Phase 1: Seed — solve problems with base, collect reflections for curation ---\n    seed = problems[:THOMPSON_DYN_SEED_PROBLEMS]\n    remaining = problems[THOMPSON_DYN_SEED_PROBLEMS:]\n    reflections = []\n\n    for prob in seed:\n        answer, bullets_used, raw = generate(prob[\"question\"], base_pb)\n        correct = answers_match(answer, prob[\"answer\"])\n        log.correct.append(correct)\n        log.playbook_sizes.append(base_pb.size)\n        reflection, tags = reflect(\n            prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, base_pb\n        )\n        for bid, label in tags.items():\n            base_pb.tag(bid, label)\n        reflections.append((reflection, prob[\"question\"]))\n\n    # Create initial variants from different reflections\n    for ref_text, ref_q in reflections[:THOMPSON_DYN_N_VARIANTS]:\n        variant = curate(base_pb, ref_text, ref_q)\n        pool.append(variant)\n\n    print(f\"  Thompson-Dyn: created {len(pool)} initial variants from {len(seed)} seed problems\")\n\n    # --- Phase 2: Thompson Sampling with dynamic arm addition ---\n    alphas = [1.0] * len(pool)\n    betas_param = [1.0] * len(pool)\n    last_reflections = {}  # arm_idx -> (reflection_text, question) for curation material\n\n    for i, prob in enumerate(remaining):\n        # Dynamic arm addition: every ARM_ADD_INTERVAL bandit-phase problems\n        if i > 0 and i % ARM_ADD_INTERVAL == 0:\n            # Find the best arm by posterior mean\n            posterior_means = [a / (a + b) for a, b in zip(alphas, betas_param)]\n            best_arm = int(np.argmax(posterior_means))\n            best_pb = pool[best_arm]\n\n            # Use the most recent reflection from the best arm (or any recent one)\n            if best_arm in last_reflections:\n                ref_text, ref_q = last_reflections[best_arm]\n            else:\n                # Fallback: use most recent reflection from any arm\n                ref_text, ref_q = next(iter(last_reflections.values()), reflections[-1])\n\n            new_variant = curate(best_pb, ref_text, ref_q)\n            pool.append(new_variant)\n            alphas.append(1.0)\n            betas_param.append(1.0)\n            print(f\"  Thompson-Dyn: added arm {len(pool)-1} (curated from best arm {best_arm}, \"\n                  f\"posterior={posterior_means[best_arm]:.2f})\")\n\n        # Thompson sample\n        ts_samples = [np.random.beta(a, b) for a, b in zip(alphas, betas_param)]\n        chosen = int(np.argmax(ts_samples))\n        pb = pool[chosen]\n\n        answer, bullets_used, raw = generate(prob[\"question\"], pb)\n        correct = answers_match(answer, prob[\"answer\"])\n        log.correct.append(correct)\n        log.playbook_sizes.append(pb.size)\n\n        reflection, tags = reflect(\n            prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, pb\n        )\n        for bid, label in tags.items():\n            pb.tag(bid, label)\n\n        # Store reflection for potential future curation\n        last_reflections[chosen] = (reflection, prob[\"question\"])\n\n        # Update Beta posterior\n        if correct:\n            alphas[chosen] += 1.0\n        else:\n            betas_param[chosen] += 1.0\n\n        if (THOMPSON_DYN_SEED_PROBLEMS + i + 1) % 10 == 0:\n            acc = sum(log.correct) / len(log.correct)\n            pulls = [int(a + b - 2) for a, b in zip(alphas, betas_param)]\n            posterior_means = [a / (a + b) for a, b in zip(alphas, betas_param)]\n            best_var = int(np.argmax(posterior_means))\n            print(f\"  Thompson-Dyn [{THOMPSON_DYN_SEED_PROBLEMS + i + 1}/{len(problems)}] \"\n                  f\"acc={acc:.2%} arms={len(pool)} pulls={pulls} best=variant-{best_var}\")\n\n    log.call_counts = get_call_counts()\n    posterior_means = [a / (a + b) for a, b in zip(alphas, betas_param)]\n    best_idx = int(np.argmax(posterior_means))\n    log.final_playbook = pool[best_idx]\n    return log",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Strategy 4: Majority Vote (no evolution baseline) ---\n",
    "\n",
    "MAJORITY_N_SAMPLES = 2  # generate N answers per problem, take majority\n",
    "\n",
    "def run_majority_vote(problems: List[dict]) -> RunLog:\n",
    "    \"\"\"No playbook evolution. Static initial playbook. Generate N answers per problem,\n",
    "    take majority vote. Tests whether sampling diversity beats playbook evolution.\n",
    "\n",
    "    Budget: 50 * 2 = 100 generate calls. No reflect or curate.\n",
    "    \"\"\"\n",
    "    reset_call_counter()\n",
    "    log = RunLog()\n",
    "    pb = make_initial_playbook()\n",
    "\n",
    "    for i, prob in enumerate(problems):\n",
    "        answers = []\n",
    "        for _ in range(MAJORITY_N_SAMPLES):\n",
    "            answer, _, _ = generate(prob[\"question\"], pb)\n",
    "            answers.append(answer)\n",
    "\n",
    "        # Majority vote — pick most common answer\n",
    "        vote_counts = Counter(answers)\n",
    "        majority_answer = vote_counts.most_common(1)[0][0]\n",
    "        correct = answers_match(majority_answer, prob[\"answer\"])\n",
    "        log.correct.append(correct)\n",
    "        log.playbook_sizes.append(pb.size)\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            acc = sum(log.correct) / len(log.correct)\n",
    "            print(f\"  MajVote [{i+1}/{len(problems)}] acc={acc:.2%}\")\n",
    "\n",
    "    log.call_counts = get_call_counts()\n",
    "    log.final_playbook = pb\n",
    "    return log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Running Majority Vote (no evolution baseline)...\")\n",
    "print(\"=\" * 60)\n",
    "majority_log = run_majority_vote(problems)\n",
    "print(f\"MajVote done. Final acc (last 20): {majority_log.final_accuracy:.2%}\")\n",
    "print(f\"Calls: {majority_log.call_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Running Greedy ACE (baseline)...\")\n",
    "print(\"=\" * 60)\n",
    "greedy_log = run_greedy(problems)\n",
    "print(f\"Greedy done. Final acc (last 20): {greedy_log.final_accuracy:.2%}\")\n",
    "print(f\"Calls: {greedy_log.call_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Running Thompson Sampling...\")\n",
    "print(\"=\" * 60)\n",
    "thompson_log = run_thompson(problems)\n",
    "print(f\"Thompson done. Final acc (last 20): {thompson_log.final_accuracy:.2%}\")\n",
    "print(f\"Calls: {thompson_log.call_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 60)\nprint(\"Running Dynamic Thompson Sampling...\")\nprint(\"=\" * 60)\nthompson_dyn_log = run_thompson_dynamic(problems)\nprint(f\"Thompson-Dyn done. Final acc (last 20): {thompson_dyn_log.final_accuracy:.2%}\")\nprint(f\"Calls: {thompson_dyn_log.call_counts}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puct_logs = {}\n",
    "for q_mode in [\"mean\", \"ema\", \"bayesian\"]:\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Running PUCT-{q_mode.upper()}...\")\n",
    "    print(\"=\" * 60)\n",
    "    log = run_puct(problems, q_mode=q_mode)\n",
    "    puct_logs[q_mode] = log\n",
    "    print(f\"PUCT-{q_mode} done. Final acc (last 20): {log.final_accuracy:.2%}\")\n",
    "    print(f\"Calls: {log.call_counts}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analysis & Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build results dict with all 7 conditions\nresults = {\n    \"Majority Vote\": majority_log,\n    \"Greedy ACE\": greedy_log,\n    \"Thompson\": thompson_log,\n    \"Thompson-Dyn\": thompson_dyn_log,\n}\nfor q_mode, log in puct_logs.items():\n    results[f\"PUCT-{q_mode.upper()}\"] = log\n\nCOLORS = {\n    \"Majority Vote\": \"#9467bd\",\n    \"Greedy ACE\": \"#1f77b4\",\n    \"Thompson\": \"#8c564b\",\n    \"Thompson-Dyn\": \"#e377c2\",\n    \"PUCT-MEAN\": \"#ff7f0e\",\n    \"PUCT-EMA\": \"#2ca02c\",\n    \"PUCT-BAYESIAN\": \"#d62728\",\n}\n\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\nfig.suptitle(\"ACE Search Strategy Comparison (GSM8K, 50 problems)\", fontsize=14, fontweight=\"bold\")\n\n# --- (a) Running accuracy curves ---\nax = axes[0, 0]\nfor name, log in results.items():\n    acc = log.running_accuracy\n    ax.plot(range(1, len(acc) + 1), acc, label=name, color=COLORS[name], linewidth=2)\nax.set_xlabel(\"Problems Solved\")\nax.set_ylabel(\"Running Accuracy\")\nax.set_title(\"(a) Running Accuracy Over Time\")\nax.legend(fontsize=7, loc=\"lower right\")\nax.grid(True, alpha=0.3)\nax.set_ylim(0, 1)\n\n# --- (b) Final accuracy bar chart with bootstrap CI ---\nax = axes[0, 1]\n\ndef bootstrap_ci(data, n_boot=1000, ci=0.95):\n    data = np.array(data, dtype=float)\n    means = [np.mean(np.random.choice(data, size=len(data), replace=True)) for _ in range(n_boot)]\n    lo = np.percentile(means, (1 - ci) / 2 * 100)\n    hi = np.percentile(means, (1 + ci) / 2 * 100)\n    return np.mean(data), lo, hi\n\nnames = list(results.keys())\nmeans, lows, highs = [], [], []\nfor name in names:\n    tail = results[name].correct[-20:]\n    m, lo, hi = bootstrap_ci(tail)\n    means.append(m)\n    lows.append(m - lo)\n    highs.append(hi - m)\n\nbars = ax.bar(names, means, color=[COLORS[n] for n in names], yerr=[lows, highs], capsize=4)\nax.set_ylabel(\"Accuracy (last 20 problems)\")\nax.set_title(\"(b) Final Accuracy Comparison\")\nax.set_ylim(0, 1)\nax.tick_params(axis='x', rotation=30, labelsize=7)\nfor bar, m in zip(bars, means):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f\"{m:.0%}\",\n            ha=\"center\", fontsize=8)\n\n# --- (c) Playbook size over time ---\nax = axes[1, 0]\nfor name, log in results.items():\n    sizes = log.playbook_sizes\n    ax.plot(range(1, len(sizes) + 1), sizes, label=name, color=COLORS[name], linewidth=2)\nax.set_xlabel(\"Problems Solved\")\nax.set_ylabel(\"Playbook Bullets\")\nax.set_title(\"(c) Playbook Size Over Time\")\nax.legend(fontsize=7)\nax.grid(True, alpha=0.3)\n\n# --- (d) Cost breakdown ---\nax = axes[1, 1]\nroles = [\"generate\", \"reflect\", \"curate\"]\nx = np.arange(len(names))\nwidth = 0.22\nfor i, role in enumerate(roles):\n    counts = [results[n].call_counts.get(role, 0) for n in names]\n    ax.bar(x + i * width, counts, width, label=role.capitalize())\nax.set_xticks(x + width)\nax.set_xticklabels(names, fontsize=6, rotation=30)\nax.set_ylabel(\"LLM Calls\")\nax.set_title(\"(d) LLM Call Breakdown\")\nax.legend()\n\nplt.tight_layout()\nplt.savefig(\"search_ace_results.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()\nprint(\"Plots saved to search_ace_results.png\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Statistical comparison ---\n\nprint(\"=\" * 60)\nprint(\"Statistical Comparison (Bootstrap 95% CI on accuracy difference vs Greedy)\")\nprint(\"=\" * 60)\n\ndef bootstrap_diff_ci(a, b, n_boot=2000):\n    a, b = np.array(a, dtype=float), np.array(b, dtype=float)\n    diffs = []\n    for _ in range(n_boot):\n        ia = np.random.choice(len(a), len(a), replace=True)\n        ib = np.random.choice(len(b), len(b), replace=True)\n        diffs.append(np.mean(b[ib]) - np.mean(a[ia]))\n    lo, hi = np.percentile(diffs, [2.5, 97.5])\n    return np.mean(diffs), lo, hi\n\ngreedy_tail = greedy_log.correct[-20:]\n\n# Each condition vs Greedy\ncomparisons = [\n    (\"Majority Vote\", majority_log.correct[-20:]),\n    (\"Thompson\", thompson_log.correct[-20:]),\n    (\"Thompson-Dyn\", thompson_dyn_log.correct[-20:]),\n]\nfor q_mode in [\"mean\", \"ema\", \"bayesian\"]:\n    comparisons.append((f\"PUCT-{q_mode.upper()}\", puct_logs[q_mode].correct[-20:]))\n\nfor name, tail in comparisons:\n    mean_diff, lo, hi = bootstrap_diff_ci(greedy_tail, tail)\n    sig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\n    print(f\"{name} vs Greedy: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")\n\n# Thompson-Dyn vs static Thompson\nprint()\nt_tail = thompson_log.correct[-20:]\ntd_tail = thompson_dyn_log.correct[-20:]\nmean_diff, lo, hi = bootstrap_diff_ci(t_tail, td_tail)\nsig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\nprint(f\"Thompson-Dyn vs Thompson: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")\n\n# Thompson vs best PUCT\nprint()\nfor q_mode in [\"mean\", \"ema\", \"bayesian\"]:\n    t_tail = thompson_log.correct[-20:]\n    p_tail = puct_logs[q_mode].correct[-20:]\n    mean_diff, lo, hi = bootstrap_diff_ci(t_tail, p_tail)\n    sig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\n    print(f\"PUCT-{q_mode.upper()} vs Thompson: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Results Summary\n\n### Experiment Design\n- **Dataset**: GSM8K (50 problems, shuffled, seed=42)\n- **Model**: Qwen2.5-7B-Instruct (local via vLLM)\n- **7 conditions** spanning no-evolution → flat bandit → dynamic bandit → sequential → tree search\n\n### Budget Comparison\n| Condition | Generate | Reflect | Curate | Total |\n|-----------|----------|---------|--------|-------|\n| Majority Vote | 100 | 0 | 0 | 100 |\n| Greedy ACE | 50 | 50 | 10 | 110 |\n| Thompson | 50 | 50 | 5 | 105 |\n| Thompson-Dyn | 50 | 50 | ~9 | ~109 |\n| PUCT-* | 50 | 50 | ≤17 | ≤117 |\n\n### Strategy Taxonomy\n| Strategy | Search Type | Evolves Playbook? | Exploration Mechanism |\n|----------|------------|-------------------|----------------------|\n| Majority Vote | None | No | Sample diversity only |\n| Greedy ACE | Sequential | Yes | None (greedy) |\n| Thompson | Flat bandit | Pool of variants (fixed) | Beta posterior sampling |\n| Thompson-Dyn | Flat bandit | Pool grows dynamically | Beta posterior sampling + periodic arm addition |\n| PUCT-Mean | Tree | Yes (progressive) | UCB with mean Q |\n| PUCT-EMA | Tree | Yes (progressive) | UCB with recency-weighted Q |\n| PUCT-Bayesian | Tree | Yes (progressive) | UCB with shrinkage-to-0.5 Q |\n\n### Key Questions\n1. **Does evolution help at all?** Majority Vote vs Greedy — if MajVote wins, sampling > evolution\n2. **Flat vs tree?** Thompson vs PUCT — is tree structure worth the overhead at 50 problems?\n3. **Does dynamic pool help?** Thompson-Dyn vs Thompson — does periodic arm addition from the best arm improve over a frozen pool?\n4. **Which Q estimator?** Mean vs EMA vs Bayesian — does exploration strategy matter for PUCT?\n5. **Free lunch?** Does any strategy consistently beat Greedy with matched budget?\n\n### Limitations\n- 50 problems is small; need 200+ for reliable conclusions\n- Single seed; should repeat with 5+ seeds\n- PUCT progressive widening alpha=0.5 not tuned\n- Majority Vote uses no reflection budget — could be reallocated\n- Thompson-Dyn ARM_ADD_INTERVAL=10 not tuned; optimal frequency unknown"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cleanup: kill vLLM server ---\n",
    "try:\n",
    "    os.kill(vllm_proc.pid, signal.SIGTERM)\n",
    "    print(f\"Killed vLLM server (PID {vllm_proc.pid})\")\n",
    "except ProcessLookupError:\n",
    "    print(\"vLLM server already stopped\")"
   ]
  }
 ]
}