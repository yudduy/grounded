{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkF8G-LlVxsu"
   },
   "source": [
    "# Search-Augmented ACE: Search Strategy Comparison\n",
    "\n",
    "**Hypothesis**: Different search strategies over ACE-style playbook space trade off exploration, exploitation, and efficiency differently. We compare fourteen conditions spanning no-evolution baselines through flat bandits to tree search.\n",
    "\n",
    "**Fourteen conditions** (matched LLM budget ~100-120 calls):\n",
    "1. **Majority Vote** — no evolution, 2 samples per problem, majority answer (null hypothesis)\n",
    "2. **Best-of-N** — no evolution, N samples per problem, pick highest-confidence answer (rejection sampling baseline)\n",
    "3. **Greedy ACE** — sequential generate -> reflect -> curate (standard ACE baseline)\n",
    "4. **Thompson Sampling** — flat bandit over a pool of curated playbook variants\n",
    "5. **UCB Bandit** — flat bandit with deterministic UCB1 selection (vs Thompson's stochastic sampling)\n",
    "6. **PUCT-Mean** — tree search, Q = mean reward\n",
    "7. **PUCT-EMA** — tree search, Q = exponential moving average (alpha=0.4)\n",
    "8. **PUCT-Bayesian** — tree search, Q = Beta posterior mean\n",
    "9. **PUCT-Variance** — tree search, Q = mean + c*sqrt(var/n) (arXiv:2512.21648)\n",
    "10. **Beam Search** — width-K beam over playbook variants, prune to top-K each round\n",
    "11. **Dynamic Thompson** — Thompson with periodic arm addition from best arm\n",
    "12. **AB-MCTS** — Adaptive Progressive Widening: Thompson-sampled wider-vs-deeper per node\n",
    "13. **Thompson-Disc** — Discounted Thompson Sampling (gamma=0.95) for non-stationary playbook performance\n",
    "14. **Discounted MCTS** — PUCT tree search with gamma=0.95 discounted Q-values for non-stationary nodes\n",
    "\n",
    "**Setup**: Qwen2.5-7B-Instruct via vLLM on A100 (bfloat16, prefix caching, async parallel eval).\n",
    "\n",
    "**GPU Optimization**: All conditions use AsyncOpenAI with semaphore-based concurrency (64 concurrent requests). Majority Vote fires all 100 calls in parallel. Greedy ACE batches 5 generate+reflect calls between curate intervals. PUCT uses virtual loss for parallel multi-leaf evaluation. vLLM configured with `--enable-prefix-caching` (playbook system prompts shared across problems, ~13% throughput gain)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-EvS5DPVxsv"
   },
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqksAzKtVxsv"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install vllm==0.6.6 openai==1.58.1 datasets==3.2.0 matplotlib==3.9.3 numpy==1.26.4 nest_asyncio==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WDfHkOJCVxsw"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import signal\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "\n",
    "# Allow nested event loops (required for Colab/Jupyter)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Checkpoint directory\n",
    "CHECKPOINT_DIR = Path(\"checkpoints\")\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Concurrency for async LLM calls (tuned for 7B on A100 40GB)\n",
    "MAX_CONCURRENT_LLM = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2HcZSPBgVxsw"
   },
   "outputs": [],
   "source": [
    "# Launch vLLM server in background — optimized for A100\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "VLLM_PORT = 8000\n",
    "\n",
    "vllm_proc = subprocess.Popen(\n",
    "    [\n",
    "        \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
    "        \"--model\", MODEL_NAME,\n",
    "        \"--port\", str(VLLM_PORT),\n",
    "        \"--max-model-len\", \"8192\",\n",
    "        \"--gpu-memory-utilization\", \"0.95\",\n",
    "        \"--dtype\", \"bfloat16\",\n",
    "        \"--max-num-seqs\", \"1024\",\n",
    "        \"--max-num-batched-tokens\", \"16384\",\n",
    "        \"--enable-prefix-caching\",\n",
    "    ],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    ")\n",
    "print(f\"vLLM server PID: {vllm_proc.pid}\")\n",
    "\n",
    "# Wait for server to be ready\n",
    "client = OpenAI(base_url=f\"http://localhost:{VLLM_PORT}/v1\", api_key=\"dummy\")\n",
    "aclient = AsyncOpenAI(base_url=f\"http://localhost:{VLLM_PORT}/v1\", api_key=\"dummy\")\n",
    "\n",
    "for attempt in range(120):\n",
    "    try:\n",
    "        client.models.list()\n",
    "        print(f\"vLLM ready after {attempt + 1}s\")\n",
    "        break\n",
    "    except Exception:\n",
    "        time.sleep(1)\n",
    "else:\n",
    "    raise RuntimeError(\"vLLM server did not start within 120s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umqE6p5vVxsw"
   },
   "source": [
    "## 2. GSM8K Data Loading & Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K7GSE4bvVxsw"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "\n",
    "def extract_gsm8k_answer(answer_text: str) -> str:\n",
    "    \"\"\"Extract numeric answer from GSM8K '#### <number>' format.\"\"\"\n",
    "    match = re.search(r\"####\\s*(-?[\\d,]+\\.?\\d*)\", answer_text)\n",
    "    if match:\n",
    "        return match.group(1).replace(\",\", \"\").strip()\n",
    "    # Fallback: last number in text\n",
    "    nums = re.findall(r\"-?[\\d,]+\\.?\\d*\", answer_text)\n",
    "    if nums:\n",
    "        return nums[-1].replace(\",\", \"\")\n",
    "    return \"\"\n",
    "\n",
    "problems = []\n",
    "for item in ds:\n",
    "    problems.append({\n",
    "        \"question\": item[\"question\"],\n",
    "        \"answer\": extract_gsm8k_answer(item[\"answer\"]),\n",
    "        \"full_answer\": item[\"answer\"],\n",
    "    })\n",
    "\n",
    "# Use first 50 problems, shuffled deterministically\n",
    "rng = random.Random(SEED)\n",
    "rng.shuffle(problems)\n",
    "problems = problems[:50]\n",
    "\n",
    "# --- Data validation ---\n",
    "assert len(problems) == 50, f\"Expected 50 problems, got {len(problems)}\"\n",
    "assert all(p[\"answer\"] for p in problems), \"Found empty ground truth answers\"\n",
    "assert all(p[\"question\"].strip() for p in problems), \"Found empty questions\"\n",
    "for p in problems:\n",
    "    try:\n",
    "        float(p[\"answer\"].replace(\",\", \"\"))\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Non-numeric ground truth: {p['answer']}\")\n",
    "\n",
    "print(f\"Loaded {len(problems)} GSM8K problems (all validated)\")\n",
    "print(f\"Example: Q='{problems[0]['question'][:80]}...' A={problems[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmyR72_GVxsw"
   },
   "source": [
    "## 3. Core Components: Playbook, Generator, Reflector, Curator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iq2yIhM3Vxsw"
   },
   "outputs": [],
   "source": [
    "# --- Playbook representation ---\n",
    "\n",
    "@dataclass\n",
    "class Bullet:\n",
    "    id: str\n",
    "    section: str  # STRATEGIES, COMMON_MISTAKES, SOLUTION_PATTERNS\n",
    "    content: str\n",
    "    helpful: int = 0\n",
    "    harmful: int = 0\n",
    "\n",
    "    def to_str(self) -> str:\n",
    "        return f\"[{self.id}] helpful={self.helpful} harmful={self.harmful} :: {self.content}\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Playbook:\n",
    "    bullets: List[Bullet] = field(default_factory=list)\n",
    "    _next_id: int = 1\n",
    "\n",
    "    def add(self, section: str, content: str) -> str:\n",
    "        prefix = {\"STRATEGIES\": \"str\", \"COMMON_MISTAKES\": \"err\", \"SOLUTION_PATTERNS\": \"sol\"}.get(section, \"gen\")\n",
    "        bid = f\"{prefix}-{self._next_id:05d}\"\n",
    "        self._next_id += 1\n",
    "        self.bullets.append(Bullet(id=bid, section=section, content=content))\n",
    "        return bid\n",
    "\n",
    "    def remove(self, bid: str):\n",
    "        self.bullets = [b for b in self.bullets if b.id != bid]\n",
    "\n",
    "    def update(self, bid: str, content: str):\n",
    "        for b in self.bullets:\n",
    "            if b.id == bid:\n",
    "                b.content = content\n",
    "                return\n",
    "\n",
    "    def tag(self, bid: str, label: str):\n",
    "        for b in self.bullets:\n",
    "            if b.id == bid:\n",
    "                if label == \"helpful\":\n",
    "                    b.helpful += 1\n",
    "                elif label == \"harmful\":\n",
    "                    b.harmful += 1\n",
    "\n",
    "    def to_str(self) -> str:\n",
    "        sections = defaultdict(list)\n",
    "        for b in self.bullets:\n",
    "            sections[b.section].append(b.to_str())\n",
    "        parts = []\n",
    "        for sec in [\"STRATEGIES\", \"COMMON_MISTAKES\", \"SOLUTION_PATTERNS\"]:\n",
    "            if sections[sec]:\n",
    "                parts.append(f\"## {sec}\")\n",
    "                parts.extend(sections[sec])\n",
    "        return \"\\n\".join(parts) if parts else \"(empty playbook)\"\n",
    "\n",
    "    def copy(self) -> \"Playbook\":\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return len(self.bullets)\n",
    "\n",
    "\n",
    "def make_initial_playbook() -> Playbook:\n",
    "    pb = Playbook()\n",
    "    pb.add(\"STRATEGIES\", \"Break word problems into step-by-step arithmetic.\")\n",
    "    pb.add(\"STRATEGIES\", \"Identify what quantity the question asks for before computing.\")\n",
    "    pb.add(\"COMMON_MISTAKES\", \"Watch for unit conversions (hours to minutes, etc).\")\n",
    "    return pb\n",
    "\n",
    "print(make_initial_playbook().to_str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0gx5qAVYVxsw"
   },
   "outputs": [],
   "source": [
    "# --- LLM call wrappers (sync + async) ---\n",
    "\n",
    "call_counter = defaultdict(int)  # track calls by role\n",
    "_llm_semaphore = asyncio.Semaphore(MAX_CONCURRENT_LLM)\n",
    "\n",
    "def llm_call(system: str, user: str, role: str = \"generate\", temperature: float = 0.7, max_tokens: int = 512) -> str:\n",
    "    \"\"\"Single synchronous LLM call via vLLM OpenAI-compatible API.\"\"\"\n",
    "    call_counter[role] += 1\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": user},\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"LLM call failed ({role}): {e}\")\n",
    "        return \"\"\n",
    "\n",
    "async def llm_call_async(system: str, user: str, role: str = \"generate\",\n",
    "                         temperature: float = 0.7, max_tokens: int = 512) -> str:\n",
    "    \"\"\"Async LLM call with semaphore-based concurrency control.\"\"\"\n",
    "    call_counter[role] += 1\n",
    "    async with _llm_semaphore:\n",
    "        try:\n",
    "            resp = await aclient.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": user},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "            return resp.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Async LLM call failed ({role}): {e}\")\n",
    "            return \"\"\n",
    "\n",
    "def reset_call_counter():\n",
    "    global call_counter\n",
    "    call_counter = defaultdict(int)\n",
    "\n",
    "def get_call_counts() -> Dict[str, int]:\n",
    "    return dict(call_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INFiB1KrVxsx"
   },
   "outputs": [],
   "source": [
    "# --- Generator (sync + async) ---\n",
    "\n",
    "def _build_generate_prompts(question: str, playbook: Playbook) -> Tuple[str, str]:\n",
    "    \"\"\"Build system/user prompts for generation.\"\"\"\n",
    "    pb_text = playbook.to_str()\n",
    "    system = (\n",
    "        \"You are a math problem solver. Use the playbook strategies below to help solve the problem.\\n\"\n",
    "        \"When you use a specific strategy, mention its ID (e.g., [str-00001]).\\n\"\n",
    "        \"Show your work step-by-step, then give the final numeric answer on its own line as: #### <number>\\n\\n\"\n",
    "        f\"PLAYBOOK:\\n{pb_text}\"\n",
    "    )\n",
    "    user = f\"Solve this problem:\\n{question}\"\n",
    "    return system, user\n",
    "\n",
    "def _parse_generate_response(raw: str) -> Tuple[str, List[str]]:\n",
    "    \"\"\"Extract answer and bullet references from raw LLM response.\"\"\"\n",
    "    answer = \"\"\n",
    "    m = re.search(r\"####\\s*(-?[\\d,]+\\.?\\d*)\", raw)\n",
    "    if m:\n",
    "        answer = m.group(1).replace(\",\", \"\").strip()\n",
    "    else:\n",
    "        nums = re.findall(r\"-?[\\d,]+\\.?\\d*\", raw)\n",
    "        if nums:\n",
    "            answer = nums[-1].replace(\",\", \"\")\n",
    "    bullets_used = re.findall(r\"\\[(\\w+-\\d+)\\]\", raw)\n",
    "    return answer, bullets_used\n",
    "\n",
    "def generate(question: str, playbook: Playbook) -> Tuple[str, List[str], str]:\n",
    "    \"\"\"Generate a solution to a math problem using the playbook (sync).\"\"\"\n",
    "    system, user = _build_generate_prompts(question, playbook)\n",
    "    raw = llm_call(system, user, role=\"generate\")\n",
    "    answer, bullets_used = _parse_generate_response(raw)\n",
    "    return answer, bullets_used, raw\n",
    "\n",
    "async def generate_async(question: str, playbook: Playbook) -> Tuple[str, List[str], str]:\n",
    "    \"\"\"Generate a solution to a math problem using the playbook (async).\"\"\"\n",
    "    system, user = _build_generate_prompts(question, playbook)\n",
    "    raw = await llm_call_async(system, user, role=\"generate\")\n",
    "    answer, bullets_used = _parse_generate_response(raw)\n",
    "    return answer, bullets_used, raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7VZ90t3Vxsx"
   },
   "outputs": [],
   "source": [
    "# --- Answer comparison ---\n",
    "\n",
    "def answers_match(predicted: str, ground_truth: str) -> bool:\n",
    "    \"\"\"Compare numeric answers with tolerance.\"\"\"\n",
    "    try:\n",
    "        p = float(predicted.replace(\",\", \"\"))\n",
    "        g = float(ground_truth.replace(\",\", \"\"))\n",
    "        return abs(p - g) < 1e-3\n",
    "    except (ValueError, TypeError):\n",
    "        return predicted.strip() == ground_truth.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKRsvPyLVxsx"
   },
   "outputs": [],
   "source": [
    "# --- Reflector (sync + async) ---\n",
    "\n",
    "def _build_reflect_prompts(question: str, raw_response: str, predicted: str, ground_truth: str,\n",
    "                           bullets_used: List[str], playbook: Playbook) -> Tuple[str, str]:\n",
    "    \"\"\"Build system/user prompts for reflection.\"\"\"\n",
    "    correct = answers_match(predicted, ground_truth)\n",
    "    feedback = \"CORRECT\" if correct else f\"INCORRECT (predicted {predicted}, expected {ground_truth})\"\n",
    "\n",
    "    bullets_text = \"\"\n",
    "    for b in playbook.bullets:\n",
    "        if b.id in bullets_used:\n",
    "            bullets_text += f\"  {b.to_str()}\\n\"\n",
    "\n",
    "    system = (\n",
    "        \"You are a math reasoning analyst. Analyze whether the solution approach was correct \"\n",
    "        \"and whether the playbook strategies used were helpful or harmful.\\n\"\n",
    "        \"For each bullet ID used, output a JSON line: {\\\"id\\\": \\\"str-00001\\\", \\\"tag\\\": \\\"helpful\\\"}\\n\"\n",
    "        \"Tags must be one of: helpful, harmful, neutral.\\n\"\n",
    "        \"End with a brief reflection paragraph.\"\n",
    "    )\n",
    "    user = (\n",
    "        f\"Problem: {question}\\n\\n\"\n",
    "        f\"Solution attempt:\\n{raw_response}\\n\\n\"\n",
    "        f\"Result: {feedback}\\n\\n\"\n",
    "        f\"Playbook bullets used:\\n{bullets_text}\"\n",
    "    )\n",
    "    return system, user\n",
    "\n",
    "def _parse_reflect_response(raw: str, bullets_used: List[str], correct: bool) -> Dict[str, str]:\n",
    "    \"\"\"Parse bullet tags from reflection response.\"\"\"\n",
    "    tags = {}\n",
    "    for m in re.finditer(r'\"id\"\\s*:\\s*\"([^\"]+)\".*?\"tag\"\\s*:\\s*\"(helpful|harmful|neutral)\"', raw):\n",
    "        tags[m.group(1)] = m.group(2)\n",
    "    if not tags:\n",
    "        for bid in bullets_used:\n",
    "            tags[bid] = \"helpful\" if correct else \"neutral\"\n",
    "    return tags\n",
    "\n",
    "def reflect(question: str, raw_response: str, predicted: str, ground_truth: str,\n",
    "            bullets_used: List[str], playbook: Playbook) -> Tuple[str, Dict[str, str]]:\n",
    "    \"\"\"Reflect on the solution attempt (sync).\"\"\"\n",
    "    system, user = _build_reflect_prompts(question, raw_response, predicted, ground_truth, bullets_used, playbook)\n",
    "    raw = llm_call(system, user, role=\"reflect\", temperature=0.3)\n",
    "    correct = answers_match(predicted, ground_truth)\n",
    "    tags = _parse_reflect_response(raw, bullets_used, correct)\n",
    "    return raw, tags\n",
    "\n",
    "async def reflect_async(question: str, raw_response: str, predicted: str, ground_truth: str,\n",
    "                        bullets_used: List[str], playbook: Playbook) -> Tuple[str, Dict[str, str]]:\n",
    "    \"\"\"Reflect on the solution attempt (async).\"\"\"\n",
    "    system, user = _build_reflect_prompts(question, raw_response, predicted, ground_truth, bullets_used, playbook)\n",
    "    raw = await llm_call_async(system, user, role=\"reflect\", temperature=0.3)\n",
    "    correct = answers_match(predicted, ground_truth)\n",
    "    tags = _parse_reflect_response(raw, bullets_used, correct)\n",
    "    return raw, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqMIjkHBVxsx"
   },
   "outputs": [],
   "source": [
    "# --- Curator ---\n",
    "\n",
    "MAX_BULLETS = 20\n",
    "\n",
    "def curate(playbook: Playbook, reflection: str, question: str) -> Playbook:\n",
    "    \"\"\"\n",
    "    Curate the playbook based on reflection.\n",
    "    Returns a new (copied) playbook with operations applied.\n",
    "    \"\"\"\n",
    "    pb = playbook.copy()\n",
    "    pb_text = pb.to_str()\n",
    "\n",
    "    system = (\n",
    "        \"You are a playbook curator for math problem solving. Based on the reflection, \"\n",
    "        \"propose operations to improve the playbook.\\n\"\n",
    "        \"Output a JSON array of operations:\\n\"\n",
    "        '[{\"op\": \"ADD\", \"section\": \"STRATEGIES\", \"content\": \"new insight\"},\\n'\n",
    "        ' {\"op\": \"UPDATE\", \"id\": \"str-00001\", \"content\": \"refined text\"},\\n'\n",
    "        ' {\"op\": \"DELETE\", \"id\": \"err-00002\"}]\\n'\n",
    "        f\"Sections: STRATEGIES, COMMON_MISTAKES, SOLUTION_PATTERNS\\n\"\n",
    "        f\"Max bullets: {MAX_BULLETS}. Current: {pb.size}.\\n\"\n",
    "        \"Only propose operations that are clearly supported by the reflection. Keep it minimal.\"\n",
    "    )\n",
    "    user = (\n",
    "        f\"Current playbook:\\n{pb_text}\\n\\n\"\n",
    "        f\"Problem context: {question[:200]}\\n\\n\"\n",
    "        f\"Reflection:\\n{reflection}\"\n",
    "    )\n",
    "    raw = llm_call(system, user, role=\"curate\", temperature=0.3)\n",
    "\n",
    "    # Parse operations from JSON array\n",
    "    ops = []\n",
    "    # Try to find JSON array in response\n",
    "    json_match = re.search(r'\\[\\s*\\{.*?\\}\\s*\\]', raw, re.DOTALL)\n",
    "    if json_match:\n",
    "        try:\n",
    "            ops = json.loads(json_match.group())\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "    # Apply operations\n",
    "    for op in ops:\n",
    "        try:\n",
    "            if op.get(\"op\") == \"ADD\" and pb.size < MAX_BULLETS:\n",
    "                pb.add(op.get(\"section\", \"STRATEGIES\"), op.get(\"content\", \"\"))\n",
    "            elif op.get(\"op\") == \"UPDATE\" and op.get(\"id\"):\n",
    "                pb.update(op[\"id\"], op.get(\"content\", \"\"))\n",
    "            elif op.get(\"op\") == \"DELETE\" and op.get(\"id\"):\n",
    "                pb.remove(op[\"id\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Safety: if curator emptied the playbook, reset\n",
    "    if pb.size == 0:\n",
    "        pb = make_initial_playbook()\n",
    "        pb._next_id = playbook._next_id\n",
    "\n",
    "    return pb\n",
    "\n",
    "\n",
    "def _apply_curate_ops(pb, raw, original_playbook):\n",
    "    \"\"\"Shared logic for parsing and applying curator operations.\"\"\"\n",
    "    ops = []\n",
    "    json_match = re.search(r'\\[\\s*\\{.*?\\}\\s*\\]', raw, re.DOTALL)\n",
    "    if json_match:\n",
    "        try:\n",
    "            ops = json.loads(json_match.group())\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    for op in ops:\n",
    "        try:\n",
    "            if op.get(\"op\") == \"ADD\" and pb.size < MAX_BULLETS:\n",
    "                pb.add(op.get(\"section\", \"STRATEGIES\"), op.get(\"content\", \"\"))\n",
    "            elif op.get(\"op\") == \"UPDATE\" and op.get(\"id\"):\n",
    "                pb.update(op[\"id\"], op.get(\"content\", \"\"))\n",
    "            elif op.get(\"op\") == \"DELETE\" and op.get(\"id\"):\n",
    "                pb.remove(op[\"id\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "    if pb.size == 0:\n",
    "        pb = make_initial_playbook()\n",
    "        pb._next_id = original_playbook._next_id\n",
    "    return pb\n",
    "\n",
    "\n",
    "async def curate_async(playbook, reflection, question):\n",
    "    \"\"\"Async version of curate. Uses llm_call_async to avoid blocking the event loop.\"\"\"\n",
    "    pb = playbook.copy()\n",
    "    pb_text = pb.to_str()\n",
    "    system = (\n",
    "        \"You are a playbook curator for math problem solving. Based on the reflection, \"\n",
    "        \"propose operations to improve the playbook.\\n\"\n",
    "        \"Output a JSON array of operations:\\n\"\n",
    "        '[{\"op\": \"ADD\", \"section\": \"STRATEGIES\", \"content\": \"new insight\"},\\n'\n",
    "        ' {\"op\": \"UPDATE\", \"id\": \"str-00001\", \"content\": \"refined text\"},\\n'\n",
    "        ' {\"op\": \"DELETE\", \"id\": \"err-00002\"}]\\n'\n",
    "        f\"Sections: STRATEGIES, COMMON_MISTAKES, SOLUTION_PATTERNS\\n\"\n",
    "        f\"Max bullets: {MAX_BULLETS}. Current: {pb.size}.\\n\"\n",
    "        \"Only propose operations that are clearly supported by the reflection. Keep it minimal.\"\n",
    "    )\n",
    "    user = (\n",
    "        f\"Current playbook:\\n{pb_text}\\n\\n\"\n",
    "        f\"Problem context: {question[:200]}\\n\\n\"\n",
    "        f\"Reflection:\\n{reflection}\"\n",
    "    )\n",
    "    raw = await llm_call_async(system, user, role=\"curate\", temperature=0.3, max_tokens=256)\n",
    "    return _apply_curate_ops(pb, raw, playbook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vunN4RhVVxsx"
   },
   "source": [
    "## 4. Search Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eoL84hplVxsx"
   },
   "outputs": [],
   "source": [
    "# --- Shared tracking ---\n",
    "\n",
    "@dataclass\n",
    "class RunLog:\n",
    "    \"\"\"Tracks per-problem results for a single condition.\"\"\"\n",
    "    correct: List[bool] = field(default_factory=list)\n",
    "    playbook_sizes: List[int] = field(default_factory=list)\n",
    "    call_counts: Dict[str, int] = field(default_factory=dict)\n",
    "    final_playbook: Optional[Playbook] = None\n",
    "\n",
    "    @property\n",
    "    def running_accuracy(self) -> List[float]:\n",
    "        acc = []\n",
    "        total = 0\n",
    "        for i, c in enumerate(self.correct):\n",
    "            total += int(c)\n",
    "            acc.append(total / (i + 1))\n",
    "        return acc\n",
    "\n",
    "    @property\n",
    "    def final_accuracy(self) -> float:\n",
    "        if not self.correct:\n",
    "            return 0.0\n",
    "        tail = self.correct[-20:]\n",
    "        return sum(tail) / len(tail)\n",
    "\n",
    "def save_checkpoint(name: str, log: RunLog):\n",
    "    \"\"\"Save a condition's RunLog to disk for crash recovery.\"\"\"\n",
    "    path = CHECKPOINT_DIR / f\"{name}.pkl\"\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(log, f)\n",
    "    print(f\"  Checkpoint saved: {path}\")\n",
    "\n",
    "def load_checkpoint(name: str) -> Optional[RunLog]:\n",
    "    \"\"\"Load a condition's RunLog from disk if it exists.\"\"\"\n",
    "    path = CHECKPOINT_DIR / f\"{name}.pkl\"\n",
    "    if path.exists():\n",
    "        with open(path, \"rb\") as f:\n",
    "            log = pickle.load(f)\n",
    "        print(f\"  Loaded checkpoint: {path} ({len(log.correct)} problems)\")\n",
    "        return log\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNeMQykWVxsx"
   },
   "outputs": [],
   "source": [
    "# --- Strategy 1: Greedy ACE (async batched within curate intervals) ---\n",
    "\n",
    "CURATE_EVERY = 5\n",
    "\n",
    "def run_greedy(problems: List[dict]) -> RunLog:\n",
    "    \"\"\"Sequential generate -> reflect -> curate loop, with async batching.\n",
    "\n",
    "    Within each curate interval (5 problems), we fire all generate calls in parallel,\n",
    "    then all reflect calls in parallel, then curate once (sync, since it's 1 call).\n",
    "    \"\"\"\n",
    "    reset_call_counter()\n",
    "    log = RunLog()\n",
    "    pb = make_initial_playbook()\n",
    "\n",
    "    async def _run():\n",
    "        nonlocal pb\n",
    "        for batch_start in range(0, len(problems), CURATE_EVERY):\n",
    "            batch = problems[batch_start:batch_start + CURATE_EVERY]\n",
    "\n",
    "            # Parallel generate\n",
    "            gen_tasks = [generate_async(prob[\"question\"], pb) for prob in batch]\n",
    "            gen_results = await asyncio.gather(*gen_tasks)\n",
    "\n",
    "            # Parallel reflect\n",
    "            ref_tasks = []\n",
    "            for prob, (answer, bullets_used, raw) in zip(batch, gen_results):\n",
    "                ref_tasks.append(reflect_async(\n",
    "                    prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, pb\n",
    "                ))\n",
    "            ref_results = await asyncio.gather(*ref_tasks)\n",
    "\n",
    "            # Process results sequentially (tag bullets, log)\n",
    "            last_reflection = \"\"\n",
    "            last_question = \"\"\n",
    "            for prob, (answer, bullets_used, raw), (reflection, tags) in zip(batch, gen_results, ref_results):\n",
    "                correct = answers_match(answer, prob[\"answer\"])\n",
    "                log.correct.append(correct)\n",
    "                log.playbook_sizes.append(pb.size)\n",
    "                for bid, label in tags.items():\n",
    "                    pb.tag(bid, label)\n",
    "                last_reflection = reflection\n",
    "                last_question = prob[\"question\"]\n",
    "\n",
    "            # Curate (single sync call)\n",
    "            if last_reflection:\n",
    "                pb = await curate_async(pb, last_reflection, last_question)\n",
    "\n",
    "            done = batch_start + len(batch)\n",
    "            if done % 10 == 0 or done == len(problems):\n",
    "                acc = sum(log.correct) / len(log.correct)\n",
    "                print(f\"  Greedy [{done}/{len(problems)}] acc={acc:.2%} bullets={pb.size}\")\n",
    "\n",
    "    asyncio.run(_run())\n",
    "    log.call_counts = get_call_counts()\n",
    "    log.final_playbook = pb\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnxrvusCVxsx"
   },
   "outputs": [],
   "source": [
    "# --- Strategy 2: PUCT-ACE with virtual loss, async eval, Q-estimator variants ---\n",
    "\n",
    "@dataclass\n",
    "class MCTSNode:\n",
    "    playbook: Playbook\n",
    "    parent: Optional[\"MCTSNode\"] = None\n",
    "    children: List[\"MCTSNode\"] = field(default_factory=list)\n",
    "    visits: int = 0\n",
    "    reward_history: List[float] = field(default_factory=list)\n",
    "    results: List[bool] = field(default_factory=list)\n",
    "    # Virtual loss for parallel leaf selection\n",
    "    virtual_loss_count: int = 0\n",
    "    # AB-MCTS: Beta posterior for wider-vs-deeper decision\n",
    "    expand_alpha: float = 1.0\n",
    "    expand_beta: float = 1.0\n",
    "\n",
    "    def q_value(self, mode: str = \"mean\") -> float:\n",
    "        if not self.reward_history:\n",
    "            return 0.5  # optimistic prior for unvisited nodes\n",
    "        if mode == \"mean\":\n",
    "            return sum(self.reward_history) / len(self.reward_history)\n",
    "        elif mode == \"ema\":\n",
    "            alpha = 0.4\n",
    "            q = self.reward_history[0]\n",
    "            for r in self.reward_history[1:]:\n",
    "                q = alpha * r + (1 - alpha) * q\n",
    "            return q\n",
    "        elif mode == \"bayesian\":\n",
    "            s = sum(self.reward_history)\n",
    "            n = len(self.reward_history)\n",
    "            return (s + 1) / (n + 2)\n",
    "        elif mode == \"variance\":\n",
    "            # Variance-aware Q (inspired by arXiv:2512.21648)\n",
    "            # UCB-V style: mean + c * sqrt(variance / n)\n",
    "            n = len(self.reward_history)\n",
    "            mean_q = sum(self.reward_history) / n\n",
    "            if n < 2:\n",
    "                return mean_q + 0.5  # high uncertainty bonus when few samples\n",
    "            var_q = sum((r - mean_q) ** 2 for r in self.reward_history) / n\n",
    "            return mean_q + 0.5 * math.sqrt(var_q / n)\n",
    "        return 0.5\n",
    "\n",
    "    def effective_q(self, mode: str = \"mean\") -> float:\n",
    "        \"\"\"Q-value adjusted for virtual losses (used during parallel selection).\"\"\"\n",
    "        if not self.reward_history and self.virtual_loss_count == 0:\n",
    "            return 0.5\n",
    "        effective_n = len(self.reward_history) + self.virtual_loss_count\n",
    "        if effective_n == 0:\n",
    "            return 0.5\n",
    "        real_sum = sum(self.reward_history)\n",
    "        # Virtual losses assume reward of 0\n",
    "        return real_sum / effective_n\n",
    "\n",
    "    @property\n",
    "    def effective_visits(self) -> int:\n",
    "        return self.visits + self.virtual_loss_count\n",
    "\n",
    "    def add_virtual_loss(self):\n",
    "        self.virtual_loss_count += 1\n",
    "\n",
    "    def remove_virtual_loss(self):\n",
    "        self.virtual_loss_count = max(0, self.virtual_loss_count - 1)\n",
    "\n",
    "\n",
    "def puct_select(node: MCTSNode, c_puct: float = 1.5, q_mode: str = \"mean\",\n",
    "                use_virtual_loss: bool = False) -> MCTSNode:\n",
    "    \"\"\"Select best child via PUCT, recurse to leaf.\"\"\"\n",
    "    if not node.children:\n",
    "        return node\n",
    "    n_parent = node.effective_visits if use_virtual_loss else node.visits\n",
    "    best, best_score = None, -float(\"inf\")\n",
    "    for child in node.children:\n",
    "        prior = 1.0 / len(node.children)\n",
    "        if use_virtual_loss:\n",
    "            exploit = child.effective_q(q_mode)\n",
    "            n_child = child.effective_visits\n",
    "        else:\n",
    "            exploit = child.q_value(q_mode)\n",
    "            n_child = child.visits\n",
    "        explore = c_puct * prior * math.sqrt(n_parent) / (1 + n_child)\n",
    "        score = exploit + explore\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best = child\n",
    "    return puct_select(best, c_puct, q_mode, use_virtual_loss)\n",
    "\n",
    "\n",
    "def select_batch_with_virtual_loss(root: MCTSNode, k: int, c_puct: float = 1.5,\n",
    "                                    q_mode: str = \"mean\") -> List[Tuple[MCTSNode, List[MCTSNode]]]:\n",
    "    \"\"\"Select k diverse leaves using virtual loss to discourage repeat selection.\"\"\"\n",
    "    selections = []\n",
    "    for _ in range(k):\n",
    "        node = root\n",
    "        path = []\n",
    "        while node.children:\n",
    "            n_parent = node.effective_visits\n",
    "            best, best_score = None, -float(\"inf\")\n",
    "            for child in node.children:\n",
    "                prior = 1.0 / len(node.children)\n",
    "                exploit = child.effective_q(q_mode)\n",
    "                explore = c_puct * prior * math.sqrt(n_parent) / (1 + child.effective_visits)\n",
    "                score = exploit + explore\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best = child\n",
    "            best.add_virtual_loss()\n",
    "            path.append(best)\n",
    "            node = best\n",
    "        selections.append((node, path))\n",
    "    return selections\n",
    "\n",
    "\n",
    "def backprop(node: MCTSNode, reward: float):\n",
    "    \"\"\"Backpropagate reward up the tree.\"\"\"\n",
    "    while node is not None:\n",
    "        node.visits += 1\n",
    "        node.reward_history.append(reward)\n",
    "        node = node.parent\n",
    "\n",
    "\n",
    "def should_expand(node: MCTSNode, pw_alpha: float = 0.5) -> bool:\n",
    "    \"\"\"Progressive widening: ceil(visits^alpha) children allowed.\"\"\"\n",
    "    max_children = max(1, math.ceil(node.visits ** pw_alpha))\n",
    "    return len(node.children) < max_children\n",
    "\n",
    "\n",
    "async def _eval_leaf_async(leaf: MCTSNode, batch: List[dict]) -> Tuple[List[bool], str, str, Playbook]:\n",
    "    \"\"\"Evaluate a leaf's playbook on a batch of problems asynchronously.\"\"\"\n",
    "    eval_pb = leaf.playbook.copy()\n",
    "\n",
    "    # Parallel generate all problems in batch\n",
    "    gen_tasks = [generate_async(prob[\"question\"], eval_pb) for prob in batch]\n",
    "    gen_results = await asyncio.gather(*gen_tasks)\n",
    "\n",
    "    # Parallel reflect all\n",
    "    ref_tasks = []\n",
    "    for prob, (answer, bullets_used, raw) in zip(batch, gen_results):\n",
    "        ref_tasks.append(reflect_async(\n",
    "            prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, eval_pb\n",
    "        ))\n",
    "    ref_results = await asyncio.gather(*ref_tasks)\n",
    "\n",
    "    # Apply tags\n",
    "    batch_correct = []\n",
    "    last_reflection = \"\"\n",
    "    last_question = \"\"\n",
    "    for prob, (answer, bullets_used, raw), (reflection, tags) in zip(batch, gen_results, ref_results):\n",
    "        correct = answers_match(answer, prob[\"answer\"])\n",
    "        batch_correct.append(correct)\n",
    "        for bid, label in tags.items():\n",
    "            eval_pb.tag(bid, label)\n",
    "        last_reflection = reflection\n",
    "        last_question = prob[\"question\"]\n",
    "\n",
    "    return batch_correct, last_reflection, last_question, eval_pb\n",
    "\n",
    "\n",
    "def run_puct(problems: List[dict], batch_size: int = 3, c_puct: float = 1.5,\n",
    "             q_mode: str = \"mean\") -> RunLog:\n",
    "    \"\"\"PUCT tree search with virtual loss parallel leaf selection and async eval.\"\"\"\n",
    "    reset_call_counter()\n",
    "    log = RunLog()\n",
    "    root = MCTSNode(playbook=make_initial_playbook())\n",
    "    prob_idx = 0\n",
    "    # Number of leaves to evaluate in parallel\n",
    "    n_parallel = min(4, max(1, len(problems) // (batch_size * 4)))\n",
    "\n",
    "    async def _run():\n",
    "        nonlocal prob_idx\n",
    "\n",
    "        while prob_idx < len(problems):\n",
    "            # How many leaves can we evaluate given remaining problems?\n",
    "            remaining = len(problems) - prob_idx\n",
    "            n_leaves = min(n_parallel, remaining // batch_size) if remaining >= batch_size else 1\n",
    "\n",
    "            if n_leaves <= 1:\n",
    "                # Single leaf evaluation (no virtual loss needed)\n",
    "                leaf = puct_select(root, c_puct, q_mode)\n",
    "                batch_end = min(prob_idx + batch_size, len(problems))\n",
    "                batch = problems[prob_idx:batch_end]\n",
    "\n",
    "                batch_correct, last_reflection, last_question, eval_pb = \\\n",
    "                    await _eval_leaf_async(leaf, batch)\n",
    "\n",
    "                prob_idx = batch_end\n",
    "                reward = sum(batch_correct) / len(batch_correct)\n",
    "                leaf.results.extend(batch_correct)\n",
    "                log.correct.extend(batch_correct)\n",
    "                for _ in batch_correct:\n",
    "                    log.playbook_sizes.append(eval_pb.size)\n",
    "\n",
    "                if should_expand(leaf):\n",
    "                    new_pb = await curate_async(eval_pb, last_reflection, last_question)\n",
    "                    child = MCTSNode(playbook=new_pb, parent=leaf)\n",
    "                    leaf.children.append(child)\n",
    "\n",
    "                backprop(leaf, reward)\n",
    "            else:\n",
    "                # Parallel multi-leaf evaluation with virtual loss\n",
    "                selections = select_batch_with_virtual_loss(root, n_leaves, c_puct, q_mode)\n",
    "\n",
    "                # Build batches for each leaf\n",
    "                eval_tasks = []\n",
    "                leaf_batches = []\n",
    "                for leaf, path in selections:\n",
    "                    batch_end = min(prob_idx + batch_size, len(problems))\n",
    "                    if prob_idx >= len(problems):\n",
    "                        break\n",
    "                    batch = problems[prob_idx:batch_end]\n",
    "                    prob_idx = batch_end\n",
    "                    leaf_batches.append((leaf, path, batch))\n",
    "                    eval_tasks.append(_eval_leaf_async(leaf, batch))\n",
    "\n",
    "                # Evaluate all leaves in parallel\n",
    "                eval_results = await asyncio.gather(*eval_tasks)\n",
    "\n",
    "                # Process results: remove virtual losses, backprop, expand\n",
    "                for (leaf, path, batch), (batch_correct, last_ref, last_q, eval_pb) in \\\n",
    "                        zip(leaf_batches, eval_results):\n",
    "                    # Remove virtual losses\n",
    "                    for node in path:\n",
    "                        node.remove_virtual_loss()\n",
    "\n",
    "                    reward = sum(batch_correct) / len(batch_correct)\n",
    "                    leaf.results.extend(batch_correct)\n",
    "                    log.correct.extend(batch_correct)\n",
    "                    for _ in batch_correct:\n",
    "                        log.playbook_sizes.append(eval_pb.size)\n",
    "\n",
    "                    if should_expand(leaf):\n",
    "                        new_pb = await curate_async(eval_pb, last_ref, last_q)\n",
    "                        child = MCTSNode(playbook=new_pb, parent=leaf)\n",
    "                        leaf.children.append(child)\n",
    "\n",
    "                    backprop(leaf, reward)\n",
    "\n",
    "            done = len(log.correct)\n",
    "            if done % 10 <= batch_size * n_parallel or done == len(problems):\n",
    "                acc = sum(log.correct) / len(log.correct)\n",
    "                depth = _tree_depth(root)\n",
    "                size = _tree_size(root)\n",
    "                branches = _branch_count(root)\n",
    "                print(f\"  PUCT-{q_mode} [{done}/{len(problems)}] acc={acc:.2%} \"\n",
    "                      f\"depth={depth} nodes={size} branches={branches}\")\n",
    "\n",
    "    asyncio.run(_run())\n",
    "    log.call_counts = get_call_counts()\n",
    "    best_leaf = _best_leaf(root, q_mode)\n",
    "    log.final_playbook = best_leaf.playbook\n",
    "    return log\n",
    "\n",
    "\n",
    "def _tree_depth(node: MCTSNode) -> int:\n",
    "    if not node.children:\n",
    "        return 0\n",
    "    return 1 + max(_tree_depth(c) for c in node.children)\n",
    "\n",
    "\n",
    "def _tree_size(node: MCTSNode) -> int:\n",
    "    return 1 + sum(_tree_size(c) for c in node.children)\n",
    "\n",
    "\n",
    "def _branch_count(node: MCTSNode) -> int:\n",
    "    count = 1 if len(node.children) > 1 else 0\n",
    "    return count + sum(_branch_count(c) for c in node.children)\n",
    "\n",
    "\n",
    "def _best_leaf(node: MCTSNode, q_mode: str = \"mean\") -> MCTSNode:\n",
    "    if not node.children:\n",
    "        return node\n",
    "    best = max(node.children, key=lambda c: c.q_value(q_mode) if c.visits > 0 else -1)\n",
    "    return _best_leaf(best, q_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8GhAoeTVxsy"
   },
   "outputs": [],
   "source": [
    "# --- Strategy 3: Batch Thompson Sampling (Optimized) ---\n",
    "\n",
    "THOMPSON_SEED_PROBLEMS = 6\n",
    "THOMPSON_N_VARIANTS = 5\n",
    "THOMPSON_BATCH_SIZE = 5\n",
    "\n",
    "def run_thompson(problems: List[dict]) -> RunLog:\n",
    "    \"\"\"Batch Thompson Sampling with async seed and batched parallel generate+reflect.\n",
    "\n",
    "    Budget: 50 generate + 50 reflect + 5 curate = 105 calls.\n",
    "    \"\"\"\n",
    "    reset_call_counter()\n",
    "    log = RunLog()\n",
    "    base_pb = make_initial_playbook()\n",
    "    pool = [base_pb]\n",
    "\n",
    "    seed = problems[:THOMPSON_SEED_PROBLEMS]\n",
    "    remaining = problems[THOMPSON_SEED_PROBLEMS:]\n",
    "\n",
    "    async def _run():\n",
    "        nonlocal pool\n",
    "\n",
    "        # --- Phase 1: Seed (Parallel) ---\n",
    "        gen_tasks = [generate_async(prob[\"question\"], base_pb) for prob in seed]\n",
    "        gen_results = await asyncio.gather(*gen_tasks)\n",
    "\n",
    "        ref_tasks = []\n",
    "        for prob, (answer, bullets_used, raw) in zip(seed, gen_results):\n",
    "            ref_tasks.append(reflect_async(\n",
    "                prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, base_pb\n",
    "            ))\n",
    "        ref_results = await asyncio.gather(*ref_tasks)\n",
    "\n",
    "        reflections = []\n",
    "        for prob, (answer, bullets_used, raw), (reflection, tags) in zip(seed, gen_results, ref_results):\n",
    "            correct = answers_match(answer, prob[\"answer\"])\n",
    "            log.correct.append(correct)\n",
    "            log.playbook_sizes.append(base_pb.size)\n",
    "            for bid, label in tags.items():\n",
    "                base_pb.tag(bid, label)\n",
    "            reflections.append((reflection, prob[\"question\"]))\n",
    "\n",
    "        # Curate variants (parallel)\n",
    "        pool = await asyncio.gather(*[\n",
    "            curate_async(base_pb, ref_text, ref_q)\n",
    "            for ref_text, ref_q in reflections[:THOMPSON_N_VARIANTS]\n",
    "        ])\n",
    "        pool = list(pool)\n",
    "\n",
    "        print(f\"  Thompson: created {len(pool)} playbook variants from {len(seed)} seed problems\")\n",
    "\n",
    "        # --- Phase 2: Batch Thompson Sampling (Parallel) ---\n",
    "        alphas = [1.0] * len(pool)\n",
    "        betas_param = [1.0] * len(pool)\n",
    "\n",
    "        for batch_start in range(0, len(remaining), THOMPSON_BATCH_SIZE):\n",
    "            batch = remaining[batch_start : batch_start + THOMPSON_BATCH_SIZE]\n",
    "\n",
    "            # Select arms for batch based on current posterior\n",
    "            selected_indices = []\n",
    "            selected_pbs = []\n",
    "            for _ in batch:\n",
    "                ts_samples = [np.random.beta(a, b) for a, b in zip(alphas, betas_param)]\n",
    "                chosen = int(np.argmax(ts_samples))\n",
    "                selected_indices.append(chosen)\n",
    "                selected_pbs.append(pool[chosen])\n",
    "\n",
    "            # Parallel Generation\n",
    "            gen_tasks = [\n",
    "                generate_async(prob[\"question\"], pb)\n",
    "                for prob, pb in zip(batch, selected_pbs)\n",
    "            ]\n",
    "            gen_results = await asyncio.gather(*gen_tasks)\n",
    "\n",
    "            # Parallel Reflection\n",
    "            ref_tasks = []\n",
    "            for prob, (ans, bullets, raw), pb in zip(batch, gen_results, selected_pbs):\n",
    "                ref_tasks.append(reflect_async(\n",
    "                    prob[\"question\"], raw, ans, prob[\"answer\"], bullets, pb\n",
    "                ))\n",
    "            ref_results = await asyncio.gather(*ref_tasks)\n",
    "\n",
    "            # Batch Update\n",
    "            for i, (prob, (ans, bullets, raw), (ref, tags)) in enumerate(zip(batch, gen_results, ref_results)):\n",
    "                correct = answers_match(ans, prob[\"answer\"])\n",
    "                log.correct.append(correct)\n",
    "\n",
    "                chosen_idx = selected_indices[i]\n",
    "                pb = selected_pbs[i]\n",
    "                log.playbook_sizes.append(pb.size)\n",
    "\n",
    "                for bid, label in tags.items():\n",
    "                    pb.tag(bid, label)\n",
    "\n",
    "                if correct:\n",
    "                    alphas[chosen_idx] += 1.0\n",
    "                else:\n",
    "                    betas_param[chosen_idx] += 1.0\n",
    "\n",
    "            # Logging\n",
    "            done = THOMPSON_SEED_PROBLEMS + batch_start + len(batch)\n",
    "            if done % 10 == 0 or done == len(problems):\n",
    "                acc = sum(log.correct) / len(log.correct)\n",
    "                pulls = [int(a + b - 2) for a, b in zip(alphas, betas_param)]\n",
    "                best_var = int(np.argmax([a / (a + b) for a, b in zip(alphas, betas_param)]))\n",
    "                print(f\"  Thompson [{done}/{len(problems)}] acc={acc:.2%} pulls={pulls} best=variant-{best_var}\")\n",
    "\n",
    "        best_idx = int(np.argmax([a / (a + b) for a, b in zip(alphas, betas_param)]))\n",
    "        log.final_playbook = pool[best_idx]\n",
    "\n",
    "    asyncio.run(_run())\n",
    "    log.call_counts = get_call_counts()\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Strategy 3b: Dynamic Thompson Sampling (async, pool grows over time) ---\n",
    "\n",
    "THOMPSON_DYN_SEED_PROBLEMS = 6\n",
    "THOMPSON_DYN_N_VARIANTS = 5\n",
    "ARM_ADD_INTERVAL = 10\n",
    "\n",
    "def run_thompson_dynamic(problems: List[dict]) -> RunLog:\n",
    "    \"\"\"Thompson Sampling with dynamic arm addition (async).\n",
    "\n",
    "    Inspired by OPTS (arXiv:2503.01163) bandit-based strategy selection.\n",
    "    Note: OPTS uses TS for mutation operator selection; our dynamic arm addition\n",
    "    from the best arm is a novel extension for the playbook evolution setting.\n",
    "\n",
    "    Budget: ~109 calls (17 seed + 88 bandit + ~4 dynamic curate).\n",
    "    \"\"\"\n",
    "    reset_call_counter()\n",
    "    log = RunLog()\n",
    "\n",
    "    base_pb = make_initial_playbook()\n",
    "    pool = [base_pb]\n",
    "\n",
    "    seed = problems[:THOMPSON_DYN_SEED_PROBLEMS]\n",
    "    remaining = problems[THOMPSON_DYN_SEED_PROBLEMS:]\n",
    "\n",
    "    async def _run():\n",
    "        nonlocal pool\n",
    "\n",
    "        # --- Phase 1: Seed (parallel generate+reflect) ---\n",
    "        gen_tasks = [generate_async(prob[\"question\"], base_pb) for prob in seed]\n",
    "        gen_results = await asyncio.gather(*gen_tasks)\n",
    "\n",
    "        ref_tasks = []\n",
    "        for prob, (answer, bullets_used, raw) in zip(seed, gen_results):\n",
    "            ref_tasks.append(reflect_async(\n",
    "                prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, base_pb\n",
    "            ))\n",
    "        ref_results = await asyncio.gather(*ref_tasks)\n",
    "\n",
    "        reflections = []\n",
    "        for prob, (answer, bullets_used, raw), (reflection, tags) in zip(seed, gen_results, ref_results):\n",
    "            correct = answers_match(answer, prob[\"answer\"])\n",
    "            log.correct.append(correct)\n",
    "            log.playbook_sizes.append(base_pb.size)\n",
    "            for bid, label in tags.items():\n",
    "                base_pb.tag(bid, label)\n",
    "            reflections.append((reflection, prob[\"question\"]))\n",
    "\n",
    "        pool = await asyncio.gather(*[\n",
    "            curate_async(base_pb, ref_text, ref_q)\n",
    "            for ref_text, ref_q in reflections[:THOMPSON_DYN_N_VARIANTS]\n",
    "        ])\n",
    "        pool = list(pool)\n",
    "\n",
    "        print(f\"  Thompson-Dyn: created {len(pool)} initial variants from {len(seed)} seed problems\")\n",
    "\n",
    "        # --- Phase 2: Thompson Sampling with dynamic arm addition ---\n",
    "        alphas = [1.0] * len(pool)\n",
    "        betas_param = [1.0] * len(pool)\n",
    "        last_reflections = {}\n",
    "\n",
    "        for i, prob in enumerate(remaining):\n",
    "            if i > 0 and i % ARM_ADD_INTERVAL == 0:\n",
    "                posterior_means = [a / (a + b) for a, b in zip(alphas, betas_param)]\n",
    "                best_arm = int(np.argmax(posterior_means))\n",
    "                best_pb = pool[best_arm]\n",
    "\n",
    "                if best_arm in last_reflections:\n",
    "                    ref_text, ref_q = last_reflections[best_arm]\n",
    "                else:\n",
    "                    ref_text, ref_q = next(iter(last_reflections.values()), reflections[-1])\n",
    "\n",
    "                new_variant = await curate_async(best_pb, ref_text, ref_q)\n",
    "                pool.append(new_variant)\n",
    "                alphas.append(1.0)\n",
    "                betas_param.append(1.0)\n",
    "                print(f\"  Thompson-Dyn: added arm {len(pool)-1} (curated from best arm {best_arm}, \"\n",
    "                      f\"posterior={posterior_means[best_arm]:.2f})\")\n",
    "\n",
    "            ts_samples = [np.random.beta(a, b) for a, b in zip(alphas, betas_param)]\n",
    "            chosen = int(np.argmax(ts_samples))\n",
    "            pb = pool[chosen]\n",
    "\n",
    "            answer, bullets_used, raw = await generate_async(prob[\"question\"], pb)\n",
    "            correct = answers_match(answer, prob[\"answer\"])\n",
    "            log.correct.append(correct)\n",
    "            log.playbook_sizes.append(pb.size)\n",
    "\n",
    "            reflection, tags = await reflect_async(\n",
    "                prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, pb\n",
    "            )\n",
    "            for bid, label in tags.items():\n",
    "                pb.tag(bid, label)\n",
    "\n",
    "            last_reflections[chosen] = (reflection, prob[\"question\"])\n",
    "\n",
    "            if correct:\n",
    "                alphas[chosen] += 1.0\n",
    "            else:\n",
    "                betas_param[chosen] += 1.0\n",
    "\n",
    "            if (THOMPSON_DYN_SEED_PROBLEMS + i + 1) % 10 == 0:\n",
    "                acc = sum(log.correct) / len(log.correct)\n",
    "                pulls = [int(a + b - 2) for a, b in zip(alphas, betas_param)]\n",
    "                posterior_means = [a / (a + b) for a, b in zip(alphas, betas_param)]\n",
    "                best_var = int(np.argmax(posterior_means))\n",
    "                print(f\"  Thompson-Dyn [{THOMPSON_DYN_SEED_PROBLEMS + i + 1}/{len(problems)}] \"\n",
    "                      f\"acc={acc:.2%} arms={len(pool)} pulls={pulls} best=variant-{best_var}\")\n",
    "\n",
    "        posterior_means = [a / (a + b) for a, b in zip(alphas, betas_param)]\n",
    "        best_idx = int(np.argmax(posterior_means))\n",
    "        log.final_playbook = pool[best_idx]\n",
    "\n",
    "    asyncio.run(_run())\n",
    "    log.call_counts = get_call_counts()\n",
    "    return log"
   ],
   "metadata": {
    "id": "qZ_XUTaaVxsy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Strategy 5: Adaptive Progressive Widening (AB-MCTS-inspired, async) ---\n",
    "# Inspired by Sakana AI (arXiv:2503.04412). Our implementation differs from the\n",
    "# original paper: we use regret-based Beta updates instead of separate GEN/CONT\n",
    "# node types with backed-up score distributions. Renamed from \"AB-MCTS\" to\n",
    "# \"Adaptive Progressive Widening\" to reflect this distinction.\n",
    "#\n",
    "# Key fix: removed counterfactual assumption in deepen+regress case. Previously\n",
    "# we incremented expand_alpha when deepening didn't improve Q, assuming expansion\n",
    "# would have helped — but we never observed that counterfactual. Now we only\n",
    "# update the posterior when we have direct evidence about the chosen action.\n",
    "\n",
    "def run_ab_mcts(problems: List[dict], batch_size: int = 3, c_puct: float = 1.5) -> RunLog:\n",
    "    \"\"\"Adaptive Progressive Widening with Thompson-sampled expand/deepen (async).\n",
    "\n",
    "    Each node uses Beta(expand_alpha, expand_beta) to decide wider vs deeper.\n",
    "    Posterior updated only on direct evidence (no counterfactual assumptions):\n",
    "      - Went wider AND reward > prev_q -> expand_alpha += 1 (expanding helped)\n",
    "      - Went wider AND reward <= prev_q -> expand_beta += 1 (expanding didn't help)\n",
    "      - Went deeper AND reward > prev_q -> expand_beta += 1 (deepening helped)\n",
    "      - Went deeper AND reward <= prev_q -> no update (no evidence either way)\n",
    "\n",
    "    Uses Bayesian Q-estimator. Budget: 50 gen + 50 ref + variable curate <= ~117.\n",
    "    \"\"\"\n",
    "    reset_call_counter()\n",
    "    log = RunLog()\n",
    "    q_mode = \"bayesian\"\n",
    "    root = MCTSNode(playbook=make_initial_playbook())\n",
    "    prob_idx = 0\n",
    "    expand_count = 0\n",
    "    deepen_count = 0\n",
    "\n",
    "    async def _run():\n",
    "        nonlocal prob_idx, expand_count, deepen_count\n",
    "\n",
    "        while prob_idx < len(problems):\n",
    "            leaf = puct_select(root, c_puct, q_mode)\n",
    "            prev_q = leaf.q_value(q_mode)\n",
    "\n",
    "            batch_end = min(prob_idx + batch_size, len(problems))\n",
    "            batch = problems[prob_idx:batch_end]\n",
    "            prob_idx = batch_end\n",
    "\n",
    "            batch_correct, last_reflection, last_question, eval_pb = \\\n",
    "                await _eval_leaf_async(leaf, batch)\n",
    "\n",
    "            reward = sum(batch_correct) / len(batch_correct)\n",
    "            leaf.results.extend(batch_correct)\n",
    "            log.correct.extend(batch_correct)\n",
    "            for _ in batch_correct:\n",
    "                log.playbook_sizes.append(eval_pb.size)\n",
    "\n",
    "            # Thompson-sampled wider-vs-deeper decision\n",
    "            ts_sample = np.random.beta(leaf.expand_alpha, leaf.expand_beta)\n",
    "            go_wider = ts_sample > 0.5 and leaf.visits > 0\n",
    "\n",
    "            if go_wider:\n",
    "                expand_count += 1\n",
    "                new_pb = await curate_async(eval_pb, last_reflection, last_question)\n",
    "                child = MCTSNode(playbook=new_pb, parent=leaf)\n",
    "                leaf.children.append(child)\n",
    "                backprop(leaf, reward)\n",
    "\n",
    "                # Direct evidence: did expanding help?\n",
    "                if reward > prev_q:\n",
    "                    leaf.expand_alpha += 1\n",
    "                else:\n",
    "                    leaf.expand_beta += 1\n",
    "            else:\n",
    "                deepen_count += 1\n",
    "                backprop(leaf, reward)\n",
    "\n",
    "                # Direct evidence only: did deepening help?\n",
    "                if reward > prev_q:\n",
    "                    leaf.expand_beta += 1\n",
    "                # No update if deepening didn't help — we have no evidence\n",
    "                # that expanding would have been better (counterfactual).\n",
    "\n",
    "            done = len(log.correct)\n",
    "            if done % 10 <= batch_size or done == len(problems):\n",
    "                acc = sum(log.correct) / len(log.correct)\n",
    "                depth = _tree_depth(root)\n",
    "                size = _tree_size(root)\n",
    "                print(f\"  AB-MCTS [{done}/{len(problems)}] acc={acc:.2%} \"\n",
    "                      f\"depth={depth} nodes={size} wider={expand_count} deeper={deepen_count}\")\n",
    "\n",
    "    asyncio.run(_run())\n",
    "    log.call_counts = get_call_counts()\n",
    "    best_leaf = _best_leaf(root, q_mode)\n",
    "    log.final_playbook = best_leaf.playbook\n",
    "    print(f\"  AB-MCTS final: wider={expand_count} deeper={deepen_count} \"\n",
    "          f\"ratio={expand_count/(expand_count+deepen_count):.2f}\")\n",
    "    return log"
   ],
   "metadata": {
    "id": "Y-xBTPKgVxsy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Strategy 3c: Discounted Thompson Sampling (async, fixed timing) ---\n",
    "# From the non-stationary bandits literature (arXiv:2305.10718):\n",
    "# Discount is applied AFTER the posterior update, not before.\n",
    "# Floor lowered from 1.0 to 0.1 to allow proper forgetting.\n",
    "\n",
    "THOMPSON_DISC_GAMMA = 0.95\n",
    "THOMPSON_DISC_SEED_PROBLEMS = 6\n",
    "THOMPSON_DISC_N_VARIANTS = 5\n",
    "\n",
    "def run_thompson_discounted(problems: List[dict]) -> RunLog:\n",
    "    \"\"\"Discounted Thompson Sampling with corrected discount timing (async).\n",
    "\n",
    "    Discount applied AFTER posterior update (per arXiv:2305.10718), not before.\n",
    "    Floor at 0.1 (not 1.0) to allow proper exponential forgetting.\n",
    "    Effective lookback window: 1/(1-0.95) = 20 problems.\n",
    "\n",
    "    Budget: 105 calls (17 seed + 88 bandit).\n",
    "    \"\"\"\n",
    "    reset_call_counter()\n",
    "    log = RunLog()\n",
    "\n",
    "    base_pb = make_initial_playbook()\n",
    "    pool = [base_pb]\n",
    "\n",
    "    seed = problems[:THOMPSON_DISC_SEED_PROBLEMS]\n",
    "    remaining = problems[THOMPSON_DISC_SEED_PROBLEMS:]\n",
    "\n",
    "    async def _run():\n",
    "        # --- Phase 1: Seed (parallel generate+reflect) ---\n",
    "        gen_tasks = [generate_async(prob[\"question\"], base_pb) for prob in seed]\n",
    "        gen_results = await asyncio.gather(*gen_tasks)\n",
    "\n",
    "        ref_tasks = []\n",
    "        for prob, (answer, bullets_used, raw) in zip(seed, gen_results):\n",
    "            ref_tasks.append(reflect_async(\n",
    "                prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, base_pb\n",
    "            ))\n",
    "        ref_results = await asyncio.gather(*ref_tasks)\n",
    "\n",
    "        reflections = []\n",
    "        for prob, (answer, bullets_used, raw), (reflection, tags) in zip(seed, gen_results, ref_results):\n",
    "            correct = answers_match(answer, prob[\"answer\"])\n",
    "            log.correct.append(correct)\n",
    "            log.playbook_sizes.append(base_pb.size)\n",
    "            for bid, label in tags.items():\n",
    "                base_pb.tag(bid, label)\n",
    "            reflections.append((reflection, prob[\"question\"]))\n",
    "\n",
    "        pool = await asyncio.gather(*[\n",
    "            curate_async(base_pb, ref_text, ref_q)\n",
    "            for ref_text, ref_q in reflections[:THOMPSON_DISC_N_VARIANTS]\n",
    "        ])\n",
    "        pool = list(pool)\n",
    "\n",
    "        print(f\"  Thompson-Disc: created {len(pool)} playbook variants from {len(seed)} seed problems\")\n",
    "        print(f\"  Thompson-Disc: gamma={THOMPSON_DISC_GAMMA}, effective horizon={1/(1-THOMPSON_DISC_GAMMA):.0f} problems\")\n",
    "\n",
    "        # --- Phase 2: Discounted Thompson Sampling ---\n",
    "        alphas = [1.0] * len(pool)\n",
    "        betas_param = [1.0] * len(pool)\n",
    "\n",
    "        for i, prob in enumerate(remaining):\n",
    "            # Thompson sample (BEFORE any discounting)\n",
    "            ts_samples = [np.random.beta(a, b) for a, b in zip(alphas, betas_param)]\n",
    "            chosen = int(np.argmax(ts_samples))\n",
    "            pb = pool[chosen]\n",
    "\n",
    "            answer, bullets_used, raw = await generate_async(prob[\"question\"], pb)\n",
    "            correct = answers_match(answer, prob[\"answer\"])\n",
    "            log.correct.append(correct)\n",
    "            log.playbook_sizes.append(pb.size)\n",
    "\n",
    "            reflection, tags = await reflect_async(\n",
    "                prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, pb\n",
    "            )\n",
    "            for bid, label in tags.items():\n",
    "                pb.tag(bid, label)\n",
    "\n",
    "            # Update Beta posterior for the chosen variant\n",
    "            if correct:\n",
    "                alphas[chosen] += 1.0\n",
    "            else:\n",
    "                betas_param[chosen] += 1.0\n",
    "\n",
    "            # Discount ALL arms AFTER update (per arXiv:2305.10718)\n",
    "            for k in range(len(pool)):\n",
    "                alphas[k] *= THOMPSON_DISC_GAMMA\n",
    "                betas_param[k] *= THOMPSON_DISC_GAMMA\n",
    "                # Floor at 0.1 (not 1.0) to allow proper forgetting\n",
    "                alphas[k] = max(alphas[k], 0.1)\n",
    "                betas_param[k] = max(betas_param[k], 0.1)\n",
    "\n",
    "            if (THOMPSON_DISC_SEED_PROBLEMS + i + 1) % 10 == 0:\n",
    "                acc = sum(log.correct) / len(log.correct)\n",
    "                pulls = [int(a + b - 2) for a, b in zip(alphas, betas_param)]\n",
    "                best_var = int(np.argmax([a / (a + b) for a, b in zip(alphas, betas_param)]))\n",
    "                print(f\"  Thompson-Disc [{THOMPSON_DISC_SEED_PROBLEMS + i + 1}/{len(problems)}] \"\n",
    "                      f\"acc={acc:.2%} pulls={pulls} best=variant-{best_var}\")\n",
    "\n",
    "        best_idx = int(np.argmax([a / (a + b) for a, b in zip(alphas, betas_param)]))\n",
    "        log.final_playbook = pool[best_idx]\n",
    "\n",
    "    asyncio.run(_run())\n",
    "    log.call_counts = get_call_counts()\n",
    "    return log"
   ],
   "metadata": {
    "id": "DteK-rtPVxsy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NywJmXpxVxsy"
   },
   "outputs": [],
   "source": [
    "# --- Strategy 4: Majority Vote (fully parallel, no evolution) ---\n",
    "\n",
    "MAJORITY_N_SAMPLES = 2\n",
    "\n",
    "def run_majority_vote(problems: List[dict]) -> RunLog:\n",
    "    \"\"\"No playbook evolution. Static initial playbook. All generate calls in parallel.\n",
    "\n",
    "    Budget: 50 * 2 = 100 generate calls. No reflect or curate.\n",
    "    \"\"\"\n",
    "    reset_call_counter()\n",
    "    log = RunLog()\n",
    "    pb = make_initial_playbook()\n",
    "\n",
    "    async def _run():\n",
    "        # Fire ALL 100 generate calls at once — no dependencies between them\n",
    "        tasks = []\n",
    "        for prob in problems:\n",
    "            for _ in range(MAJORITY_N_SAMPLES):\n",
    "                tasks.append(generate_async(prob[\"question\"], pb))\n",
    "\n",
    "        all_results = await asyncio.gather(*tasks)\n",
    "\n",
    "        # Group results by problem (every MAJORITY_N_SAMPLES consecutive results)\n",
    "        for i, prob in enumerate(problems):\n",
    "            answers = []\n",
    "            for j in range(MAJORITY_N_SAMPLES):\n",
    "                answer, _, _ = all_results[i * MAJORITY_N_SAMPLES + j]\n",
    "                answers.append(answer)\n",
    "\n",
    "            vote_counts = Counter(answers)\n",
    "            majority_answer = vote_counts.most_common(1)[0][0]\n",
    "            correct = answers_match(majority_answer, prob[\"answer\"])\n",
    "            log.correct.append(correct)\n",
    "            log.playbook_sizes.append(pb.size)\n",
    "\n",
    "        acc = sum(log.correct) / len(log.correct)\n",
    "        print(f\"  MajVote [done] acc={acc:.2%}\")\n",
    "\n",
    "    asyncio.run(_run())\n",
    "    log.call_counts = get_call_counts()\n",
    "    log.final_playbook = pb\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Strategy 5: Best-of-N (Rejection Sampling, fully parallel) ---\n",
    "\n",
    "BEST_OF_N_SAMPLES = 2\n",
    "\n",
    "def run_best_of_n(problems: List[dict]) -> RunLog:\n",
    "    \"\"\"No playbook evolution. Generate N solutions per problem, pick the one with\n",
    "    highest answer frequency (like majority vote but framed as rejection sampling).\n",
    "\n",
    "    This is the \"does a smart tree search actually beat simply generating N random\n",
    "    variations and picking the best one?\" baseline.\n",
    "\n",
    "    Budget: 50 * 2 = 100 generate calls. No reflect or curate.\n",
    "    \"\"\"\n",
    "    reset_call_counter()\n",
    "    log = RunLog()\n",
    "    pb = make_initial_playbook()\n",
    "\n",
    "    async def _run():\n",
    "        # Fire all generate calls in parallel\n",
    "        tasks = []\n",
    "        for prob in problems:\n",
    "            for _ in range(BEST_OF_N_SAMPLES):\n",
    "                tasks.append(generate_async(prob[\"question\"], pb))\n",
    "\n",
    "        all_results = await asyncio.gather(*tasks)\n",
    "\n",
    "        for i, prob in enumerate(problems):\n",
    "            answers = []\n",
    "            raws = []\n",
    "            for j in range(BEST_OF_N_SAMPLES):\n",
    "                answer, _, raw = all_results[i * BEST_OF_N_SAMPLES + j]\n",
    "                answers.append(answer)\n",
    "                raws.append(raw)\n",
    "\n",
    "            # Pick the answer that appears most (ties broken by first occurrence)\n",
    "            # Unlike majority vote which just takes the mode, best-of-N conceptually\n",
    "            # selects the \"best\" — here we use consistency as a proxy for confidence\n",
    "            vote_counts = Counter(answers)\n",
    "            best_answer = vote_counts.most_common(1)[0][0]\n",
    "            correct = answers_match(best_answer, prob[\"answer\"])\n",
    "            log.correct.append(correct)\n",
    "            log.playbook_sizes.append(pb.size)\n",
    "\n",
    "        acc = sum(log.correct) / len(log.correct)\n",
    "        print(f\"  Best-of-N [done] acc={acc:.2%}\")\n",
    "\n",
    "    asyncio.run(_run())\n",
    "    log.call_counts = get_call_counts()\n",
    "    log.final_playbook = pb\n",
    "    return log"
   ],
   "metadata": {
    "id": "ddcSP3-TVxsz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Strategy 6: Beam Search over Playbook Variants ---\n",
    "# The \"missing link\" between Greedy (width=1) and MCTS (complex tree).\n",
    "# Beam search maintains K candidate playbooks, evaluates all on each batch,\n",
    "# prunes to top-K, then curates from the survivors.\n",
    "\n",
    "BEAM_WIDTH = 3\n",
    "BEAM_EVAL_BATCH = 5\n",
    "\n",
    "def run_beam_search(problems: List[dict], beam_width: int = BEAM_WIDTH,\n",
    "                    eval_batch: int = BEAM_EVAL_BATCH) -> RunLog:\n",
    "    \"\"\"Beam search over playbook space.\n",
    "\n",
    "    Maintains K playbook candidates. Each round:\n",
    "    1. Evaluate all K beams on the same batch of problems (parallel)\n",
    "    2. Rank beams by batch accuracy\n",
    "    3. Keep top-K\n",
    "    4. Curate each surviving beam to produce next-generation candidates\n",
    "\n",
    "    Hypothesis: captures ~80% of tree search gains at ~20% of the cost.\n",
    "\n",
    "    Budget: ~50 gen + ~50 ref + ~30 curate = ~130 calls (slightly over budget\n",
    "    due to K-way parallel eval, but comparable total compute since batches overlap).\n",
    "    \"\"\"\n",
    "    reset_call_counter()\n",
    "    log = RunLog()\n",
    "\n",
    "    # Initialize beam with K copies of the base playbook curated from different seeds\n",
    "    base_pb = make_initial_playbook()\n",
    "\n",
    "    async def _run():\n",
    "        # Seed phase: generate+reflect on first few problems to get diverse curations\n",
    "        seed = problems[:eval_batch]\n",
    "        remaining = problems[eval_batch:]\n",
    "\n",
    "        gen_tasks = [generate_async(prob[\"question\"], base_pb) for prob in seed]\n",
    "        gen_results = await asyncio.gather(*gen_tasks)\n",
    "\n",
    "        ref_tasks = []\n",
    "        for prob, (answer, bullets_used, raw) in zip(seed, gen_results):\n",
    "            ref_tasks.append(reflect_async(\n",
    "                prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, base_pb\n",
    "            ))\n",
    "        ref_results = await asyncio.gather(*ref_tasks)\n",
    "\n",
    "        reflections = []\n",
    "        for prob, (answer, bullets_used, raw), (reflection, tags) in zip(seed, gen_results, ref_results):\n",
    "            correct = answers_match(answer, prob[\"answer\"])\n",
    "            log.correct.append(correct)\n",
    "            log.playbook_sizes.append(base_pb.size)\n",
    "            for bid, label in tags.items():\n",
    "                base_pb.tag(bid, label)\n",
    "            reflections.append((reflection, prob[\"question\"]))\n",
    "\n",
    "        # Create initial beam from diverse curations\n",
    "        beams = []\n",
    "        _beam_variants = await asyncio.gather(*[\n",
    "            curate_async(base_pb, reflections[i % len(reflections)][0], reflections[i % len(reflections)][1])\n",
    "            for i in range(beam_width)\n",
    "        ])\n",
    "        for variant in _beam_variants:\n",
    "            beams.append({\"playbook\": variant, \"score\": 0.0, \"total_correct\": 0, \"total_seen\": 0})\n",
    "\n",
    "        print(f\"  Beam: initialized {len(beams)} beams from {len(seed)} seed problems\")\n",
    "\n",
    "        # Main beam search loop\n",
    "        prob_idx = 0\n",
    "        while prob_idx < len(remaining):\n",
    "            batch_end = min(prob_idx + eval_batch, len(remaining))\n",
    "            batch = remaining[prob_idx:batch_end]\n",
    "            prob_idx = batch_end\n",
    "\n",
    "            # Evaluate ALL beams on the same batch (parallel across beams AND problems)\n",
    "            all_eval_tasks = []\n",
    "            for beam in beams:\n",
    "                beam_tasks = [generate_async(prob[\"question\"], beam[\"playbook\"]) for prob in batch]\n",
    "                all_eval_tasks.extend(beam_tasks)\n",
    "\n",
    "            all_gen_results = await asyncio.gather(*all_eval_tasks)\n",
    "\n",
    "            # Reflect on all results\n",
    "            all_ref_tasks = []\n",
    "            idx = 0\n",
    "            for beam in beams:\n",
    "                for prob in batch:\n",
    "                    answer, bullets_used, raw = all_gen_results[idx]\n",
    "                    all_ref_tasks.append(reflect_async(\n",
    "                        prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, beam[\"playbook\"]\n",
    "                    ))\n",
    "                    idx += 1\n",
    "\n",
    "            all_ref_results = await asyncio.gather(*all_ref_tasks)\n",
    "\n",
    "            # Score each beam on this batch\n",
    "            idx = 0\n",
    "            beam_batch_scores = []\n",
    "            beam_reflections = []\n",
    "            for beam in beams:\n",
    "                batch_correct = 0\n",
    "                last_ref = \"\"\n",
    "                last_q = \"\"\n",
    "                for prob in batch:\n",
    "                    answer, bullets_used, raw = all_gen_results[idx]\n",
    "                    reflection, tags = all_ref_results[idx]\n",
    "                    correct = answers_match(answer, prob[\"answer\"])\n",
    "                    if correct:\n",
    "                        batch_correct += 1\n",
    "                    for bid, label in tags.items():\n",
    "                        beam[\"playbook\"].tag(bid, label)\n",
    "                    last_ref = reflection\n",
    "                    last_q = prob[\"question\"]\n",
    "                    idx += 1\n",
    "\n",
    "                beam[\"total_correct\"] += batch_correct\n",
    "                beam[\"total_seen\"] += len(batch)\n",
    "                beam[\"score\"] = beam[\"total_correct\"] / beam[\"total_seen\"]\n",
    "                beam_batch_scores.append(batch_correct / len(batch))\n",
    "                beam_reflections.append((last_ref, last_q))\n",
    "\n",
    "            # Log results from the BEST beam for this batch\n",
    "            best_beam_idx = int(np.argmax(beam_batch_scores))\n",
    "            best_beam = beams[best_beam_idx]\n",
    "            # Re-evaluate best beam's answers for logging (use cached results)\n",
    "            gen_offset = best_beam_idx * len(batch)\n",
    "            for j, prob in enumerate(batch):\n",
    "                answer, _, _ = all_gen_results[gen_offset + j]\n",
    "                correct = answers_match(answer, prob[\"answer\"])\n",
    "                log.correct.append(correct)\n",
    "                log.playbook_sizes.append(best_beam[\"playbook\"].size)\n",
    "\n",
    "            # Prune: keep top-K beams by cumulative score\n",
    "            beams.sort(key=lambda b: b[\"score\"], reverse=True)\n",
    "            beams = beams[:beam_width]\n",
    "\n",
    "            # Curate each surviving beam\n",
    "            new_beams = []\n",
    "            _new_pbs = await asyncio.gather(*[\n",
    "                curate_async(beam[\"playbook\"], ref_text, ref_q)\n",
    "                for beam, (ref_text, ref_q) in zip(beams, beam_reflections[:beam_width])\n",
    "            ])\n",
    "            for new_pb, beam in zip(_new_pbs, beams):\n",
    "                new_beams.append({\n",
    "                    \"playbook\": new_pb,\n",
    "                    \"score\": beam[\"score\"],\n",
    "                    \"total_correct\": beam[\"total_correct\"],\n",
    "                    \"total_seen\": beam[\"total_seen\"],\n",
    "                })\n",
    "            beams = new_beams\n",
    "\n",
    "            done = len(log.correct)\n",
    "            if done % 10 <= eval_batch or done == len(problems):\n",
    "                acc = sum(log.correct) / len(log.correct)\n",
    "                scores = [f\"{b['score']:.0%}\" for b in beams]\n",
    "                print(f\"  Beam [{done}/{len(problems)}] acc={acc:.2%} beam_scores={scores}\")\n",
    "\n",
    "        # Final playbook = best beam\n",
    "        beams.sort(key=lambda b: b[\"score\"], reverse=True)\n",
    "        log.final_playbook = beams[0][\"playbook\"]\n",
    "\n",
    "    asyncio.run(_run())\n",
    "    log.call_counts = get_call_counts()\n",
    "    return log"
   ],
   "metadata": {
    "id": "1Qy57--BVxsz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Strategy 7: UCB Bandit (deterministic counterpart to Thompson Sampling) ---\n",
    "# Standard UCB1 (Auer et al., 2002) uses a deterministic upper confidence bound\n",
    "# instead of Thompson Sampling's stochastic posterior sampling.\n",
    "# Hypothesis: Thompson is usually better for exploration, but UCB is often more\n",
    "# sample-efficient. Comparing them isolates the \"randomness\" variable.\n",
    "\n",
    "UCB_SEED_PROBLEMS = 6\n",
    "UCB_N_VARIANTS = 5\n",
    "UCB_C = math.sqrt(2)  # standard UCB1 exploration constant\n",
    "\n",
    "def run_ucb_bandit(problems: List[dict]) -> RunLog:\n",
    "    \"\"\"Flat bandit with UCB1 selection over playbook pool (async).\n",
    "\n",
    "    UCB1 selects: argmax_k [ mean_reward_k + c * sqrt(ln(t) / n_k) ]\n",
    "    where t = total pulls, n_k = pulls of arm k, c = sqrt(2).\n",
    "\n",
    "    Budget: 50 generate + 50 reflect + 5 curate = 105 calls.\n",
    "    \"\"\"\n",
    "    reset_call_counter()\n",
    "    log = RunLog()\n",
    "\n",
    "    base_pb = make_initial_playbook()\n",
    "    pool = [base_pb]\n",
    "\n",
    "    seed = problems[:UCB_SEED_PROBLEMS]\n",
    "    remaining = problems[UCB_SEED_PROBLEMS:]\n",
    "\n",
    "    async def _run():\n",
    "        nonlocal pool\n",
    "\n",
    "        # --- Phase 1: Seed (parallel generate+reflect) ---\n",
    "        gen_tasks = [generate_async(prob[\"question\"], base_pb) for prob in seed]\n",
    "        gen_results = await asyncio.gather(*gen_tasks)\n",
    "\n",
    "        ref_tasks = []\n",
    "        for prob, (answer, bullets_used, raw) in zip(seed, gen_results):\n",
    "            ref_tasks.append(reflect_async(\n",
    "                prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, base_pb\n",
    "            ))\n",
    "        ref_results = await asyncio.gather(*ref_tasks)\n",
    "\n",
    "        reflections = []\n",
    "        for prob, (answer, bullets_used, raw), (reflection, tags) in zip(seed, gen_results, ref_results):\n",
    "            correct = answers_match(answer, prob[\"answer\"])\n",
    "            log.correct.append(correct)\n",
    "            log.playbook_sizes.append(base_pb.size)\n",
    "            for bid, label in tags.items():\n",
    "                base_pb.tag(bid, label)\n",
    "            reflections.append((reflection, prob[\"question\"]))\n",
    "\n",
    "        pool = await asyncio.gather(*[\n",
    "            curate_async(base_pb, ref_text, ref_q)\n",
    "            for ref_text, ref_q in reflections[:UCB_N_VARIANTS]\n",
    "        ])\n",
    "        pool = list(pool)\n",
    "\n",
    "        print(f\"  UCB: created {len(pool)} playbook variants from {len(seed)} seed problems\")\n",
    "\n",
    "        # --- Phase 2: UCB1 selection ---\n",
    "        rewards = [[] for _ in pool]  # per-arm reward history\n",
    "        total_pulls = 0\n",
    "\n",
    "        for i, prob in enumerate(remaining):\n",
    "            # UCB1 selection: ensure each arm pulled at least once\n",
    "            unpulled = [k for k in range(len(pool)) if not rewards[k]]\n",
    "            if unpulled:\n",
    "                chosen = unpulled[0]\n",
    "            else:\n",
    "                total_pulls_val = sum(len(r) for r in rewards)\n",
    "                ucb_scores = []\n",
    "                for k in range(len(pool)):\n",
    "                    mean_r = sum(rewards[k]) / len(rewards[k])\n",
    "                    explore = UCB_C * math.sqrt(math.log(total_pulls_val) / len(rewards[k]))\n",
    "                    ucb_scores.append(mean_r + explore)\n",
    "                chosen = int(np.argmax(ucb_scores))\n",
    "\n",
    "            pb = pool[chosen]\n",
    "            answer, bullets_used, raw = await generate_async(prob[\"question\"], pb)\n",
    "            correct = answers_match(answer, prob[\"answer\"])\n",
    "            log.correct.append(correct)\n",
    "            log.playbook_sizes.append(pb.size)\n",
    "\n",
    "            reflection, tags = await reflect_async(\n",
    "                prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, pb\n",
    "            )\n",
    "            for bid, label in tags.items():\n",
    "                pb.tag(bid, label)\n",
    "\n",
    "            rewards[chosen].append(1.0 if correct else 0.0)\n",
    "            total_pulls += 1\n",
    "\n",
    "            if (UCB_SEED_PROBLEMS + i + 1) % 10 == 0:\n",
    "                acc = sum(log.correct) / len(log.correct)\n",
    "                pulls = [len(r) for r in rewards]\n",
    "                means = [f\"{sum(r)/len(r):.0%}\" if r else \"?\" for r in rewards]\n",
    "                best_var = int(np.argmax([sum(r)/len(r) if r else 0 for r in rewards]))\n",
    "                print(f\"  UCB [{UCB_SEED_PROBLEMS + i + 1}/{len(problems)}] \"\n",
    "                      f\"acc={acc:.2%} pulls={pulls} means={means} best=variant-{best_var}\")\n",
    "\n",
    "        best_idx = int(np.argmax([sum(r)/len(r) if r else 0 for r in rewards]))\n",
    "        log.final_playbook = pool[best_idx]\n",
    "\n",
    "    asyncio.run(_run())\n",
    "    log.call_counts = get_call_counts()\n",
    "    return log"
   ],
   "metadata": {
    "id": "6j5JQSZWVxsz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Strategy 8: Discounted MCTS (PUCT with gamma-decayed Q-values) ---\n",
    "# Applies the same gamma=0.95 discount idea from Thompson-Disc but inside the\n",
    "# MCTS tree nodes. Standard PUCT suffers from non-stationarity: if a child node\n",
    "# improves via curation, the parent's average Q doesn't update fast enough.\n",
    "# Discounting recent rewards more heavily addresses this.\n",
    "\n",
    "DISC_MCTS_GAMMA = 0.95\n",
    "\n",
    "def run_discounted_mcts(problems: List[dict], batch_size: int = 3, c_puct: float = 1.5) -> RunLog:\n",
    "    \"\"\"PUCT tree search with discounted Q-values for non-stationary nodes.\n",
    "\n",
    "    Q-values use exponentially weighted mean: recent rewards count more.\n",
    "    This addresses the problem where a node's playbook improves over time\n",
    "    (via child curations) but the parent's stale Q-average doesn't reflect it.\n",
    "\n",
    "    Uses Bayesian base Q-estimator with discount overlay.\n",
    "    Budget: 50 gen + 50 ref + variable curate <= ~117.\n",
    "    \"\"\"\n",
    "    reset_call_counter()\n",
    "    log = RunLog()\n",
    "    root = MCTSNode(playbook=make_initial_playbook())\n",
    "    prob_idx = 0\n",
    "\n",
    "    def discounted_q(node: MCTSNode) -> float:\n",
    "        \"\"\"Compute gamma-discounted Q: recent rewards weighted more heavily.\"\"\"\n",
    "        if not node.reward_history:\n",
    "            return 0.5  # optimistic prior\n",
    "        # Exponentially discount: most recent reward has weight 1, next has gamma, etc.\n",
    "        weights = []\n",
    "        w = 1.0\n",
    "        for _ in reversed(node.reward_history):\n",
    "            weights.append(w)\n",
    "            w *= DISC_MCTS_GAMMA\n",
    "        weights.reverse()\n",
    "        total_w = sum(weights)\n",
    "        discounted = sum(r * w for r, w in zip(node.reward_history, weights))\n",
    "        return discounted / total_w\n",
    "\n",
    "    def disc_puct_select(node: MCTSNode, c: float = 1.5) -> MCTSNode:\n",
    "        \"\"\"PUCT selection using discounted Q.\"\"\"\n",
    "        if not node.children:\n",
    "            return node\n",
    "        n_parent = node.visits\n",
    "        best, best_score = None, -float(\"inf\")\n",
    "        for child in node.children:\n",
    "            prior = 1.0 / len(node.children)\n",
    "            exploit = discounted_q(child)\n",
    "            explore = c * prior * math.sqrt(n_parent) / (1 + child.visits)\n",
    "            score = exploit + explore\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best = child\n",
    "        return disc_puct_select(best, c)\n",
    "\n",
    "    async def _run():\n",
    "        nonlocal prob_idx\n",
    "\n",
    "        while prob_idx < len(problems):\n",
    "            leaf = disc_puct_select(root, c_puct)\n",
    "\n",
    "            batch_end = min(prob_idx + batch_size, len(problems))\n",
    "            batch = problems[prob_idx:batch_end]\n",
    "            prob_idx = batch_end\n",
    "\n",
    "            batch_correct, last_reflection, last_question, eval_pb = \\\n",
    "                await _eval_leaf_async(leaf, batch)\n",
    "\n",
    "            reward = sum(batch_correct) / len(batch_correct)\n",
    "            leaf.results.extend(batch_correct)\n",
    "            log.correct.extend(batch_correct)\n",
    "            for _ in batch_correct:\n",
    "                log.playbook_sizes.append(eval_pb.size)\n",
    "\n",
    "            if should_expand(leaf):\n",
    "                new_pb = await curate_async(eval_pb, last_reflection, last_question)\n",
    "                child = MCTSNode(playbook=new_pb, parent=leaf)\n",
    "                leaf.children.append(child)\n",
    "\n",
    "            backprop(leaf, reward)\n",
    "\n",
    "            done = len(log.correct)\n",
    "            if done % 10 <= batch_size or done == len(problems):\n",
    "                acc = sum(log.correct) / len(log.correct)\n",
    "                depth = _tree_depth(root)\n",
    "                size = _tree_size(root)\n",
    "                print(f\"  Disc-MCTS [{done}/{len(problems)}] acc={acc:.2%} \"\n",
    "                      f\"depth={depth} nodes={size}\")\n",
    "\n",
    "    asyncio.run(_run())\n",
    "    log.call_counts = get_call_counts()\n",
    "    best_leaf = _best_leaf(root, \"mean\")  # use mean for final selection\n",
    "    log.final_playbook = best_leaf.playbook\n",
    "    return log"
   ],
   "metadata": {
    "id": "a2eezK_-Vxsz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jo8PR8gZVxsz"
   },
   "source": [
    "## 5. Experiment Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9ieE6bcVxsz"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Running Majority Vote (no evolution baseline)...\")\n",
    "print(\"=\" * 60)\n",
    "majority_log = load_checkpoint(\"majority_vote\")\n",
    "if majority_log is None:\n",
    "    majority_log = run_majority_vote(problems)\n",
    "    save_checkpoint(\"majority_vote\", majority_log)\n",
    "print(f\"MajVote done. Final acc (last 20): {majority_log.final_accuracy:.2%}\")\n",
    "print(f\"Calls: {majority_log.call_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eXzrBiBVxsz"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Running Greedy ACE (baseline)...\")\n",
    "print(\"=\" * 60)\n",
    "greedy_log = load_checkpoint(\"greedy_ace\")\n",
    "if greedy_log is None:\n",
    "    greedy_log = run_greedy(problems)\n",
    "    save_checkpoint(\"greedy_ace\", greedy_log)\n",
    "print(f\"Greedy done. Final acc (last 20): {greedy_log.final_accuracy:.2%}\")\n",
    "print(f\"Calls: {greedy_log.call_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nk-LlE0yVxsz"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Running Thompson Sampling...\")\n",
    "print(\"=\" * 60)\n",
    "thompson_log = load_checkpoint(\"thompson\")\n",
    "if thompson_log is None:\n",
    "    thompson_log = run_thompson(problems)\n",
    "    save_checkpoint(\"thompson\", thompson_log)\n",
    "print(f\"Thompson done. Final acc (last 20): {thompson_log.final_accuracy:.2%}\")\n",
    "print(f\"Calls: {thompson_log.call_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Running Dynamic Thompson Sampling...\")\n",
    "print(\"=\" * 60)\n",
    "thompson_dyn_log = load_checkpoint(\"thompson_dyn\")\n",
    "if thompson_dyn_log is None:\n",
    "    thompson_dyn_log = run_thompson_dynamic(problems)\n",
    "    save_checkpoint(\"thompson_dyn\", thompson_dyn_log)\n",
    "print(f\"Thompson-Dyn done. Final acc (last 20): {thompson_dyn_log.final_accuracy:.2%}\")\n",
    "print(f\"Calls: {thompson_dyn_log.call_counts}\")"
   ],
   "metadata": {
    "id": "6XvYq481Vxsz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MatuZEiKVxsz"
   },
   "outputs": [],
   "source": [
    "puct_logs = {}\n",
    "for q_mode in [\"mean\", \"ema\", \"bayesian\", \"variance\"]:\n",
    "    ckpt_name = f\"puct_{q_mode}\"\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Running PUCT-{q_mode.upper()}...\")\n",
    "    print(\"=\" * 60)\n",
    "    log = load_checkpoint(ckpt_name)\n",
    "    if log is None:\n",
    "        log = run_puct(problems, q_mode=q_mode)\n",
    "        save_checkpoint(ckpt_name, log)\n",
    "    puct_logs[q_mode] = log\n",
    "    print(f\"PUCT-{q_mode} done. Final acc (last 20): {log.final_accuracy:.2%}\")\n",
    "    print(f\"Calls: {log.call_counts}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Running AB-MCTS (Adaptive Progressive Widening)...\")\n",
    "print(\"=\" * 60)\n",
    "ab_mcts_log = load_checkpoint(\"ab_mcts\")\n",
    "if ab_mcts_log is None:\n",
    "    ab_mcts_log = run_ab_mcts(problems)\n",
    "    save_checkpoint(\"ab_mcts\", ab_mcts_log)\n",
    "print(f\"AB-MCTS done. Final acc (last 20): {ab_mcts_log.final_accuracy:.2%}\")\n",
    "print(f\"Calls: {ab_mcts_log.call_counts}\")"
   ],
   "metadata": {
    "id": "dbciz-YrVxsz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Running Discounted Thompson Sampling (gamma=0.95)...\")\n",
    "print(\"=\" * 60)\n",
    "thompson_disc_log = load_checkpoint(\"thompson_disc\")\n",
    "if thompson_disc_log is None:\n",
    "    thompson_disc_log = run_thompson_discounted(problems)\n",
    "    save_checkpoint(\"thompson_disc\", thompson_disc_log)\n",
    "print(f\"Thompson-Disc done. Final acc (last 20): {thompson_disc_log.final_accuracy:.2%}\")\n",
    "print(f\"Calls: {thompson_disc_log.call_counts}\")"
   ],
   "metadata": {
    "id": "pP3vADm2Vxs0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vOXz8rYLVxs0"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Running Best-of-N (rejection sampling baseline)...\")\n",
    "print(\"=\" * 60)\n",
    "best_of_n_log = load_checkpoint(\"best_of_n\")\n",
    "if best_of_n_log is None:\n",
    "    best_of_n_log = run_best_of_n(problems)\n",
    "    save_checkpoint(\"best_of_n\", best_of_n_log)\n",
    "print(f\"Best-of-N done. Final acc (last 20): {best_of_n_log.final_accuracy:.2%}\")\n",
    "print(f\"Calls: {best_of_n_log.call_counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uGW7XB0zVxs0"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Running UCB Bandit...\")\n",
    "print(\"=\" * 60)\n",
    "ucb_log = load_checkpoint(\"ucb_bandit\")\n",
    "if ucb_log is None:\n",
    "    ucb_log = run_ucb_bandit(problems)\n",
    "    save_checkpoint(\"ucb_bandit\", ucb_log)\n",
    "print(f\"UCB done. Final acc (last 20): {ucb_log.final_accuracy:.2%}\")\n",
    "print(f\"Calls: {ucb_log.call_counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_du2M1CNVxs0"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Running Beam Search (width=3)...\")\n",
    "print(\"=\" * 60)\n",
    "beam_log = load_checkpoint(\"beam_search\")\n",
    "if beam_log is None:\n",
    "    beam_log = run_beam_search(problems)\n",
    "    save_checkpoint(\"beam_search\", beam_log)\n",
    "print(f\"Beam done. Final acc (last 20): {beam_log.final_accuracy:.2%}\")\n",
    "print(f\"Calls: {beam_log.call_counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YrRL4A20Vxs0"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Running Discounted MCTS (gamma=0.95)...\")\n",
    "print(\"=\" * 60)\n",
    "disc_mcts_log = load_checkpoint(\"disc_mcts\")\n",
    "if disc_mcts_log is None:\n",
    "    disc_mcts_log = run_discounted_mcts(problems)\n",
    "    save_checkpoint(\"disc_mcts\", disc_mcts_log)\n",
    "print(f\"Disc-MCTS done. Final acc (last 20): {disc_mcts_log.final_accuracy:.2%}\")\n",
    "print(f\"Calls: {disc_mcts_log.call_counts}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcIOXqb2Vxs0"
   },
   "source": [
    "## 6. Analysis & Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVvIvM1rVxs0"
   },
   "outputs": [],
   "source": [
    "# Build results dict with all 14 conditions\n",
    "results = {\n",
    "    \"Majority Vote\": majority_log,\n",
    "    \"Best-of-N\": best_of_n_log,\n",
    "    \"Greedy ACE\": greedy_log,\n",
    "    \"Thompson\": thompson_log,\n",
    "    \"UCB Bandit\": ucb_log,\n",
    "    \"Thompson-Dyn\": thompson_dyn_log,\n",
    "    \"Thompson-Disc\": thompson_disc_log,\n",
    "    \"Beam Search\": beam_log,\n",
    "}\n",
    "for q_mode, log in puct_logs.items():\n",
    "    results[f\"PUCT-{q_mode.upper()}\"] = log\n",
    "results[\"AB-MCTS\"] = ab_mcts_log\n",
    "results[\"Disc-MCTS\"] = disc_mcts_log\n",
    "\n",
    "COLORS = {\n",
    "    \"Majority Vote\": \"#9467bd\",\n",
    "    \"Best-of-N\": \"#aec7e8\",\n",
    "    \"Greedy ACE\": \"#1f77b4\",\n",
    "    \"Thompson\": \"#8c564b\",\n",
    "    \"UCB Bandit\": \"#c49c94\",\n",
    "    \"Thompson-Dyn\": \"#e377c2\",\n",
    "    \"Thompson-Disc\": \"#bcbd22\",\n",
    "    \"Beam Search\": \"#98df8a\",\n",
    "    \"PUCT-MEAN\": \"#ff7f0e\",\n",
    "    \"PUCT-EMA\": \"#2ca02c\",\n",
    "    \"PUCT-BAYESIAN\": \"#d62728\",\n",
    "    \"PUCT-VARIANCE\": \"#7f7f7f\",\n",
    "    \"AB-MCTS\": \"#17becf\",\n",
    "    \"Disc-MCTS\": \"#dbdb8d\",\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "fig.suptitle(\"ACE Search Strategy Comparison (GSM8K, 50 problems)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# --- (a) Running accuracy curves ---\n",
    "ax = axes[0, 0]\n",
    "for name, log in results.items():\n",
    "    acc = log.running_accuracy\n",
    "    ax.plot(range(1, len(acc) + 1), acc, label=name, color=COLORS[name], linewidth=2)\n",
    "ax.set_xlabel(\"Problems Solved\")\n",
    "ax.set_ylabel(\"Running Accuracy\")\n",
    "ax.set_title(\"(a) Running Accuracy Over Time\")\n",
    "ax.legend(fontsize=5, loc=\"lower right\", ncol=2)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# --- (b) Final accuracy bar chart with bootstrap CI ---\n",
    "ax = axes[0, 1]\n",
    "\n",
    "def bootstrap_ci(data, n_boot=1000, ci=0.95):\n",
    "    data = np.array(data, dtype=float)\n",
    "    means = [np.mean(np.random.choice(data, size=len(data), replace=True)) for _ in range(n_boot)]\n",
    "    lo = np.percentile(means, (1 - ci) / 2 * 100)\n",
    "    hi = np.percentile(means, (1 + ci) / 2 * 100)\n",
    "    return np.mean(data), lo, hi\n",
    "\n",
    "names = list(results.keys())\n",
    "means, lows, highs = [], [], []\n",
    "for name in names:\n",
    "    tail = results[name].correct[-20:]\n",
    "    m, lo, hi = bootstrap_ci(tail)\n",
    "    means.append(m)\n",
    "    lows.append(m - lo)\n",
    "    highs.append(hi - m)\n",
    "\n",
    "bars = ax.bar(names, means, color=[COLORS[n] for n in names], yerr=[lows, highs], capsize=3)\n",
    "ax.set_ylabel(\"Accuracy (last 20 problems)\")\n",
    "ax.set_title(\"(b) Final Accuracy Comparison\")\n",
    "ax.set_ylim(0, 1)\n",
    "ax.tick_params(axis='x', rotation=45, labelsize=4)\n",
    "for bar, m in zip(bars, means):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f\"{m:.0%}\",\n",
    "            ha=\"center\", fontsize=5)\n",
    "\n",
    "# --- (c) Playbook size over time ---\n",
    "ax = axes[1, 0]\n",
    "for name, log in results.items():\n",
    "    sizes = log.playbook_sizes\n",
    "    ax.plot(range(1, len(sizes) + 1), sizes, label=name, color=COLORS[name], linewidth=2)\n",
    "ax.set_xlabel(\"Problems Solved\")\n",
    "ax.set_ylabel(\"Playbook Bullets\")\n",
    "ax.set_title(\"(c) Playbook Size Over Time\")\n",
    "ax.legend(fontsize=5, ncol=2)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- (d) Cost breakdown ---\n",
    "ax = axes[1, 1]\n",
    "roles = [\"generate\", \"reflect\", \"curate\"]\n",
    "x = np.arange(len(names))\n",
    "width = 0.22\n",
    "for i, role in enumerate(roles):\n",
    "    counts = [results[n].call_counts.get(role, 0) for n in names]\n",
    "    ax.bar(x + i * width, counts, width, label=role.capitalize())\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(names, fontsize=3.5, rotation=45)\n",
    "ax.set_ylabel(\"LLM Calls\")\n",
    "ax.set_title(\"(d) LLM Call Breakdown\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"search_ace_results.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Plots saved to search_ace_results.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPscJerlVxs0"
   },
   "outputs": [],
   "source": [
    "# --- Statistical comparison ---\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Statistical Comparison (Bootstrap 95% CI on accuracy difference vs Greedy)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def bootstrap_diff_ci(a, b, n_boot=2000):\n",
    "    a, b = np.array(a, dtype=float), np.array(b, dtype=float)\n",
    "    diffs = []\n",
    "    for _ in range(n_boot):\n",
    "        ia = np.random.choice(len(a), len(a), replace=True)\n",
    "        ib = np.random.choice(len(b), len(b), replace=True)\n",
    "        diffs.append(np.mean(b[ib]) - np.mean(a[ia]))\n",
    "    lo, hi = np.percentile(diffs, [2.5, 97.5])\n",
    "    return np.mean(diffs), lo, hi\n",
    "\n",
    "greedy_tail = greedy_log.correct[-20:]\n",
    "\n",
    "# Each condition vs Greedy\n",
    "comparisons = [\n",
    "    (\"Majority Vote\", majority_log.correct[-20:]),\n",
    "    (\"Best-of-N\", best_of_n_log.correct[-20:]),\n",
    "    (\"Thompson\", thompson_log.correct[-20:]),\n",
    "    (\"UCB Bandit\", ucb_log.correct[-20:]),\n",
    "    (\"Thompson-Dyn\", thompson_dyn_log.correct[-20:]),\n",
    "    (\"Thompson-Disc\", thompson_disc_log.correct[-20:]),\n",
    "    (\"Beam Search\", beam_log.correct[-20:]),\n",
    "]\n",
    "for q_mode in [\"mean\", \"ema\", \"bayesian\", \"variance\"]:\n",
    "    comparisons.append((f\"PUCT-{q_mode.upper()}\", puct_logs[q_mode].correct[-20:]))\n",
    "comparisons.append((\"AB-MCTS\", ab_mcts_log.correct[-20:]))\n",
    "comparisons.append((\"Disc-MCTS\", disc_mcts_log.correct[-20:]))\n",
    "\n",
    "for name, tail in comparisons:\n",
    "    mean_diff, lo, hi = bootstrap_diff_ci(greedy_tail, tail)\n",
    "    sig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\n",
    "    print(f\"{name} vs Greedy: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")\n",
    "\n",
    "# Thompson vs UCB (isolate randomness variable)\n",
    "print()\n",
    "t_tail = thompson_log.correct[-20:]\n",
    "ucb_tail = ucb_log.correct[-20:]\n",
    "mean_diff, lo, hi = bootstrap_diff_ci(t_tail, ucb_tail)\n",
    "sig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\n",
    "print(f\"UCB vs Thompson: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")\n",
    "\n",
    "# Beam Search vs Greedy (does width>1 help?)\n",
    "beam_tail = beam_log.correct[-20:]\n",
    "mean_diff, lo, hi = bootstrap_diff_ci(greedy_tail, beam_tail)\n",
    "sig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\n",
    "print(f\"Beam vs Greedy: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")\n",
    "\n",
    "# Beam Search vs PUCT-Mean (beam vs tree)\n",
    "pm_tail = puct_logs[\"mean\"].correct[-20:]\n",
    "mean_diff, lo, hi = bootstrap_diff_ci(pm_tail, beam_tail)\n",
    "sig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\n",
    "print(f\"Beam vs PUCT-Mean: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")\n",
    "\n",
    "# Disc-MCTS vs PUCT-Mean (does discounting help in tree search?)\n",
    "disc_tail = disc_mcts_log.correct[-20:]\n",
    "mean_diff, lo, hi = bootstrap_diff_ci(pm_tail, disc_tail)\n",
    "sig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\n",
    "print(f\"Disc-MCTS vs PUCT-Mean: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")\n",
    "\n",
    "# Thompson-Dyn vs static Thompson\n",
    "print()\n",
    "td_tail = thompson_dyn_log.correct[-20:]\n",
    "mean_diff, lo, hi = bootstrap_diff_ci(t_tail, td_tail)\n",
    "sig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\n",
    "print(f\"Thompson-Dyn vs Thompson: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")\n",
    "\n",
    "# Thompson-Disc vs static Thompson\n",
    "tdisc_tail = thompson_disc_log.correct[-20:]\n",
    "mean_diff, lo, hi = bootstrap_diff_ci(t_tail, tdisc_tail)\n",
    "sig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\n",
    "print(f\"Thompson-Disc vs Thompson: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")\n",
    "\n",
    "# Thompson vs each PUCT\n",
    "print()\n",
    "for q_mode in [\"mean\", \"ema\", \"bayesian\", \"variance\"]:\n",
    "    p_tail = puct_logs[q_mode].correct[-20:]\n",
    "    mean_diff, lo, hi = bootstrap_diff_ci(t_tail, p_tail)\n",
    "    sig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\n",
    "    print(f\"PUCT-{q_mode.upper()} vs Thompson: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")\n",
    "\n",
    "# AB-MCTS vs PUCT-Bayesian\n",
    "print()\n",
    "ab_tail = ab_mcts_log.correct[-20:]\n",
    "pb_tail = puct_logs[\"bayesian\"].correct[-20:]\n",
    "mean_diff, lo, hi = bootstrap_diff_ci(pb_tail, ab_tail)\n",
    "sig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\n",
    "print(f\"AB-MCTS vs PUCT-Bayesian: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")\n",
    "\n",
    "# PUCT-VARIANCE vs PUCT-Bayesian\n",
    "pv_tail = puct_logs[\"variance\"].correct[-20:]\n",
    "mean_diff, lo, hi = bootstrap_diff_ci(pb_tail, pv_tail)\n",
    "sig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\n",
    "print(f\"PUCT-VARIANCE vs PUCT-Bayesian: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")\n",
    "\n",
    "# Best-of-N vs Majority Vote (same budget, different selection)\n",
    "bon_tail = best_of_n_log.correct[-20:]\n",
    "mv_tail = majority_log.correct[-20:]\n",
    "mean_diff, lo, hi = bootstrap_diff_ci(mv_tail, bon_tail)\n",
    "sig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\n",
    "print(f\"Best-of-N vs Majority Vote: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGUc6ZL3Vxs0"
   },
   "source": [
    "## 7. Results Summary\n",
    "\n",
    "### Experiment Design\n",
    "- **Dataset**: GSM8K (50 problems, shuffled, seed=42)\n",
    "- **Model**: Qwen2.5-7B-Instruct (local via vLLM, bfloat16)\n",
    "- **14 conditions** spanning no-evolution -> flat bandit -> beam search -> sequential -> tree search -> adaptive tree search\n",
    "- **GPU optimization**: AsyncOpenAI with Semaphore(64), prefix caching, virtual loss parallel PUCT\n",
    "\n",
    "### Budget Comparison\n",
    "| Condition | Generate | Reflect | Curate | Total |\n",
    "|-----------|----------|---------|--------|-------|\n",
    "| Majority Vote | 100 | 0 | 0 | 100 |\n",
    "| Best-of-N | 100 | 0 | 0 | 100 |\n",
    "| Greedy ACE | 50 | 50 | 10 | 110 |\n",
    "| Thompson | 50 | 50 | 5 | 105 |\n",
    "| UCB Bandit | 50 | 50 | 5 | 105 |\n",
    "| Thompson-Disc | 50 | 50 | 5 | 105 |\n",
    "| Thompson-Dyn | 50 | 50 | ~9 | ~109 |\n",
    "| Beam Search | ~150 | ~150 | ~27 | ~327 |\n",
    "| PUCT-Mean | 50 | 50 | <=17 | <=117 |\n",
    "| PUCT-EMA | 50 | 50 | <=17 | <=117 |\n",
    "| PUCT-Bayesian | 50 | 50 | <=17 | <=117 |\n",
    "| PUCT-Variance | 50 | 50 | <=17 | <=117 |\n",
    "| AB-MCTS | 50 | 50 | variable | <=117 |\n",
    "| Disc-MCTS | 50 | 50 | <=17 | <=117 |\n",
    "\n",
    "### Strategy Taxonomy\n",
    "| Strategy | Search Type | Evolves Playbook? | Exploration Mechanism | Parallelization |\n",
    "|----------|------------|-------------------|----------------------|------------------|\n",
    "| Majority Vote | None | No | Sample diversity only | All 100 calls parallel |\n",
    "| Best-of-N | None | No | Rejection sampling (consistency as confidence) | All 100 calls parallel |\n",
    "| Greedy ACE | Sequential | Yes | None (greedy) | Batch 5 gen + 5 ref between curate |\n",
    "| Thompson | Flat bandit | Pool of variants (fixed) | Beta posterior sampling | Seed phase parallel |\n",
    "| UCB Bandit | Flat bandit | Pool of variants (fixed) | Deterministic UCB1 bound | Seed phase parallel |\n",
    "| Thompson-Disc | Flat bandit | Pool of variants (fixed) | Discounted Beta posterior (gamma=0.95) | Seed phase parallel |\n",
    "| Thompson-Dyn | Flat bandit | Pool grows dynamically | Beta posterior + periodic arm addition | Seed phase parallel |\n",
    "| Beam Search | Width-K beam | Yes (prune + curate each round) | K parallel candidates, top-K survival | All K beams evaluated in parallel |\n",
    "| PUCT-Mean | Tree | Yes (progressive widening) | UCB with mean Q | Virtual loss, K=4 leaves |\n",
    "| PUCT-EMA | Tree | Yes (progressive widening) | UCB with recency-weighted Q (alpha=0.4) | Virtual loss, K=4 leaves |\n",
    "| PUCT-Bayesian | Tree | Yes (progressive widening) | UCB with shrinkage-to-0.5 Q | Virtual loss, K=4 leaves |\n",
    "| PUCT-Variance | Tree | Yes (progressive widening) | UCB with variance-aware Q (arXiv:2512.21648) | Virtual loss, K=4 leaves |\n",
    "| AB-MCTS | Tree (adaptive) | Yes (adaptive widening) | UCB + Thompson-sampled wider-vs-deeper | Async batch eval |\n",
    "| Disc-MCTS | Tree (discounted) | Yes (progressive widening) | UCB with gamma-discounted Q (gamma=0.95) | Async batch eval |\n",
    "\n",
    "### Key Questions\n",
    "1. **Does evolution help at all?** Majority Vote vs Greedy — if MajVote wins, sampling > evolution\n",
    "2. **Does smart selection beat random?** Best-of-N vs Majority Vote — same budget, different selection\n",
    "3. **Flat vs tree?** Thompson vs PUCT — is tree structure worth the overhead at 50 problems?\n",
    "4. **Beam vs tree?** Beam Search vs PUCT — does the simpler beam approach capture most of tree search gains?\n",
    "5. **Stochastic vs deterministic bandits?** Thompson vs UCB — does randomized exploration beat deterministic UCB1?\n",
    "6. **Does dynamic pool help?** Thompson-Dyn vs Thompson — does periodic arm addition improve over frozen pool?\n",
    "7. **Does discounting help in bandits?** Thompson-Disc vs Thompson — does forgetting old observations improve adaptation?\n",
    "8. **Does discounting help in trees?** Disc-MCTS vs PUCT-Mean — does gamma-decay in tree nodes help non-stationarity?\n",
    "9. **Which Q estimator?** Mean vs EMA vs Bayesian vs Variance — does exploration strategy matter for PUCT?\n",
    "10. **Fixed vs adaptive branching?** AB-MCTS vs PUCT-Bayesian — does Thompson-sampled wider/deeper beat fixed progressive widening?\n",
    "11. **Free lunch?** Does any strategy consistently beat Greedy with matched budget?\n",
    "\n",
    "### New Condition Designs\n",
    "\n",
    "#### Best-of-N (Rejection Sampling)\n",
    "Generates N solutions per problem with the static initial playbook, picks the most frequently occurring answer (consistency as a proxy for confidence). Same budget as Majority Vote (100 generate calls). Tests whether a \"smart\" tree search actually beats simply generating many random variations and picking the most consistent one.\n",
    "\n",
    "#### Beam Search (Width-K)\n",
    "The \"missing link\" between Greedy (width=1) and MCTS (complex tree). Maintains K=3 candidate playbooks. Each round: (1) evaluate all K beams on the same problem batch in parallel, (2) rank by cumulative accuracy, (3) prune to top-K, (4) curate each survivor. Hypothesis: captures ~80% of tree search gains at ~20% of the cost. Note: higher total budget (~327 calls) because all K beams are evaluated on every batch — the tradeoff is breadth of exploration vs total compute.\n",
    "\n",
    "#### UCB Bandit (Deterministic)\n",
    "Standard UCB1 (Auer et al., 2002) as the deterministic counterpart to Thompson Sampling. Selects: argmax_k [mean_reward_k + sqrt(2) * sqrt(ln(t) / n_k)]. Same seed/pool structure as Thompson. Isolates the \"randomness\" variable: Thompson uses stochastic posterior sampling, UCB uses a deterministic confidence bound. Thompson is usually better for exploration, but UCB is often more sample-efficient.\n",
    "\n",
    "#### Discounted MCTS (gamma=0.95)\n",
    "Applies the non-stationarity fix from Thompson-Disc but inside the MCTS tree. Standard PUCT suffers when a child node improves (via curation) but the parent's average Q is stale. Discounted MCTS uses exponentially weighted mean Q-values where recent rewards have weight 1 and older rewards decay by gamma=0.95 per step. This lets the tree adapt to improving playbooks without needing the EMA estimator's fixed alpha.\n",
    "\n",
    "### Previous Condition Designs\n",
    "\n",
    "#### Thompson-Disc (non-stationary bandits, arXiv:2305.10718)\n",
    "Thompson-Disc applies exponential discounting to Beta posteriors AFTER each update:\n",
    "- After each round, multiply all arms' alpha and beta by gamma=0.95\n",
    "- Floor at 0.1 (not 1.0) to allow proper forgetting\n",
    "- Effective lookback window: 1/(1-gamma) = 20 problems\n",
    "\n",
    "#### AB-MCTS (Adaptive Progressive Widening, inspired by arXiv:2503.04412)\n",
    "Replaces PUCT's fixed progressive widening with an adaptive decision per node:\n",
    "- Each node maintains Beta(expand_alpha, expand_beta) posterior\n",
    "- Thompson-sample to decide: go WIDER (curate new child) or go DEEPER (re-evaluate)\n",
    "- Posterior updated only on direct evidence (no counterfactual assumptions)\n",
    "\n",
    "#### PUCT-Variance (arXiv:2512.21648)\n",
    "Variance-aware Q-estimator: Q = mean(rewards) + 0.5 * sqrt(var(rewards) / n)\n",
    "- High variance nodes get explored more (UCB-V style)\n",
    "- Hypothesis: better calibrated exploration for heteroscedastic rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# --- 8. Save & Export Results ---\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "def generate_report(results, filename=\"experiment_report.md\"):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"# Search-Augmented ACE: Experiment Report\\n\\n\")\n",
    "        f.write(f\"**Date:** {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "\n",
    "        f.write(\"## 1. Final Accuracy Summary\\n\")\n",
    "        f.write(\"| Strategy | Accuracy (Last 20) | Budget Used |\\n\")\n",
    "        f.write(\"| :--- | :--- | :--- |\\n\")\n",
    "\n",
    "        for name, log in results.items():\n",
    "            acc = log.final_accuracy\n",
    "            calls = sum(log.call_counts.values())\n",
    "            f.write(f\"| {name} | {acc:.2%} | {calls} |\\n\")\n",
    "\n",
    "        f.write(\"\\n## 2. Strategy Details\\n\")\n",
    "        for name, log in results.items():\n",
    "            f.write(f\"### {name}\\n\")\n",
    "            f.write(f\"- **Final Playbook Size:** {log.playbook_sizes[-1] if log.playbook_sizes else 0} bullets\\n\")\n",
    "            f.write(f\"- **Call Breakdown:** {dict(log.call_counts)}\\n\")\n",
    "            if log.final_playbook:\n",
    "                f.write(\"- **Final Playbook Preview:**\\n\")\n",
    "                content = log.final_playbook.to_str()\n",
    "                # Truncate if too long for report\n",
    "                preview = content[:500] + \"...\" if len(content) > 500 else content\n",
    "                f.write(f\"```\\n{preview}\\n```\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "# 1. Generate human-readable report\n",
    "generate_report(results, \"ace_experiment_summary.md\")\n",
    "print(\"Generated readable report: ace_experiment_summary.md\")\n",
    "\n",
    "# 2. Archive everything (Checkpoints + Plot + Report)\n",
    "# This allows you to 'reanalyze' later by loading the .pkl files\n",
    "shutil.make_archive('ace_experiment_data', 'zip', '.', 'checkpoints')\n",
    "# Manually add the report and plot to a final zip if they aren't in checkpoints\n",
    "final_zip = \"full_experiment_results.zip\"\n",
    "os.system(f\"zip -r {final_zip} checkpoints ace_experiment_summary.md search_ace_results.png\")\n",
    "\n",
    "print(f\"Created archive: {final_zip}\")\n",
    "\n",
    "# 3. Download to your local machine\n",
    "try:\n",
    "    files.download(final_zip)\n",
    "except Exception as e:\n",
    "    print(f\"Auto-download failed (browser block?). Download '{final_zip}' manually from the file explorer on the left.\")"
   ],
   "metadata": {
    "id": "5tdNLxHcWEi7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2USvf2dqVxs1"
   },
   "outputs": [],
   "source": [
    "# --- Cleanup: kill vLLM server ---\n",
    "try:\n",
    "    os.kill(vllm_proc.pid, signal.SIGTERM)\n",
    "    print(f\"Killed vLLM server (PID {vllm_proc.pid})\")\n",
    "except ProcessLookupError:\n",
    "    print(\"vLLM server already stopped\")"
   ]
  }
 ]
}
