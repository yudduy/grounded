{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Search-Augmented ACE: Search Strategy Comparison\n\n**Hypothesis**: Different search strategies over ACE-style playbook space trade off exploration, exploitation, and efficiency differently. We compare ten conditions spanning no-evolution baselines through flat bandits to tree search.\n\n**Ten conditions** (matched LLM budget ~100-120 calls):\n1. **Majority Vote** — no evolution, 2 samples per problem, majority answer (null hypothesis)\n2. **Greedy ACE** — sequential generate -> reflect -> curate (standard ACE baseline)\n3. **Thompson Sampling** — flat bandit over a pool of curated playbook variants\n4. **PUCT-Mean** — tree search, Q = mean reward\n5. **PUCT-EMA** — tree search, Q = exponential moving average (alpha=0.4)\n6. **PUCT-Bayesian** — tree search, Q = Beta posterior mean\n7. **PUCT-Variance** — tree search, Q = mean + c*sqrt(var/n) (arXiv:2512.21648)\n8. **Dynamic Thompson** — Thompson with periodic arm addition from best arm\n9. **AB-MCTS** — Adaptive Progressive Widening: Thompson-sampled wider-vs-deeper per node\n10. **Thompson-Disc** — Discounted Thompson Sampling (gamma=0.95) for non-stationary playbook performance\n\n**Setup**: Qwen2.5-7B-Instruct via vLLM on A100 (bfloat16, prefix caching, async parallel eval).\n\n**GPU Optimization**: All conditions use AsyncOpenAI with semaphore-based concurrency (64 concurrent requests). Majority Vote fires all 100 calls in parallel. Greedy ACE batches 5 generate+reflect calls between curate intervals. PUCT uses virtual loss for parallel multi-leaf evaluation. vLLM configured with `--enable-prefix-caching` (playbook system prompts shared across problems, ~13% throughput gain)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%capture\n!pip install vllm==0.6.6 openai==1.58.1 datasets==3.2.0 matplotlib==3.9.3 numpy==1.26.4 nest_asyncio==1.6.0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess\nimport time\nimport os\nimport signal\nimport json\nimport re\nimport copy\nimport math\nimport random\nimport pickle\nimport asyncio\nimport nest_asyncio\nfrom dataclasses import dataclass, field\nfrom typing import List, Dict, Tuple, Optional\nfrom collections import defaultdict, Counter\nfrom pathlib import Path\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom openai import OpenAI, AsyncOpenAI\n\n# Allow nested event loops (required for Colab/Jupyter)\nnest_asyncio.apply()\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\n# Checkpoint directory\nCHECKPOINT_DIR = Path(\"checkpoints\")\nCHECKPOINT_DIR.mkdir(exist_ok=True)\n\n# Concurrency for async LLM calls (tuned for 7B on A100 40GB)\nMAX_CONCURRENT_LLM = 64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Launch vLLM server in background — optimized for A100\nMODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\nVLLM_PORT = 8000\n\nvllm_proc = subprocess.Popen(\n    [\n        \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n        \"--model\", MODEL_NAME,\n        \"--port\", str(VLLM_PORT),\n        \"--max-model-len\", \"8192\",\n        \"--gpu-memory-utilization\", \"0.90\",\n        \"--dtype\", \"bfloat16\",\n        \"--max-num-seqs\", \"1024\",\n        \"--max-num-batched-tokens\", \"16384\",\n        \"--enable-prefix-caching\",\n    ],\n    stdout=subprocess.PIPE,\n    stderr=subprocess.STDOUT,\n)\nprint(f\"vLLM server PID: {vllm_proc.pid}\")\n\n# Wait for server to be ready\nclient = OpenAI(base_url=f\"http://localhost:{VLLM_PORT}/v1\", api_key=\"dummy\")\naclient = AsyncOpenAI(base_url=f\"http://localhost:{VLLM_PORT}/v1\", api_key=\"dummy\")\n\nfor attempt in range(120):\n    try:\n        client.models.list()\n        print(f\"vLLM ready after {attempt + 1}s\")\n        break\n    except Exception:\n        time.sleep(1)\nelse:\n    raise RuntimeError(\"vLLM server did not start within 120s\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GSM8K Data Loading & Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset\n\nds = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n\ndef extract_gsm8k_answer(answer_text: str) -> str:\n    \"\"\"Extract numeric answer from GSM8K '#### <number>' format.\"\"\"\n    match = re.search(r\"####\\s*(-?[\\d,]+\\.?\\d*)\", answer_text)\n    if match:\n        return match.group(1).replace(\",\", \"\").strip()\n    # Fallback: last number in text\n    nums = re.findall(r\"-?[\\d,]+\\.?\\d*\", answer_text)\n    if nums:\n        return nums[-1].replace(\",\", \"\")\n    return \"\"\n\nproblems = []\nfor item in ds:\n    problems.append({\n        \"question\": item[\"question\"],\n        \"answer\": extract_gsm8k_answer(item[\"answer\"]),\n        \"full_answer\": item[\"answer\"],\n    })\n\n# Use first 50 problems, shuffled deterministically\nrng = random.Random(SEED)\nrng.shuffle(problems)\nproblems = problems[:50]\n\n# --- Data validation ---\nassert len(problems) == 50, f\"Expected 50 problems, got {len(problems)}\"\nassert all(p[\"answer\"] for p in problems), \"Found empty ground truth answers\"\nassert all(p[\"question\"].strip() for p in problems), \"Found empty questions\"\nfor p in problems:\n    try:\n        float(p[\"answer\"].replace(\",\", \"\"))\n    except ValueError:\n        raise ValueError(f\"Non-numeric ground truth: {p['answer']}\")\n\nprint(f\"Loaded {len(problems)} GSM8K problems (all validated)\")\nprint(f\"Example: Q='{problems[0]['question'][:80]}...' A={problems[0]['answer']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core Components: Playbook, Generator, Reflector, Curator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Playbook representation ---\n",
    "\n",
    "@dataclass\n",
    "class Bullet:\n",
    "    id: str\n",
    "    section: str  # STRATEGIES, COMMON_MISTAKES, SOLUTION_PATTERNS\n",
    "    content: str\n",
    "    helpful: int = 0\n",
    "    harmful: int = 0\n",
    "\n",
    "    def to_str(self) -> str:\n",
    "        return f\"[{self.id}] helpful={self.helpful} harmful={self.harmful} :: {self.content}\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Playbook:\n",
    "    bullets: List[Bullet] = field(default_factory=list)\n",
    "    _next_id: int = 1\n",
    "\n",
    "    def add(self, section: str, content: str) -> str:\n",
    "        prefix = {\"STRATEGIES\": \"str\", \"COMMON_MISTAKES\": \"err\", \"SOLUTION_PATTERNS\": \"sol\"}.get(section, \"gen\")\n",
    "        bid = f\"{prefix}-{self._next_id:05d}\"\n",
    "        self._next_id += 1\n",
    "        self.bullets.append(Bullet(id=bid, section=section, content=content))\n",
    "        return bid\n",
    "\n",
    "    def remove(self, bid: str):\n",
    "        self.bullets = [b for b in self.bullets if b.id != bid]\n",
    "\n",
    "    def update(self, bid: str, content: str):\n",
    "        for b in self.bullets:\n",
    "            if b.id == bid:\n",
    "                b.content = content\n",
    "                return\n",
    "\n",
    "    def tag(self, bid: str, label: str):\n",
    "        for b in self.bullets:\n",
    "            if b.id == bid:\n",
    "                if label == \"helpful\":\n",
    "                    b.helpful += 1\n",
    "                elif label == \"harmful\":\n",
    "                    b.harmful += 1\n",
    "\n",
    "    def to_str(self) -> str:\n",
    "        sections = defaultdict(list)\n",
    "        for b in self.bullets:\n",
    "            sections[b.section].append(b.to_str())\n",
    "        parts = []\n",
    "        for sec in [\"STRATEGIES\", \"COMMON_MISTAKES\", \"SOLUTION_PATTERNS\"]:\n",
    "            if sections[sec]:\n",
    "                parts.append(f\"## {sec}\")\n",
    "                parts.extend(sections[sec])\n",
    "        return \"\\n\".join(parts) if parts else \"(empty playbook)\"\n",
    "\n",
    "    def copy(self) -> \"Playbook\":\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return len(self.bullets)\n",
    "\n",
    "\n",
    "def make_initial_playbook() -> Playbook:\n",
    "    pb = Playbook()\n",
    "    pb.add(\"STRATEGIES\", \"Break word problems into step-by-step arithmetic.\")\n",
    "    pb.add(\"STRATEGIES\", \"Identify what quantity the question asks for before computing.\")\n",
    "    pb.add(\"COMMON_MISTAKES\", \"Watch for unit conversions (hours to minutes, etc).\")\n",
    "    return pb\n",
    "\n",
    "print(make_initial_playbook().to_str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- LLM call wrappers (sync + async) ---\n\ncall_counter = defaultdict(int)  # track calls by role\n_llm_semaphore = asyncio.Semaphore(MAX_CONCURRENT_LLM)\n\ndef llm_call(system: str, user: str, role: str = \"generate\", temperature: float = 0.7, max_tokens: int = 1024) -> str:\n    \"\"\"Single synchronous LLM call via vLLM OpenAI-compatible API.\"\"\"\n    call_counter[role] += 1\n    try:\n        resp = client.chat.completions.create(\n            model=MODEL_NAME,\n            messages=[\n                {\"role\": \"system\", \"content\": system},\n                {\"role\": \"user\", \"content\": user},\n            ],\n            temperature=temperature,\n            max_tokens=max_tokens,\n        )\n        return resp.choices[0].message.content.strip()\n    except Exception as e:\n        print(f\"LLM call failed ({role}): {e}\")\n        return \"\"\n\nasync def llm_call_async(system: str, user: str, role: str = \"generate\",\n                         temperature: float = 0.7, max_tokens: int = 1024) -> str:\n    \"\"\"Async LLM call with semaphore-based concurrency control.\"\"\"\n    call_counter[role] += 1\n    async with _llm_semaphore:\n        try:\n            resp = await aclient.chat.completions.create(\n                model=MODEL_NAME,\n                messages=[\n                    {\"role\": \"system\", \"content\": system},\n                    {\"role\": \"user\", \"content\": user},\n                ],\n                temperature=temperature,\n                max_tokens=max_tokens,\n            )\n            return resp.choices[0].message.content.strip()\n        except Exception as e:\n            print(f\"Async LLM call failed ({role}): {e}\")\n            return \"\"\n\ndef reset_call_counter():\n    global call_counter\n    call_counter = defaultdict(int)\n\ndef get_call_counts() -> Dict[str, int]:\n    return dict(call_counter)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Generator (sync + async) ---\n\ndef _build_generate_prompts(question: str, playbook: Playbook) -> Tuple[str, str]:\n    \"\"\"Build system/user prompts for generation.\"\"\"\n    pb_text = playbook.to_str()\n    system = (\n        \"You are a math problem solver. Use the playbook strategies below to help solve the problem.\\n\"\n        \"When you use a specific strategy, mention its ID (e.g., [str-00001]).\\n\"\n        \"Show your work step-by-step, then give the final numeric answer on its own line as: #### <number>\\n\\n\"\n        f\"PLAYBOOK:\\n{pb_text}\"\n    )\n    user = f\"Solve this problem:\\n{question}\"\n    return system, user\n\ndef _parse_generate_response(raw: str) -> Tuple[str, List[str]]:\n    \"\"\"Extract answer and bullet references from raw LLM response.\"\"\"\n    answer = \"\"\n    m = re.search(r\"####\\s*(-?[\\d,]+\\.?\\d*)\", raw)\n    if m:\n        answer = m.group(1).replace(\",\", \"\").strip()\n    else:\n        nums = re.findall(r\"-?[\\d,]+\\.?\\d*\", raw)\n        if nums:\n            answer = nums[-1].replace(\",\", \"\")\n    bullets_used = re.findall(r\"\\[(\\w+-\\d+)\\]\", raw)\n    return answer, bullets_used\n\ndef generate(question: str, playbook: Playbook) -> Tuple[str, List[str], str]:\n    \"\"\"Generate a solution to a math problem using the playbook (sync).\"\"\"\n    system, user = _build_generate_prompts(question, playbook)\n    raw = llm_call(system, user, role=\"generate\")\n    answer, bullets_used = _parse_generate_response(raw)\n    return answer, bullets_used, raw\n\nasync def generate_async(question: str, playbook: Playbook) -> Tuple[str, List[str], str]:\n    \"\"\"Generate a solution to a math problem using the playbook (async).\"\"\"\n    system, user = _build_generate_prompts(question, playbook)\n    raw = await llm_call_async(system, user, role=\"generate\")\n    answer, bullets_used = _parse_generate_response(raw)\n    return answer, bullets_used, raw"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Answer comparison ---\n",
    "\n",
    "def answers_match(predicted: str, ground_truth: str) -> bool:\n",
    "    \"\"\"Compare numeric answers with tolerance.\"\"\"\n",
    "    try:\n",
    "        p = float(predicted.replace(\",\", \"\"))\n",
    "        g = float(ground_truth.replace(\",\", \"\"))\n",
    "        return abs(p - g) < 1e-3\n",
    "    except (ValueError, TypeError):\n",
    "        return predicted.strip() == ground_truth.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Reflector (sync + async) ---\n\ndef _build_reflect_prompts(question: str, raw_response: str, predicted: str, ground_truth: str,\n                           bullets_used: List[str], playbook: Playbook) -> Tuple[str, str]:\n    \"\"\"Build system/user prompts for reflection.\"\"\"\n    correct = answers_match(predicted, ground_truth)\n    feedback = \"CORRECT\" if correct else f\"INCORRECT (predicted {predicted}, expected {ground_truth})\"\n\n    bullets_text = \"\"\n    for b in playbook.bullets:\n        if b.id in bullets_used:\n            bullets_text += f\"  {b.to_str()}\\n\"\n\n    system = (\n        \"You are a math reasoning analyst. Analyze whether the solution approach was correct \"\n        \"and whether the playbook strategies used were helpful or harmful.\\n\"\n        \"For each bullet ID used, output a JSON line: {\\\"id\\\": \\\"str-00001\\\", \\\"tag\\\": \\\"helpful\\\"}\\n\"\n        \"Tags must be one of: helpful, harmful, neutral.\\n\"\n        \"End with a brief reflection paragraph.\"\n    )\n    user = (\n        f\"Problem: {question}\\n\\n\"\n        f\"Solution attempt:\\n{raw_response}\\n\\n\"\n        f\"Result: {feedback}\\n\\n\"\n        f\"Playbook bullets used:\\n{bullets_text}\"\n    )\n    return system, user\n\ndef _parse_reflect_response(raw: str, bullets_used: List[str], correct: bool) -> Dict[str, str]:\n    \"\"\"Parse bullet tags from reflection response.\"\"\"\n    tags = {}\n    for m in re.finditer(r'\"id\"\\s*:\\s*\"([^\"]+)\".*?\"tag\"\\s*:\\s*\"(helpful|harmful|neutral)\"', raw):\n        tags[m.group(1)] = m.group(2)\n    if not tags:\n        for bid in bullets_used:\n            tags[bid] = \"helpful\" if correct else \"neutral\"\n    return tags\n\ndef reflect(question: str, raw_response: str, predicted: str, ground_truth: str,\n            bullets_used: List[str], playbook: Playbook) -> Tuple[str, Dict[str, str]]:\n    \"\"\"Reflect on the solution attempt (sync).\"\"\"\n    system, user = _build_reflect_prompts(question, raw_response, predicted, ground_truth, bullets_used, playbook)\n    raw = llm_call(system, user, role=\"reflect\", temperature=0.3)\n    correct = answers_match(predicted, ground_truth)\n    tags = _parse_reflect_response(raw, bullets_used, correct)\n    return raw, tags\n\nasync def reflect_async(question: str, raw_response: str, predicted: str, ground_truth: str,\n                        bullets_used: List[str], playbook: Playbook) -> Tuple[str, Dict[str, str]]:\n    \"\"\"Reflect on the solution attempt (async).\"\"\"\n    system, user = _build_reflect_prompts(question, raw_response, predicted, ground_truth, bullets_used, playbook)\n    raw = await llm_call_async(system, user, role=\"reflect\", temperature=0.3)\n    correct = answers_match(predicted, ground_truth)\n    tags = _parse_reflect_response(raw, bullets_used, correct)\n    return raw, tags"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Curator ---\n",
    "\n",
    "MAX_BULLETS = 20\n",
    "\n",
    "def curate(playbook: Playbook, reflection: str, question: str) -> Playbook:\n",
    "    \"\"\"\n",
    "    Curate the playbook based on reflection.\n",
    "    Returns a new (copied) playbook with operations applied.\n",
    "    \"\"\"\n",
    "    pb = playbook.copy()\n",
    "    pb_text = pb.to_str()\n",
    "\n",
    "    system = (\n",
    "        \"You are a playbook curator for math problem solving. Based on the reflection, \"\n",
    "        \"propose operations to improve the playbook.\\n\"\n",
    "        \"Output a JSON array of operations:\\n\"\n",
    "        '[{\"op\": \"ADD\", \"section\": \"STRATEGIES\", \"content\": \"new insight\"},\\n'\n",
    "        ' {\"op\": \"UPDATE\", \"id\": \"str-00001\", \"content\": \"refined text\"},\\n'\n",
    "        ' {\"op\": \"DELETE\", \"id\": \"err-00002\"}]\\n'\n",
    "        f\"Sections: STRATEGIES, COMMON_MISTAKES, SOLUTION_PATTERNS\\n\"\n",
    "        f\"Max bullets: {MAX_BULLETS}. Current: {pb.size}.\\n\"\n",
    "        \"Only propose operations that are clearly supported by the reflection. Keep it minimal.\"\n",
    "    )\n",
    "    user = (\n",
    "        f\"Current playbook:\\n{pb_text}\\n\\n\"\n",
    "        f\"Problem context: {question[:200]}\\n\\n\"\n",
    "        f\"Reflection:\\n{reflection}\"\n",
    "    )\n",
    "    raw = llm_call(system, user, role=\"curate\", temperature=0.3)\n",
    "\n",
    "    # Parse operations from JSON array\n",
    "    ops = []\n",
    "    # Try to find JSON array in response\n",
    "    json_match = re.search(r'\\[\\s*\\{.*?\\}\\s*\\]', raw, re.DOTALL)\n",
    "    if json_match:\n",
    "        try:\n",
    "            ops = json.loads(json_match.group())\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "    # Apply operations\n",
    "    for op in ops:\n",
    "        try:\n",
    "            if op.get(\"op\") == \"ADD\" and pb.size < MAX_BULLETS:\n",
    "                pb.add(op.get(\"section\", \"STRATEGIES\"), op.get(\"content\", \"\"))\n",
    "            elif op.get(\"op\") == \"UPDATE\" and op.get(\"id\"):\n",
    "                pb.update(op[\"id\"], op.get(\"content\", \"\"))\n",
    "            elif op.get(\"op\") == \"DELETE\" and op.get(\"id\"):\n",
    "                pb.remove(op[\"id\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Safety: if curator emptied the playbook, reset\n",
    "    if pb.size == 0:\n",
    "        pb = make_initial_playbook()\n",
    "        pb._next_id = playbook._next_id\n",
    "\n",
    "    return pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Search Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Shared tracking ---\n\n@dataclass\nclass RunLog:\n    \"\"\"Tracks per-problem results for a single condition.\"\"\"\n    correct: List[bool] = field(default_factory=list)\n    playbook_sizes: List[int] = field(default_factory=list)\n    call_counts: Dict[str, int] = field(default_factory=dict)\n    final_playbook: Optional[Playbook] = None\n\n    @property\n    def running_accuracy(self) -> List[float]:\n        acc = []\n        total = 0\n        for i, c in enumerate(self.correct):\n            total += int(c)\n            acc.append(total / (i + 1))\n        return acc\n\n    @property\n    def final_accuracy(self) -> float:\n        if not self.correct:\n            return 0.0\n        tail = self.correct[-20:]\n        return sum(tail) / len(tail)\n\ndef save_checkpoint(name: str, log: RunLog):\n    \"\"\"Save a condition's RunLog to disk for crash recovery.\"\"\"\n    path = CHECKPOINT_DIR / f\"{name}.pkl\"\n    with open(path, \"wb\") as f:\n        pickle.dump(log, f)\n    print(f\"  Checkpoint saved: {path}\")\n\ndef load_checkpoint(name: str) -> Optional[RunLog]:\n    \"\"\"Load a condition's RunLog from disk if it exists.\"\"\"\n    path = CHECKPOINT_DIR / f\"{name}.pkl\"\n    if path.exists():\n        with open(path, \"rb\") as f:\n            log = pickle.load(f)\n        print(f\"  Loaded checkpoint: {path} ({len(log.correct)} problems)\")\n        return log\n    return None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Strategy 1: Greedy ACE (async batched within curate intervals) ---\n\nCURATE_EVERY = 5\n\ndef run_greedy(problems: List[dict]) -> RunLog:\n    \"\"\"Sequential generate -> reflect -> curate loop, with async batching.\n\n    Within each curate interval (5 problems), we fire all generate calls in parallel,\n    then all reflect calls in parallel, then curate once (sync, since it's 1 call).\n    \"\"\"\n    reset_call_counter()\n    log = RunLog()\n    pb = make_initial_playbook()\n\n    async def _run():\n        nonlocal pb\n        for batch_start in range(0, len(problems), CURATE_EVERY):\n            batch = problems[batch_start:batch_start + CURATE_EVERY]\n\n            # Parallel generate\n            gen_tasks = [generate_async(prob[\"question\"], pb) for prob in batch]\n            gen_results = await asyncio.gather(*gen_tasks)\n\n            # Parallel reflect\n            ref_tasks = []\n            for prob, (answer, bullets_used, raw) in zip(batch, gen_results):\n                ref_tasks.append(reflect_async(\n                    prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, pb\n                ))\n            ref_results = await asyncio.gather(*ref_tasks)\n\n            # Process results sequentially (tag bullets, log)\n            last_reflection = \"\"\n            last_question = \"\"\n            for prob, (answer, bullets_used, raw), (reflection, tags) in zip(batch, gen_results, ref_results):\n                correct = answers_match(answer, prob[\"answer\"])\n                log.correct.append(correct)\n                log.playbook_sizes.append(pb.size)\n                for bid, label in tags.items():\n                    pb.tag(bid, label)\n                last_reflection = reflection\n                last_question = prob[\"question\"]\n\n            # Curate (single sync call)\n            if last_reflection:\n                pb = curate(pb, last_reflection, last_question)\n\n            done = batch_start + len(batch)\n            if done % 10 == 0 or done == len(problems):\n                acc = sum(log.correct) / len(log.correct)\n                print(f\"  Greedy [{done}/{len(problems)}] acc={acc:.2%} bullets={pb.size}\")\n\n    asyncio.run(_run())\n    log.call_counts = get_call_counts()\n    log.final_playbook = pb\n    return log"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Strategy 2: PUCT-ACE with virtual loss, async eval, Q-estimator variants ---\n\n@dataclass\nclass MCTSNode:\n    playbook: Playbook\n    parent: Optional[\"MCTSNode\"] = None\n    children: List[\"MCTSNode\"] = field(default_factory=list)\n    visits: int = 0\n    reward_history: List[float] = field(default_factory=list)\n    results: List[bool] = field(default_factory=list)\n    # Virtual loss for parallel leaf selection\n    virtual_loss_count: int = 0\n    # AB-MCTS: Beta posterior for wider-vs-deeper decision\n    expand_alpha: float = 1.0\n    expand_beta: float = 1.0\n\n    def q_value(self, mode: str = \"mean\") -> float:\n        if not self.reward_history:\n            return 0.5  # optimistic prior for unvisited nodes\n        if mode == \"mean\":\n            return sum(self.reward_history) / len(self.reward_history)\n        elif mode == \"ema\":\n            alpha = 0.4\n            q = self.reward_history[0]\n            for r in self.reward_history[1:]:\n                q = alpha * r + (1 - alpha) * q\n            return q\n        elif mode == \"bayesian\":\n            s = sum(self.reward_history)\n            n = len(self.reward_history)\n            return (s + 1) / (n + 2)\n        elif mode == \"variance\":\n            # Variance-aware Q (inspired by arXiv:2512.21648)\n            # UCB-V style: mean + c * sqrt(variance / n)\n            n = len(self.reward_history)\n            mean_q = sum(self.reward_history) / n\n            if n < 2:\n                return mean_q + 0.5  # high uncertainty bonus when few samples\n            var_q = sum((r - mean_q) ** 2 for r in self.reward_history) / n\n            return mean_q + 0.5 * math.sqrt(var_q / n)\n        return 0.5\n\n    def effective_q(self, mode: str = \"mean\") -> float:\n        \"\"\"Q-value adjusted for virtual losses (used during parallel selection).\"\"\"\n        if not self.reward_history and self.virtual_loss_count == 0:\n            return 0.5\n        effective_n = len(self.reward_history) + self.virtual_loss_count\n        if effective_n == 0:\n            return 0.5\n        real_sum = sum(self.reward_history)\n        # Virtual losses assume reward of 0\n        return real_sum / effective_n\n\n    @property\n    def effective_visits(self) -> int:\n        return self.visits + self.virtual_loss_count\n\n    def add_virtual_loss(self):\n        self.virtual_loss_count += 1\n\n    def remove_virtual_loss(self):\n        self.virtual_loss_count = max(0, self.virtual_loss_count - 1)\n\n\ndef puct_select(node: MCTSNode, c_puct: float = 1.5, q_mode: str = \"mean\",\n                use_virtual_loss: bool = False) -> MCTSNode:\n    \"\"\"Select best child via PUCT, recurse to leaf.\"\"\"\n    if not node.children:\n        return node\n    n_parent = node.effective_visits if use_virtual_loss else node.visits\n    best, best_score = None, -float(\"inf\")\n    for child in node.children:\n        prior = 1.0 / len(node.children)\n        if use_virtual_loss:\n            exploit = child.effective_q(q_mode)\n            n_child = child.effective_visits\n        else:\n            exploit = child.q_value(q_mode)\n            n_child = child.visits\n        explore = c_puct * prior * math.sqrt(n_parent) / (1 + n_child)\n        score = exploit + explore\n        if score > best_score:\n            best_score = score\n            best = child\n    return puct_select(best, c_puct, q_mode, use_virtual_loss)\n\n\ndef select_batch_with_virtual_loss(root: MCTSNode, k: int, c_puct: float = 1.5,\n                                    q_mode: str = \"mean\") -> List[Tuple[MCTSNode, List[MCTSNode]]]:\n    \"\"\"Select k diverse leaves using virtual loss to discourage repeat selection.\"\"\"\n    selections = []\n    for _ in range(k):\n        node = root\n        path = []\n        while node.children:\n            n_parent = node.effective_visits\n            best, best_score = None, -float(\"inf\")\n            for child in node.children:\n                prior = 1.0 / len(node.children)\n                exploit = child.effective_q(q_mode)\n                explore = c_puct * prior * math.sqrt(n_parent) / (1 + child.effective_visits)\n                score = exploit + explore\n                if score > best_score:\n                    best_score = score\n                    best = child\n            best.add_virtual_loss()\n            path.append(best)\n            node = best\n        selections.append((node, path))\n    return selections\n\n\ndef backprop(node: MCTSNode, reward: float):\n    \"\"\"Backpropagate reward up the tree.\"\"\"\n    while node is not None:\n        node.visits += 1\n        node.reward_history.append(reward)\n        node = node.parent\n\n\ndef should_expand(node: MCTSNode, pw_alpha: float = 0.5) -> bool:\n    \"\"\"Progressive widening: ceil(visits^alpha) children allowed.\"\"\"\n    max_children = max(1, math.ceil(node.visits ** pw_alpha))\n    return len(node.children) < max_children\n\n\nasync def _eval_leaf_async(leaf: MCTSNode, batch: List[dict]) -> Tuple[List[bool], str, str, Playbook]:\n    \"\"\"Evaluate a leaf's playbook on a batch of problems asynchronously.\"\"\"\n    eval_pb = leaf.playbook.copy()\n\n    # Parallel generate all problems in batch\n    gen_tasks = [generate_async(prob[\"question\"], eval_pb) for prob in batch]\n    gen_results = await asyncio.gather(*gen_tasks)\n\n    # Parallel reflect all\n    ref_tasks = []\n    for prob, (answer, bullets_used, raw) in zip(batch, gen_results):\n        ref_tasks.append(reflect_async(\n            prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, eval_pb\n        ))\n    ref_results = await asyncio.gather(*ref_tasks)\n\n    # Apply tags\n    batch_correct = []\n    last_reflection = \"\"\n    last_question = \"\"\n    for prob, (answer, bullets_used, raw), (reflection, tags) in zip(batch, gen_results, ref_results):\n        correct = answers_match(answer, prob[\"answer\"])\n        batch_correct.append(correct)\n        for bid, label in tags.items():\n            eval_pb.tag(bid, label)\n        last_reflection = reflection\n        last_question = prob[\"question\"]\n\n    return batch_correct, last_reflection, last_question, eval_pb\n\n\ndef run_puct(problems: List[dict], batch_size: int = 3, c_puct: float = 1.5,\n             q_mode: str = \"mean\") -> RunLog:\n    \"\"\"PUCT tree search with virtual loss parallel leaf selection and async eval.\"\"\"\n    reset_call_counter()\n    log = RunLog()\n    root = MCTSNode(playbook=make_initial_playbook())\n    prob_idx = 0\n    # Number of leaves to evaluate in parallel\n    n_parallel = min(4, max(1, len(problems) // (batch_size * 4)))\n\n    async def _run():\n        nonlocal prob_idx\n\n        while prob_idx < len(problems):\n            # How many leaves can we evaluate given remaining problems?\n            remaining = len(problems) - prob_idx\n            n_leaves = min(n_parallel, remaining // batch_size) if remaining >= batch_size else 1\n\n            if n_leaves <= 1:\n                # Single leaf evaluation (no virtual loss needed)\n                leaf = puct_select(root, c_puct, q_mode)\n                batch_end = min(prob_idx + batch_size, len(problems))\n                batch = problems[prob_idx:batch_end]\n\n                batch_correct, last_reflection, last_question, eval_pb = \\\n                    await _eval_leaf_async(leaf, batch)\n\n                prob_idx = batch_end\n                reward = sum(batch_correct) / len(batch_correct)\n                leaf.results.extend(batch_correct)\n                log.correct.extend(batch_correct)\n                for _ in batch_correct:\n                    log.playbook_sizes.append(eval_pb.size)\n\n                if should_expand(leaf):\n                    new_pb = curate(eval_pb, last_reflection, last_question)\n                    child = MCTSNode(playbook=new_pb, parent=leaf)\n                    leaf.children.append(child)\n\n                backprop(leaf, reward)\n            else:\n                # Parallel multi-leaf evaluation with virtual loss\n                selections = select_batch_with_virtual_loss(root, n_leaves, c_puct, q_mode)\n\n                # Build batches for each leaf\n                eval_tasks = []\n                leaf_batches = []\n                for leaf, path in selections:\n                    batch_end = min(prob_idx + batch_size, len(problems))\n                    if prob_idx >= len(problems):\n                        break\n                    batch = problems[prob_idx:batch_end]\n                    prob_idx = batch_end\n                    leaf_batches.append((leaf, path, batch))\n                    eval_tasks.append(_eval_leaf_async(leaf, batch))\n\n                # Evaluate all leaves in parallel\n                eval_results = await asyncio.gather(*eval_tasks)\n\n                # Process results: remove virtual losses, backprop, expand\n                for (leaf, path, batch), (batch_correct, last_ref, last_q, eval_pb) in \\\n                        zip(leaf_batches, eval_results):\n                    # Remove virtual losses\n                    for node in path:\n                        node.remove_virtual_loss()\n\n                    reward = sum(batch_correct) / len(batch_correct)\n                    leaf.results.extend(batch_correct)\n                    log.correct.extend(batch_correct)\n                    for _ in batch_correct:\n                        log.playbook_sizes.append(eval_pb.size)\n\n                    if should_expand(leaf):\n                        new_pb = curate(eval_pb, last_ref, last_q)\n                        child = MCTSNode(playbook=new_pb, parent=leaf)\n                        leaf.children.append(child)\n\n                    backprop(leaf, reward)\n\n            done = len(log.correct)\n            if done % 10 <= batch_size * n_parallel or done == len(problems):\n                acc = sum(log.correct) / len(log.correct)\n                depth = _tree_depth(root)\n                size = _tree_size(root)\n                branches = _branch_count(root)\n                print(f\"  PUCT-{q_mode} [{done}/{len(problems)}] acc={acc:.2%} \"\n                      f\"depth={depth} nodes={size} branches={branches}\")\n\n    asyncio.run(_run())\n    log.call_counts = get_call_counts()\n    best_leaf = _best_leaf(root, q_mode)\n    log.final_playbook = best_leaf.playbook\n    return log\n\n\ndef _tree_depth(node: MCTSNode) -> int:\n    if not node.children:\n        return 0\n    return 1 + max(_tree_depth(c) for c in node.children)\n\n\ndef _tree_size(node: MCTSNode) -> int:\n    return 1 + sum(_tree_size(c) for c in node.children)\n\n\ndef _branch_count(node: MCTSNode) -> int:\n    count = 1 if len(node.children) > 1 else 0\n    return count + sum(_branch_count(c) for c in node.children)\n\n\ndef _best_leaf(node: MCTSNode, q_mode: str = \"mean\") -> MCTSNode:\n    if not node.children:\n        return node\n    best = max(node.children, key=lambda c: c.q_value(q_mode) if c.visits > 0 else -1)\n    return _best_leaf(best, q_mode)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Strategy 3: Thompson Sampling over Playbook Pool (async) ---\n\nTHOMPSON_SEED_PROBLEMS = 6\nTHOMPSON_N_VARIANTS = 5\n\ndef run_thompson(problems: List[dict]) -> RunLog:\n    \"\"\"Flat bandit with async seed phase and per-problem async generate+reflect.\n\n    Budget: 50 generate + 50 reflect + 5 curate = 105 calls.\n    \"\"\"\n    reset_call_counter()\n    log = RunLog()\n\n    base_pb = make_initial_playbook()\n    pool = [base_pb]\n\n    seed = problems[:THOMPSON_SEED_PROBLEMS]\n    remaining = problems[THOMPSON_SEED_PROBLEMS:]\n\n    async def _run():\n        nonlocal pool\n\n        # --- Phase 1: Seed (all generate+reflect in parallel) ---\n        gen_tasks = [generate_async(prob[\"question\"], base_pb) for prob in seed]\n        gen_results = await asyncio.gather(*gen_tasks)\n\n        ref_tasks = []\n        for prob, (answer, bullets_used, raw) in zip(seed, gen_results):\n            ref_tasks.append(reflect_async(\n                prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, base_pb\n            ))\n        ref_results = await asyncio.gather(*ref_tasks)\n\n        reflections = []\n        for prob, (answer, bullets_used, raw), (reflection, tags) in zip(seed, gen_results, ref_results):\n            correct = answers_match(answer, prob[\"answer\"])\n            log.correct.append(correct)\n            log.playbook_sizes.append(base_pb.size)\n            for bid, label in tags.items():\n                base_pb.tag(bid, label)\n            reflections.append((reflection, prob[\"question\"]))\n\n        # Curate variants (sequential — each variant is independent)\n        for ref_text, ref_q in reflections[:THOMPSON_N_VARIANTS]:\n            variant = curate(base_pb, ref_text, ref_q)\n            pool.append(variant)\n\n        print(f\"  Thompson: created {len(pool)} playbook variants from {len(seed)} seed problems\")\n\n        # --- Phase 2: Thompson Sampling (async generate+reflect per problem) ---\n        alphas = [1.0] * len(pool)\n        betas_param = [1.0] * len(pool)\n\n        for i, prob in enumerate(remaining):\n            ts_samples = [np.random.beta(a, b) for a, b in zip(alphas, betas_param)]\n            chosen = int(np.argmax(ts_samples))\n            pb = pool[chosen]\n\n            answer, bullets_used, raw = await generate_async(prob[\"question\"], pb)\n            correct = answers_match(answer, prob[\"answer\"])\n            log.correct.append(correct)\n            log.playbook_sizes.append(pb.size)\n\n            reflection, tags = await reflect_async(\n                prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, pb\n            )\n            for bid, label in tags.items():\n                pb.tag(bid, label)\n\n            if correct:\n                alphas[chosen] += 1.0\n            else:\n                betas_param[chosen] += 1.0\n\n            if (THOMPSON_SEED_PROBLEMS + i + 1) % 10 == 0:\n                acc = sum(log.correct) / len(log.correct)\n                pulls = [int(a + b - 2) for a, b in zip(alphas, betas_param)]\n                best_var = int(np.argmax([a / (a + b) for a, b in zip(alphas, betas_param)]))\n                print(f\"  Thompson [{THOMPSON_SEED_PROBLEMS + i + 1}/{len(problems)}] \"\n                      f\"acc={acc:.2%} pulls={pulls} best=variant-{best_var}\")\n\n        best_idx = int(np.argmax([a / (a + b) for a, b in zip(alphas, betas_param)]))\n        log.final_playbook = pool[best_idx]\n\n    asyncio.run(_run())\n    log.call_counts = get_call_counts()\n    return log"
  },
  {
   "cell_type": "code",
   "source": "# --- Strategy 3b: Dynamic Thompson Sampling (async, pool grows over time) ---\n\nTHOMPSON_DYN_SEED_PROBLEMS = 6\nTHOMPSON_DYN_N_VARIANTS = 5\nARM_ADD_INTERVAL = 10\n\ndef run_thompson_dynamic(problems: List[dict]) -> RunLog:\n    \"\"\"Thompson Sampling with dynamic arm addition (async).\n\n    Inspired by OPTS (arXiv:2503.01163) bandit-based strategy selection.\n    Note: OPTS uses TS for mutation operator selection; our dynamic arm addition\n    from the best arm is a novel extension for the playbook evolution setting.\n\n    Budget: ~109 calls (17 seed + 88 bandit + ~4 dynamic curate).\n    \"\"\"\n    reset_call_counter()\n    log = RunLog()\n\n    base_pb = make_initial_playbook()\n    pool = [base_pb]\n\n    seed = problems[:THOMPSON_DYN_SEED_PROBLEMS]\n    remaining = problems[THOMPSON_DYN_SEED_PROBLEMS:]\n\n    async def _run():\n        nonlocal pool\n\n        # --- Phase 1: Seed (parallel generate+reflect) ---\n        gen_tasks = [generate_async(prob[\"question\"], base_pb) for prob in seed]\n        gen_results = await asyncio.gather(*gen_tasks)\n\n        ref_tasks = []\n        for prob, (answer, bullets_used, raw) in zip(seed, gen_results):\n            ref_tasks.append(reflect_async(\n                prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, base_pb\n            ))\n        ref_results = await asyncio.gather(*ref_tasks)\n\n        reflections = []\n        for prob, (answer, bullets_used, raw), (reflection, tags) in zip(seed, gen_results, ref_results):\n            correct = answers_match(answer, prob[\"answer\"])\n            log.correct.append(correct)\n            log.playbook_sizes.append(base_pb.size)\n            for bid, label in tags.items():\n                base_pb.tag(bid, label)\n            reflections.append((reflection, prob[\"question\"]))\n\n        for ref_text, ref_q in reflections[:THOMPSON_DYN_N_VARIANTS]:\n            variant = curate(base_pb, ref_text, ref_q)\n            pool.append(variant)\n\n        print(f\"  Thompson-Dyn: created {len(pool)} initial variants from {len(seed)} seed problems\")\n\n        # --- Phase 2: Thompson Sampling with dynamic arm addition ---\n        alphas = [1.0] * len(pool)\n        betas_param = [1.0] * len(pool)\n        last_reflections = {}\n\n        for i, prob in enumerate(remaining):\n            if i > 0 and i % ARM_ADD_INTERVAL == 0:\n                posterior_means = [a / (a + b) for a, b in zip(alphas, betas_param)]\n                best_arm = int(np.argmax(posterior_means))\n                best_pb = pool[best_arm]\n\n                if best_arm in last_reflections:\n                    ref_text, ref_q = last_reflections[best_arm]\n                else:\n                    ref_text, ref_q = next(iter(last_reflections.values()), reflections[-1])\n\n                new_variant = curate(best_pb, ref_text, ref_q)\n                pool.append(new_variant)\n                alphas.append(1.0)\n                betas_param.append(1.0)\n                print(f\"  Thompson-Dyn: added arm {len(pool)-1} (curated from best arm {best_arm}, \"\n                      f\"posterior={posterior_means[best_arm]:.2f})\")\n\n            ts_samples = [np.random.beta(a, b) for a, b in zip(alphas, betas_param)]\n            chosen = int(np.argmax(ts_samples))\n            pb = pool[chosen]\n\n            answer, bullets_used, raw = await generate_async(prob[\"question\"], pb)\n            correct = answers_match(answer, prob[\"answer\"])\n            log.correct.append(correct)\n            log.playbook_sizes.append(pb.size)\n\n            reflection, tags = await reflect_async(\n                prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, pb\n            )\n            for bid, label in tags.items():\n                pb.tag(bid, label)\n\n            last_reflections[chosen] = (reflection, prob[\"question\"])\n\n            if correct:\n                alphas[chosen] += 1.0\n            else:\n                betas_param[chosen] += 1.0\n\n            if (THOMPSON_DYN_SEED_PROBLEMS + i + 1) % 10 == 0:\n                acc = sum(log.correct) / len(log.correct)\n                pulls = [int(a + b - 2) for a, b in zip(alphas, betas_param)]\n                posterior_means = [a / (a + b) for a, b in zip(alphas, betas_param)]\n                best_var = int(np.argmax(posterior_means))\n                print(f\"  Thompson-Dyn [{THOMPSON_DYN_SEED_PROBLEMS + i + 1}/{len(problems)}] \"\n                      f\"acc={acc:.2%} arms={len(pool)} pulls={pulls} best=variant-{best_var}\")\n\n        posterior_means = [a / (a + b) for a, b in zip(alphas, betas_param)]\n        best_idx = int(np.argmax(posterior_means))\n        log.final_playbook = pool[best_idx]\n\n    asyncio.run(_run())\n    log.call_counts = get_call_counts()\n    return log",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# --- Strategy 5: Adaptive Progressive Widening (AB-MCTS-inspired, async) ---\n# Inspired by Sakana AI (arXiv:2503.04412). Our implementation differs from the\n# original paper: we use regret-based Beta updates instead of separate GEN/CONT\n# node types with backed-up score distributions. Renamed from \"AB-MCTS\" to\n# \"Adaptive Progressive Widening\" to reflect this distinction.\n#\n# Key fix: removed counterfactual assumption in deepen+regress case. Previously\n# we incremented expand_alpha when deepening didn't improve Q, assuming expansion\n# would have helped — but we never observed that counterfactual. Now we only\n# update the posterior when we have direct evidence about the chosen action.\n\ndef run_ab_mcts(problems: List[dict], batch_size: int = 3, c_puct: float = 1.5) -> RunLog:\n    \"\"\"Adaptive Progressive Widening with Thompson-sampled expand/deepen (async).\n\n    Each node uses Beta(expand_alpha, expand_beta) to decide wider vs deeper.\n    Posterior updated only on direct evidence (no counterfactual assumptions):\n      - Went wider AND reward > prev_q -> expand_alpha += 1 (expanding helped)\n      - Went wider AND reward <= prev_q -> expand_beta += 1 (expanding didn't help)\n      - Went deeper AND reward > prev_q -> expand_beta += 1 (deepening helped)\n      - Went deeper AND reward <= prev_q -> no update (no evidence either way)\n\n    Uses Bayesian Q-estimator. Budget: 50 gen + 50 ref + variable curate <= ~117.\n    \"\"\"\n    reset_call_counter()\n    log = RunLog()\n    q_mode = \"bayesian\"\n    root = MCTSNode(playbook=make_initial_playbook())\n    prob_idx = 0\n    expand_count = 0\n    deepen_count = 0\n\n    async def _run():\n        nonlocal prob_idx, expand_count, deepen_count\n\n        while prob_idx < len(problems):\n            leaf = puct_select(root, c_puct, q_mode)\n            prev_q = leaf.q_value(q_mode)\n\n            batch_end = min(prob_idx + batch_size, len(problems))\n            batch = problems[prob_idx:batch_end]\n            prob_idx = batch_end\n\n            batch_correct, last_reflection, last_question, eval_pb = \\\n                await _eval_leaf_async(leaf, batch)\n\n            reward = sum(batch_correct) / len(batch_correct)\n            leaf.results.extend(batch_correct)\n            log.correct.extend(batch_correct)\n            for _ in batch_correct:\n                log.playbook_sizes.append(eval_pb.size)\n\n            # Thompson-sampled wider-vs-deeper decision\n            ts_sample = np.random.beta(leaf.expand_alpha, leaf.expand_beta)\n            go_wider = ts_sample > 0.5 and leaf.visits > 0\n\n            if go_wider:\n                expand_count += 1\n                new_pb = curate(eval_pb, last_reflection, last_question)\n                child = MCTSNode(playbook=new_pb, parent=leaf)\n                leaf.children.append(child)\n                backprop(leaf, reward)\n\n                # Direct evidence: did expanding help?\n                if reward > prev_q:\n                    leaf.expand_alpha += 1\n                else:\n                    leaf.expand_beta += 1\n            else:\n                deepen_count += 1\n                backprop(leaf, reward)\n\n                # Direct evidence only: did deepening help?\n                if reward > prev_q:\n                    leaf.expand_beta += 1\n                # No update if deepening didn't help — we have no evidence\n                # that expanding would have been better (counterfactual).\n\n            done = len(log.correct)\n            if done % 10 <= batch_size or done == len(problems):\n                acc = sum(log.correct) / len(log.correct)\n                depth = _tree_depth(root)\n                size = _tree_size(root)\n                print(f\"  AB-MCTS [{done}/{len(problems)}] acc={acc:.2%} \"\n                      f\"depth={depth} nodes={size} wider={expand_count} deeper={deepen_count}\")\n\n    asyncio.run(_run())\n    log.call_counts = get_call_counts()\n    best_leaf = _best_leaf(root, q_mode)\n    log.final_playbook = best_leaf.playbook\n    print(f\"  AB-MCTS final: wider={expand_count} deeper={deepen_count} \"\n          f\"ratio={expand_count/(expand_count+deepen_count):.2f}\")\n    return log",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# --- Strategy 3c: Discounted Thompson Sampling (async, fixed timing) ---\n# From the non-stationary bandits literature (arXiv:2305.10718):\n# Discount is applied AFTER the posterior update, not before.\n# Floor lowered from 1.0 to 0.1 to allow proper forgetting.\n\nTHOMPSON_DISC_GAMMA = 0.95\nTHOMPSON_DISC_SEED_PROBLEMS = 6\nTHOMPSON_DISC_N_VARIANTS = 5\n\ndef run_thompson_discounted(problems: List[dict]) -> RunLog:\n    \"\"\"Discounted Thompson Sampling with corrected discount timing (async).\n\n    Discount applied AFTER posterior update (per arXiv:2305.10718), not before.\n    Floor at 0.1 (not 1.0) to allow proper exponential forgetting.\n    Effective lookback window: 1/(1-0.95) = 20 problems.\n\n    Budget: 105 calls (17 seed + 88 bandit).\n    \"\"\"\n    reset_call_counter()\n    log = RunLog()\n\n    base_pb = make_initial_playbook()\n    pool = [base_pb]\n\n    seed = problems[:THOMPSON_DISC_SEED_PROBLEMS]\n    remaining = problems[THOMPSON_DISC_SEED_PROBLEMS:]\n\n    async def _run():\n        # --- Phase 1: Seed (parallel generate+reflect) ---\n        gen_tasks = [generate_async(prob[\"question\"], base_pb) for prob in seed]\n        gen_results = await asyncio.gather(*gen_tasks)\n\n        ref_tasks = []\n        for prob, (answer, bullets_used, raw) in zip(seed, gen_results):\n            ref_tasks.append(reflect_async(\n                prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, base_pb\n            ))\n        ref_results = await asyncio.gather(*ref_tasks)\n\n        reflections = []\n        for prob, (answer, bullets_used, raw), (reflection, tags) in zip(seed, gen_results, ref_results):\n            correct = answers_match(answer, prob[\"answer\"])\n            log.correct.append(correct)\n            log.playbook_sizes.append(base_pb.size)\n            for bid, label in tags.items():\n                base_pb.tag(bid, label)\n            reflections.append((reflection, prob[\"question\"]))\n\n        for ref_text, ref_q in reflections[:THOMPSON_DISC_N_VARIANTS]:\n            variant = curate(base_pb, ref_text, ref_q)\n            pool.append(variant)\n\n        print(f\"  Thompson-Disc: created {len(pool)} playbook variants from {len(seed)} seed problems\")\n        print(f\"  Thompson-Disc: gamma={THOMPSON_DISC_GAMMA}, effective horizon={1/(1-THOMPSON_DISC_GAMMA):.0f} problems\")\n\n        # --- Phase 2: Discounted Thompson Sampling ---\n        alphas = [1.0] * len(pool)\n        betas_param = [1.0] * len(pool)\n\n        for i, prob in enumerate(remaining):\n            # Thompson sample (BEFORE any discounting)\n            ts_samples = [np.random.beta(a, b) for a, b in zip(alphas, betas_param)]\n            chosen = int(np.argmax(ts_samples))\n            pb = pool[chosen]\n\n            answer, bullets_used, raw = await generate_async(prob[\"question\"], pb)\n            correct = answers_match(answer, prob[\"answer\"])\n            log.correct.append(correct)\n            log.playbook_sizes.append(pb.size)\n\n            reflection, tags = await reflect_async(\n                prob[\"question\"], raw, answer, prob[\"answer\"], bullets_used, pb\n            )\n            for bid, label in tags.items():\n                pb.tag(bid, label)\n\n            # Update Beta posterior for the chosen variant\n            if correct:\n                alphas[chosen] += 1.0\n            else:\n                betas_param[chosen] += 1.0\n\n            # Discount ALL arms AFTER update (per arXiv:2305.10718)\n            for k in range(len(pool)):\n                alphas[k] *= THOMPSON_DISC_GAMMA\n                betas_param[k] *= THOMPSON_DISC_GAMMA\n                # Floor at 0.1 (not 1.0) to allow proper forgetting\n                alphas[k] = max(alphas[k], 0.1)\n                betas_param[k] = max(betas_param[k], 0.1)\n\n            if (THOMPSON_DISC_SEED_PROBLEMS + i + 1) % 10 == 0:\n                acc = sum(log.correct) / len(log.correct)\n                pulls = [int(a + b - 2) for a, b in zip(alphas, betas_param)]\n                best_var = int(np.argmax([a / (a + b) for a, b in zip(alphas, betas_param)]))\n                print(f\"  Thompson-Disc [{THOMPSON_DISC_SEED_PROBLEMS + i + 1}/{len(problems)}] \"\n                      f\"acc={acc:.2%} pulls={pulls} best=variant-{best_var}\")\n\n        best_idx = int(np.argmax([a / (a + b) for a, b in zip(alphas, betas_param)]))\n        log.final_playbook = pool[best_idx]\n\n    asyncio.run(_run())\n    log.call_counts = get_call_counts()\n    return log",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Strategy 4: Majority Vote (fully parallel, no evolution) ---\n\nMAJORITY_N_SAMPLES = 2\n\ndef run_majority_vote(problems: List[dict]) -> RunLog:\n    \"\"\"No playbook evolution. Static initial playbook. All generate calls in parallel.\n\n    Budget: 50 * 2 = 100 generate calls. No reflect or curate.\n    \"\"\"\n    reset_call_counter()\n    log = RunLog()\n    pb = make_initial_playbook()\n\n    async def _run():\n        # Fire ALL 100 generate calls at once — no dependencies between them\n        tasks = []\n        for prob in problems:\n            for _ in range(MAJORITY_N_SAMPLES):\n                tasks.append(generate_async(prob[\"question\"], pb))\n\n        all_results = await asyncio.gather(*tasks)\n\n        # Group results by problem (every MAJORITY_N_SAMPLES consecutive results)\n        for i, prob in enumerate(problems):\n            answers = []\n            for j in range(MAJORITY_N_SAMPLES):\n                answer, _, _ = all_results[i * MAJORITY_N_SAMPLES + j]\n                answers.append(answer)\n\n            vote_counts = Counter(answers)\n            majority_answer = vote_counts.most_common(1)[0][0]\n            correct = answers_match(majority_answer, prob[\"answer\"])\n            log.correct.append(correct)\n            log.playbook_sizes.append(pb.size)\n\n        acc = sum(log.correct) / len(log.correct)\n        print(f\"  MajVote [done] acc={acc:.2%}\")\n\n    asyncio.run(_run())\n    log.call_counts = get_call_counts()\n    log.final_playbook = pb\n    return log"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"Running Majority Vote (no evolution baseline)...\")\nprint(\"=\" * 60)\nmajority_log = load_checkpoint(\"majority_vote\")\nif majority_log is None:\n    majority_log = run_majority_vote(problems)\n    save_checkpoint(\"majority_vote\", majority_log)\nprint(f\"MajVote done. Final acc (last 20): {majority_log.final_accuracy:.2%}\")\nprint(f\"Calls: {majority_log.call_counts}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"Running Greedy ACE (baseline)...\")\nprint(\"=\" * 60)\ngreedy_log = load_checkpoint(\"greedy_ace\")\nif greedy_log is None:\n    greedy_log = run_greedy(problems)\n    save_checkpoint(\"greedy_ace\", greedy_log)\nprint(f\"Greedy done. Final acc (last 20): {greedy_log.final_accuracy:.2%}\")\nprint(f\"Calls: {greedy_log.call_counts}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"Running Thompson Sampling...\")\nprint(\"=\" * 60)\nthompson_log = load_checkpoint(\"thompson\")\nif thompson_log is None:\n    thompson_log = run_thompson(problems)\n    save_checkpoint(\"thompson\", thompson_log)\nprint(f\"Thompson done. Final acc (last 20): {thompson_log.final_accuracy:.2%}\")\nprint(f\"Calls: {thompson_log.call_counts}\")"
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 60)\nprint(\"Running Dynamic Thompson Sampling...\")\nprint(\"=\" * 60)\nthompson_dyn_log = load_checkpoint(\"thompson_dyn\")\nif thompson_dyn_log is None:\n    thompson_dyn_log = run_thompson_dynamic(problems)\n    save_checkpoint(\"thompson_dyn\", thompson_dyn_log)\nprint(f\"Thompson-Dyn done. Final acc (last 20): {thompson_dyn_log.final_accuracy:.2%}\")\nprint(f\"Calls: {thompson_dyn_log.call_counts}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "puct_logs = {}\nfor q_mode in [\"mean\", \"ema\", \"bayesian\", \"variance\"]:\n    ckpt_name = f\"puct_{q_mode}\"\n    print(\"=\" * 60)\n    print(f\"Running PUCT-{q_mode.upper()}...\")\n    print(\"=\" * 60)\n    log = load_checkpoint(ckpt_name)\n    if log is None:\n        log = run_puct(problems, q_mode=q_mode)\n        save_checkpoint(ckpt_name, log)\n    puct_logs[q_mode] = log\n    print(f\"PUCT-{q_mode} done. Final acc (last 20): {log.final_accuracy:.2%}\")\n    print(f\"Calls: {log.call_counts}\")\n    print()"
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 60)\nprint(\"Running AB-MCTS (Adaptive Progressive Widening)...\")\nprint(\"=\" * 60)\nab_mcts_log = load_checkpoint(\"ab_mcts\")\nif ab_mcts_log is None:\n    ab_mcts_log = run_ab_mcts(problems)\n    save_checkpoint(\"ab_mcts\", ab_mcts_log)\nprint(f\"AB-MCTS done. Final acc (last 20): {ab_mcts_log.final_accuracy:.2%}\")\nprint(f\"Calls: {ab_mcts_log.call_counts}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 60)\nprint(\"Running Discounted Thompson Sampling (gamma=0.95)...\")\nprint(\"=\" * 60)\nthompson_disc_log = load_checkpoint(\"thompson_disc\")\nif thompson_disc_log is None:\n    thompson_disc_log = run_thompson_discounted(problems)\n    save_checkpoint(\"thompson_disc\", thompson_disc_log)\nprint(f\"Thompson-Disc done. Final acc (last 20): {thompson_disc_log.final_accuracy:.2%}\")\nprint(f\"Calls: {thompson_disc_log.call_counts}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analysis & Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build results dict with all 10 conditions\nresults = {\n    \"Majority Vote\": majority_log,\n    \"Greedy ACE\": greedy_log,\n    \"Thompson\": thompson_log,\n    \"Thompson-Dyn\": thompson_dyn_log,\n    \"Thompson-Disc\": thompson_disc_log,\n}\nfor q_mode, log in puct_logs.items():\n    results[f\"PUCT-{q_mode.upper()}\"] = log\nresults[\"AB-MCTS\"] = ab_mcts_log\n\nCOLORS = {\n    \"Majority Vote\": \"#9467bd\",\n    \"Greedy ACE\": \"#1f77b4\",\n    \"Thompson\": \"#8c564b\",\n    \"Thompson-Dyn\": \"#e377c2\",\n    \"Thompson-Disc\": \"#bcbd22\",\n    \"PUCT-MEAN\": \"#ff7f0e\",\n    \"PUCT-EMA\": \"#2ca02c\",\n    \"PUCT-BAYESIAN\": \"#d62728\",\n    \"PUCT-VARIANCE\": \"#7f7f7f\",\n    \"AB-MCTS\": \"#17becf\",\n}\n\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\nfig.suptitle(\"ACE Search Strategy Comparison (GSM8K, 50 problems)\", fontsize=14, fontweight=\"bold\")\n\n# --- (a) Running accuracy curves ---\nax = axes[0, 0]\nfor name, log in results.items():\n    acc = log.running_accuracy\n    ax.plot(range(1, len(acc) + 1), acc, label=name, color=COLORS[name], linewidth=2)\nax.set_xlabel(\"Problems Solved\")\nax.set_ylabel(\"Running Accuracy\")\nax.set_title(\"(a) Running Accuracy Over Time\")\nax.legend(fontsize=6, loc=\"lower right\")\nax.grid(True, alpha=0.3)\nax.set_ylim(0, 1)\n\n# --- (b) Final accuracy bar chart with bootstrap CI ---\nax = axes[0, 1]\n\ndef bootstrap_ci(data, n_boot=1000, ci=0.95):\n    data = np.array(data, dtype=float)\n    means = [np.mean(np.random.choice(data, size=len(data), replace=True)) for _ in range(n_boot)]\n    lo = np.percentile(means, (1 - ci) / 2 * 100)\n    hi = np.percentile(means, (1 + ci) / 2 * 100)\n    return np.mean(data), lo, hi\n\nnames = list(results.keys())\nmeans, lows, highs = [], [], []\nfor name in names:\n    tail = results[name].correct[-20:]\n    m, lo, hi = bootstrap_ci(tail)\n    means.append(m)\n    lows.append(m - lo)\n    highs.append(hi - m)\n\nbars = ax.bar(names, means, color=[COLORS[n] for n in names], yerr=[lows, highs], capsize=4)\nax.set_ylabel(\"Accuracy (last 20 problems)\")\nax.set_title(\"(b) Final Accuracy Comparison\")\nax.set_ylim(0, 1)\nax.tick_params(axis='x', rotation=35, labelsize=5)\nfor bar, m in zip(bars, means):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f\"{m:.0%}\",\n            ha=\"center\", fontsize=6)\n\n# --- (c) Playbook size over time ---\nax = axes[1, 0]\nfor name, log in results.items():\n    sizes = log.playbook_sizes\n    ax.plot(range(1, len(sizes) + 1), sizes, label=name, color=COLORS[name], linewidth=2)\nax.set_xlabel(\"Problems Solved\")\nax.set_ylabel(\"Playbook Bullets\")\nax.set_title(\"(c) Playbook Size Over Time\")\nax.legend(fontsize=6)\nax.grid(True, alpha=0.3)\n\n# --- (d) Cost breakdown ---\nax = axes[1, 1]\nroles = [\"generate\", \"reflect\", \"curate\"]\nx = np.arange(len(names))\nwidth = 0.22\nfor i, role in enumerate(roles):\n    counts = [results[n].call_counts.get(role, 0) for n in names]\n    ax.bar(x + i * width, counts, width, label=role.capitalize())\nax.set_xticks(x + width)\nax.set_xticklabels(names, fontsize=4, rotation=35)\nax.set_ylabel(\"LLM Calls\")\nax.set_title(\"(d) LLM Call Breakdown\")\nax.legend()\n\nplt.tight_layout()\nplt.savefig(\"search_ace_results.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()\nprint(\"Plots saved to search_ace_results.png\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Statistical comparison ---\n\nprint(\"=\" * 60)\nprint(\"Statistical Comparison (Bootstrap 95% CI on accuracy difference vs Greedy)\")\nprint(\"=\" * 60)\n\ndef bootstrap_diff_ci(a, b, n_boot=2000):\n    a, b = np.array(a, dtype=float), np.array(b, dtype=float)\n    diffs = []\n    for _ in range(n_boot):\n        ia = np.random.choice(len(a), len(a), replace=True)\n        ib = np.random.choice(len(b), len(b), replace=True)\n        diffs.append(np.mean(b[ib]) - np.mean(a[ia]))\n    lo, hi = np.percentile(diffs, [2.5, 97.5])\n    return np.mean(diffs), lo, hi\n\ngreedy_tail = greedy_log.correct[-20:]\n\n# Each condition vs Greedy\ncomparisons = [\n    (\"Majority Vote\", majority_log.correct[-20:]),\n    (\"Thompson\", thompson_log.correct[-20:]),\n    (\"Thompson-Dyn\", thompson_dyn_log.correct[-20:]),\n    (\"Thompson-Disc\", thompson_disc_log.correct[-20:]),\n]\nfor q_mode in [\"mean\", \"ema\", \"bayesian\", \"variance\"]:\n    comparisons.append((f\"PUCT-{q_mode.upper()}\", puct_logs[q_mode].correct[-20:]))\ncomparisons.append((\"AB-MCTS\", ab_mcts_log.correct[-20:]))\n\nfor name, tail in comparisons:\n    mean_diff, lo, hi = bootstrap_diff_ci(greedy_tail, tail)\n    sig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\n    print(f\"{name} vs Greedy: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")\n\n# Thompson-Dyn vs static Thompson\nprint()\nt_tail = thompson_log.correct[-20:]\ntd_tail = thompson_dyn_log.correct[-20:]\nmean_diff, lo, hi = bootstrap_diff_ci(t_tail, td_tail)\nsig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\nprint(f\"Thompson-Dyn vs Thompson: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")\n\n# Thompson-Disc vs static Thompson\ntdisc_tail = thompson_disc_log.correct[-20:]\nmean_diff, lo, hi = bootstrap_diff_ci(t_tail, tdisc_tail)\nsig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\nprint(f\"Thompson-Disc vs Thompson: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")\n\n# Thompson vs each PUCT\nprint()\nfor q_mode in [\"mean\", \"ema\", \"bayesian\", \"variance\"]:\n    p_tail = puct_logs[q_mode].correct[-20:]\n    mean_diff, lo, hi = bootstrap_diff_ci(t_tail, p_tail)\n    sig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\n    print(f\"PUCT-{q_mode.upper()} vs Thompson: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")\n\n# AB-MCTS vs PUCT-Bayesian\nprint()\nab_tail = ab_mcts_log.correct[-20:]\npb_tail = puct_logs[\"bayesian\"].correct[-20:]\nmean_diff, lo, hi = bootstrap_diff_ci(pb_tail, ab_tail)\nsig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\nprint(f\"AB-MCTS vs PUCT-Bayesian: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")\n\n# PUCT-VARIANCE vs PUCT-Bayesian\npv_tail = puct_logs[\"variance\"].correct[-20:]\nmean_diff, lo, hi = bootstrap_diff_ci(pb_tail, pv_tail)\nsig = \"YES\" if lo > 0 else (\"YES (worse)\" if hi < 0 else \"NO\")\nprint(f\"PUCT-VARIANCE vs PUCT-Bayesian: diff={mean_diff:+.1%} 95%CI=[{lo:+.1%}, {hi:+.1%}] significant={sig}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Results Summary\n\n### Experiment Design\n- **Dataset**: GSM8K (50 problems, shuffled, seed=42)\n- **Model**: Qwen2.5-7B-Instruct (local via vLLM)\n- **9 conditions** spanning no-evolution -> flat bandit -> discounted bandit -> dynamic bandit -> sequential -> tree search -> adaptive tree search\n\n### Budget Comparison\n| Condition | Generate | Reflect | Curate | Total |\n|-----------|----------|---------|--------|-------|\n| Majority Vote | 100 | 0 | 0 | 100 |\n| Greedy ACE | 50 | 50 | 10 | 110 |\n| Thompson | 50 | 50 | 5 | 105 |\n| Thompson-Disc | 50 | 50 | 5 | 105 |\n| Thompson-Dyn | 50 | 50 | ~9 | ~109 |\n| PUCT-* | 50 | 50 | <=17 | <=117 |\n| AB-MCTS | 50 | 50 | variable | <=117 |\n\n### Strategy Taxonomy\n| Strategy | Search Type | Evolves Playbook? | Exploration Mechanism |\n|----------|------------|-------------------|----------------------|\n| Majority Vote | None | No | Sample diversity only |\n| Greedy ACE | Sequential | Yes | None (greedy) |\n| Thompson | Flat bandit | Pool of variants (fixed) | Beta posterior sampling |\n| Thompson-Disc | Flat bandit | Pool of variants (fixed) | Discounted Beta posterior (gamma=0.95, horizon=20) |\n| Thompson-Dyn | Flat bandit | Pool grows dynamically | Beta posterior sampling + periodic arm addition |\n| PUCT-Mean | Tree | Yes (progressive) | UCB with mean Q |\n| PUCT-EMA | Tree | Yes (progressive) | UCB with recency-weighted Q |\n| PUCT-Bayesian | Tree | Yes (progressive) | UCB with shrinkage-to-0.5 Q |\n| AB-MCTS | Tree (adaptive) | Yes (adaptive) | UCB + Thompson-sampled wider-vs-deeper |\n\n### Key Questions\n1. **Does evolution help at all?** Majority Vote vs Greedy -- if MajVote wins, sampling > evolution\n2. **Flat vs tree?** Thompson vs PUCT -- is tree structure worth the overhead at 50 problems?\n3. **Does dynamic pool help?** Thompson-Dyn vs Thompson -- does periodic arm addition from the best arm improve over a frozen pool?\n4. **Does discounting help?** Thompson-Disc vs Thompson -- does forgetting old observations improve adaptation as playbooks mature?\n5. **Which Q estimator?** Mean vs EMA vs Bayesian -- does exploration strategy matter for PUCT?\n6. **Fixed vs adaptive branching?** AB-MCTS vs PUCT-Bayesian -- does Thompson-sampled wider/deeper beat fixed progressive widening?\n7. **Free lunch?** Does any strategy consistently beat Greedy with matched budget?\n\n### Thompson-Disc Design (non-stationary bandits)\nThompson-Disc applies exponential discounting to Beta posteriors before each update:\n- Before each round, multiply all arms' alpha and beta by gamma=0.95\n- Floor at 1.0 to maintain a minimal uniform prior\n- Effective lookback window: 1/(1-gamma) = 20 problems\n- Motivation: playbook bullets accumulate helpful/harmful tags over time, so a variant that performed poorly early may improve later -- discounting lets the bandit adapt to this non-stationarity\n\n### AB-MCTS Design (arXiv:2503.04412)\nAB-MCTS replaces PUCT's fixed progressive widening with an adaptive decision per node:\n- Each node maintains Beta(expand_alpha, expand_beta) posterior\n- Thompson-sample to decide: go WIDER (curate new child) or go DEEPER (re-evaluate)\n- Posterior updated based on whether the chosen action improved the node's Q-value\n- Hypothesis: adaptive branching should allocate curate budget more efficiently than visits^alpha"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cleanup: kill vLLM server ---\n",
    "try:\n",
    "    os.kill(vllm_proc.pid, signal.SIGTERM)\n",
    "    print(f\"Killed vLLM server (PID {vllm_proc.pid})\")\n",
    "except ProcessLookupError:\n",
    "    print(\"vLLM server already stopped\")"
   ]
  }
 ]
}