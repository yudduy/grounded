{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verified Archetype Discovery PoC\n",
    "\n",
    "**Hypothesis**: Identifying \"archetype\" problems first improves learning compared to random curriculum. The key innovation is rigorous verification of strategies before trusting them.\n",
    "\n",
    "**Game of 24**: Given 4 numbers, use +, -, *, / to make 24 (each number used exactly once).\n",
    "\n",
    "**Approach**:\n",
    "1. Embed problems and cluster to find representative archetypes\n",
    "2. Rigorously verify strategies extracted from archetypes\n",
    "3. Bootstrap playbook from verified strategies\n",
    "4. Use LinUCB contextual bandit for curriculum selection\n",
    "5. Compare archetype-first vs random curriculum\n",
    "\n",
    "**Setup**: Qwen2.5-7B-Instruct via vLLM on A100 (bfloat16, prefix caching, async parallel eval)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install vllm==0.6.6 openai==1.58.1 datasets==3.2.0 matplotlib==3.9.3 numpy==1.26.4 nest_asyncio==1.6.0 sentence-transformers==2.2.2 scikit-learn==1.4.0 scipy==1.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import signal\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import asyncio\n",
    "import itertools\n",
    "import nest_asyncio\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Tuple, Optional, Set, Any\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from scipy import stats\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "\n",
    "# Allow nested event loops (required for Colab/Jupyter)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Checkpoint directory\n",
    "CHECKPOINT_DIR = Path(\"checkpoints_archetype\")\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Concurrency for async LLM calls\n",
    "MAX_CONCURRENT_LLM = 64\n",
    "\n",
    "print(f\"Setup complete. Seed={SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch vLLM server in background\n",
    "VLLM_PORT = 8000\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "print(\"Launching vLLM server...\")\n",
    "vllm_log = open('/tmp/vllm_server.log', 'w')\n",
    "\n",
    "vllm_proc = subprocess.Popen(\n",
    "    [\n",
    "        \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
    "        \"--model\", MODEL_NAME,\n",
    "        \"--port\", str(VLLM_PORT),\n",
    "        \"--max-model-len\", \"8192\",\n",
    "        \"--gpu-memory-utilization\", \"0.95\",\n",
    "        \"--dtype\", \"bfloat16\",\n",
    "        \"--max-num-seqs\", \"1024\",\n",
    "        \"--max-num-batched-tokens\", \"16384\",\n",
    "        \"--enable-prefix-caching\",\n",
    "        \"--disable-log-requests\",\n",
    "    ],\n",
    "    stdout=vllm_log,\n",
    "    stderr=subprocess.STDOUT,\n",
    ")\n",
    "\n",
    "# Wait for server to be ready\n",
    "client = OpenAI(base_url=f\"http://localhost:{VLLM_PORT}/v1\", api_key=\"dummy\")\n",
    "aclient = AsyncOpenAI(base_url=f\"http://localhost:{VLLM_PORT}/v1\", api_key=\"dummy\")\n",
    "print(\"Waiting for vLLM server to be ready...\")\n",
    "\n",
    "for attempt in range(180):  # 3 min timeout\n",
    "    try:\n",
    "        client.models.list()\n",
    "        print(f\"vLLM ready after {attempt + 1}s\")\n",
    "        break\n",
    "    except Exception:\n",
    "        time.sleep(1)\n",
    "else:\n",
    "    raise RuntimeError(\"vLLM server failed to start within 180s\")\n",
    "\n",
    "# Warmup: trigger CUDA graph compilation\n",
    "print(\"Warming up vLLM (CUDA graph compilation)...\")\n",
    "try:\n",
    "    warmup_resp = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}],\n",
    "        max_tokens=16,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    print(f\"Warmup complete: {warmup_resp.choices[0].message.content.strip()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warmup failed (non-fatal): {e}\")\n",
    "\n",
    "print(f\"vLLM server running on port {VLLM_PORT} (PID: {vllm_proc.pid})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Game of 24 Data Loading & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Game of 24 Problem Generation & Validation ---\n",
    "\n",
    "def generate_all_game24_problems(max_num: int = 13) -> List[Tuple[int, int, int, int]]:\n",
    "    \"\"\"Generate all unique Game of 24 problems with numbers 1-max_num.\"\"\"\n",
    "    problems = set()\n",
    "    for a in range(1, max_num + 1):\n",
    "        for b in range(a, max_num + 1):\n",
    "            for c in range(b, max_num + 1):\n",
    "                for d in range(c, max_num + 1):\n",
    "                    problems.add((a, b, c, d))\n",
    "    return list(problems)\n",
    "\n",
    "\n",
    "def solve_24_exhaustive(numbers: Tuple[int, ...], target: float = 24.0) -> List[str]:\n",
    "    \"\"\"\n",
    "    Find all distinct solutions to Game of 24 using exhaustive search.\n",
    "    Returns list of expression strings that evaluate to target.\n",
    "    \"\"\"\n",
    "    if len(numbers) == 1:\n",
    "        if abs(numbers[0] - target) < 1e-9:\n",
    "            return [str(int(numbers[0])) if numbers[0] == int(numbers[0]) else str(numbers[0])]\n",
    "        return []\n",
    "    \n",
    "    solutions = set()\n",
    "    ops = [('+', lambda a, b: a + b),\n",
    "           ('-', lambda a, b: a - b),\n",
    "           ('*', lambda a, b: a * b),\n",
    "           ('/', lambda a, b: a / b if b != 0 else float('inf'))]\n",
    "    \n",
    "    # Try all pairs of numbers\n",
    "    for i in range(len(numbers)):\n",
    "        for j in range(len(numbers)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            a, b = numbers[i], numbers[j]\n",
    "            remaining = tuple(numbers[k] for k in range(len(numbers)) if k != i and k != j)\n",
    "            \n",
    "            for op_str, op_func in ops:\n",
    "                try:\n",
    "                    result = op_func(a, b)\n",
    "                    if result == float('inf') or result == float('-inf'):\n",
    "                        continue\n",
    "                    \n",
    "                    # Format expression part\n",
    "                    a_str = str(int(a)) if a == int(a) else str(a)\n",
    "                    b_str = str(int(b)) if b == int(b) else str(b)\n",
    "                    expr_part = f\"({a_str} {op_str} {b_str})\"\n",
    "                    \n",
    "                    # Recurse with new number list\n",
    "                    new_numbers = remaining + (result,)\n",
    "                    sub_solutions = solve_24_exhaustive(new_numbers, target)\n",
    "                    \n",
    "                    for sub_sol in sub_solutions:\n",
    "                        # Replace the result placeholder with the expression\n",
    "                        result_str = str(int(result)) if result == int(result) else str(result)\n",
    "                        if result_str in sub_sol:\n",
    "                            full_expr = sub_sol.replace(result_str, expr_part, 1)\n",
    "                            solutions.add(full_expr)\n",
    "                        else:\n",
    "                            solutions.add(f\"{expr_part} -> {sub_sol}\")\n",
    "                except (ZeroDivisionError, OverflowError):\n",
    "                    continue\n",
    "    \n",
    "    return list(solutions)\n",
    "\n",
    "\n",
    "def is_solvable_24(numbers: Tuple[int, ...]) -> bool:\n",
    "    \"\"\"Check if a Game of 24 problem is solvable.\"\"\"\n",
    "    return len(solve_24_exhaustive(numbers)) > 0\n",
    "\n",
    "\n",
    "def count_solutions_24(numbers: Tuple[int, ...]) -> int:\n",
    "    \"\"\"Count the number of distinct solutions.\"\"\"\n",
    "    return len(solve_24_exhaustive(numbers))\n",
    "\n",
    "\n",
    "# Generate solvable problems\n",
    "print(\"Generating Game of 24 problems...\")\n",
    "all_problems = generate_all_game24_problems(max_num=13)\n",
    "print(f\"Total candidate problems: {len(all_problems)}\")\n",
    "\n",
    "# Filter to solvable problems (this takes a few seconds)\n",
    "print(\"Filtering to solvable problems...\")\n",
    "solvable_problems = []\n",
    "for i, nums in enumerate(all_problems):\n",
    "    if is_solvable_24(nums):\n",
    "        solvable_problems.append({\n",
    "            \"numbers\": nums,\n",
    "            \"text\": f\"Use {nums[0]}, {nums[1]}, {nums[2]}, {nums[3]} to make 24\",\n",
    "            \"n_solutions\": count_solutions_24(nums),\n",
    "        })\n",
    "    if (i + 1) % 200 == 0:\n",
    "        print(f\"  Processed {i + 1}/{len(all_problems)}...\")\n",
    "\n",
    "print(f\"Solvable problems: {len(solvable_problems)}\")\n",
    "\n",
    "# Shuffle and take subset for experiment\n",
    "rng = random.Random(SEED)\n",
    "rng.shuffle(solvable_problems)\n",
    "problems = solvable_problems[:200]  # Use 200 for clustering, 100 for eval\n",
    "\n",
    "print(f\"\\nUsing {len(problems)} problems for experiment\")\n",
    "print(f\"Example: {problems[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Known Hard Game of 24 Problems (for adversarial testing) ---\n",
    "# These require non-obvious operations like fractions or specific orderings\n",
    "\n",
    "HARD_PROBLEMS = [\n",
    "    # Requires fractions\n",
    "    {\"numbers\": (1, 5, 5, 5), \"text\": \"Use 1, 5, 5, 5 to make 24\", \"solution\": \"5 * (5 - 1/5) = 24\", \"difficulty\": \"fraction\"},\n",
    "    {\"numbers\": (3, 3, 8, 8), \"text\": \"Use 3, 3, 8, 8 to make 24\", \"solution\": \"8 / (3 - 8/3) = 24\", \"difficulty\": \"fraction\"},\n",
    "    {\"numbers\": (1, 3, 4, 6), \"text\": \"Use 1, 3, 4, 6 to make 24\", \"solution\": \"6 / (1 - 3/4) = 24\", \"difficulty\": \"fraction\"},\n",
    "    {\"numbers\": (1, 4, 5, 6), \"text\": \"Use 1, 4, 5, 6 to make 24\", \"solution\": \"4 / (1 - 5/6) = 24\", \"difficulty\": \"fraction\"},\n",
    "    {\"numbers\": (1, 6, 6, 8), \"text\": \"Use 1, 6, 6, 8 to make 24\", \"solution\": \"8 / (1 - 6/6) - nope, (6 - 1) * 6 - 8 = 22 - no, 8 * 6 / (6 - 1) = 48/5\", \"difficulty\": \"tricky\"},\n",
    "    # Requires specific ordering\n",
    "    {\"numbers\": (2, 3, 5, 12), \"text\": \"Use 2, 3, 5, 12 to make 24\", \"solution\": \"(5 - 3 + 2) * 12 / 2 = 24 - check\", \"difficulty\": \"ordering\"},\n",
    "    {\"numbers\": (1, 2, 7, 7), \"text\": \"Use 1, 2, 7, 7 to make 24\", \"solution\": \"(7 + 1) * (7 - 2) / ... = (7 - 1) * (7 - 2) = 30 - no, 7 * 7 / 2 - 1 = 23.5\", \"difficulty\": \"tricky\"},\n",
    "    {\"numbers\": (4, 4, 7, 7), \"text\": \"Use 4, 4, 7, 7 to make 24\", \"solution\": \"(7 - 4) * (7 + 4/4) = 3 * 8 = 24? - no 4/4=1, 7+1=8, 7-4=3, 3*8=24!\", \"difficulty\": \"tricky\"},\n",
    "    {\"numbers\": (3, 3, 7, 7), \"text\": \"Use 3, 3, 7, 7 to make 24\", \"solution\": \"(7 + 7) * (3 - 3) = 0 - no, (3 + 3/7) * 7 = 24\", \"difficulty\": \"fraction\"},\n",
    "    {\"numbers\": (2, 5, 5, 10), \"text\": \"Use 2, 5, 5, 10 to make 24\", \"solution\": \"(5 - 5/10) * 2 = 9 - no, 10 * 5 / 2 - 5 = 20\", \"difficulty\": \"tricky\"},\n",
    "]\n",
    "\n",
    "# Verify and correct hard problems\n",
    "verified_hard = []\n",
    "for hp in HARD_PROBLEMS:\n",
    "    nums = hp[\"numbers\"]\n",
    "    solutions = solve_24_exhaustive(nums)\n",
    "    if solutions:\n",
    "        hp[\"verified_solutions\"] = solutions[:3]  # Keep up to 3\n",
    "        hp[\"n_solutions\"] = len(solutions)\n",
    "        verified_hard.append(hp)\n",
    "        print(f\"{nums}: {len(solutions)} solutions - {solutions[0][:50]}...\")\n",
    "    else:\n",
    "        print(f\"{nums}: NO SOLUTION (removing from hard set)\")\n",
    "\n",
    "HARD_PROBLEMS = verified_hard\n",
    "print(f\"\\nVerified {len(HARD_PROBLEMS)} hard problems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Problem Embedding & Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentence transformer for embeddings\n",
    "print(\"Loading sentence transformer model...\")\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "def compute_problem_features(problem: Dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute feature vector for a Game of 24 problem.\n",
    "    Combines semantic embedding with numerical features.\n",
    "    \"\"\"\n",
    "    nums = problem[\"numbers\"]\n",
    "    \n",
    "    # Numerical features (normalized)\n",
    "    num_features = [\n",
    "        sum(nums) / 52,  # sum normalized by max possible (13*4)\n",
    "        np.prod(nums) / (13**4),  # product normalized\n",
    "        max(nums) / 13,  # max normalized\n",
    "        min(nums) / 13,  # min normalized\n",
    "        len(set(nums)) / 4,  # uniqueness ratio\n",
    "        (max(nums) - min(nums)) / 12,  # range normalized\n",
    "        np.std(nums) / 5,  # std normalized\n",
    "        sum(1 for n in nums if n % 2 == 0) / 4,  # even ratio\n",
    "        sum(1 for n in nums if 24 % n == 0) / 4,  # divisor of 24 ratio\n",
    "        1.0 if any(a * b == 24 for a, b in itertools.combinations(nums, 2)) else 0.0,  # has factor pair\n",
    "    ]\n",
    "    \n",
    "    # Semantic embedding of problem text\n",
    "    text_embedding = embed_model.encode(problem[\"text\"], convert_to_numpy=True)\n",
    "    \n",
    "    # Concatenate (weight numerical features higher for clustering)\n",
    "    num_features_scaled = np.array(num_features) * 2.0  # Scale up numerical importance\n",
    "    combined = np.concatenate([text_embedding, num_features_scaled])\n",
    "    \n",
    "    return combined\n",
    "\n",
    "\n",
    "# Compute embeddings for all problems\n",
    "print(\"Computing problem embeddings...\")\n",
    "embeddings = []\n",
    "for i, prob in enumerate(problems):\n",
    "    emb = compute_problem_features(prob)\n",
    "    embeddings.append(emb)\n",
    "    prob[\"embedding\"] = emb\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means clustering\n",
    "N_CLUSTERS = 10\n",
    "\n",
    "print(f\"Clustering into {N_CLUSTERS} clusters...\")\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=SEED, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Assign cluster info to problems\n",
    "for i, prob in enumerate(problems):\n",
    "    prob[\"cluster\"] = cluster_labels[i]\n",
    "    # Distance to cluster center (for centrality)\n",
    "    center = kmeans.cluster_centers_[cluster_labels[i]]\n",
    "    prob[\"dist_to_center\"] = np.linalg.norm(prob[\"embedding\"] - center)\n",
    "\n",
    "# Cluster statistics\n",
    "cluster_sizes = Counter(cluster_labels)\n",
    "print(f\"\\nCluster sizes: {dict(cluster_sizes)}\")\n",
    "\n",
    "# Show example from each cluster\n",
    "print(\"\\nCluster examples:\")\n",
    "for c in range(N_CLUSTERS):\n",
    "    cluster_probs = [p for p in problems if p[\"cluster\"] == c]\n",
    "    if cluster_probs:\n",
    "        ex = min(cluster_probs, key=lambda p: p[\"dist_to_center\"])\n",
    "        print(f\"  Cluster {c}: {ex['numbers']} (size={len(cluster_probs)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Archetype Identification Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.1 Embedding Centrality ---\n",
    "\n",
    "def compute_centrality_score(problem: Dict, cluster_probs: List[Dict]) -> float:\n",
    "    \"\"\"\n",
    "    Centrality = negative distance to cluster center.\n",
    "    Higher is better (closer to center).\n",
    "    \"\"\"\n",
    "    # Normalize by max distance in cluster\n",
    "    max_dist = max(p[\"dist_to_center\"] for p in cluster_probs) + 1e-6\n",
    "    return 1.0 - (problem[\"dist_to_center\"] / max_dist)\n",
    "\n",
    "\n",
    "# Compute centrality for all problems\n",
    "for c in range(N_CLUSTERS):\n",
    "    cluster_probs = [p for p in problems if p[\"cluster\"] == c]\n",
    "    for prob in cluster_probs:\n",
    "        prob[\"centrality\"] = compute_centrality_score(prob, cluster_probs)\n",
    "\n",
    "print(\"Centrality scores computed.\")\n",
    "print(f\"Example: {problems[0]['numbers']} centrality = {problems[0]['centrality']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.2 Solution Diversity (via LLM) ---\n",
    "\n",
    "call_counter = defaultdict(int)\n",
    "_llm_semaphore = asyncio.Semaphore(MAX_CONCURRENT_LLM)\n",
    "\n",
    "\n",
    "async def llm_call_async(system: str, user: str, role: str = \"generate\",\n",
    "                         temperature: float = 0.7, max_tokens: int = 512) -> str:\n",
    "    \"\"\"Async LLM call with semaphore-based concurrency control.\"\"\"\n",
    "    call_counter[role] += 1\n",
    "    async with _llm_semaphore:\n",
    "        try:\n",
    "            resp = await aclient.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": user},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "            return resp.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Async LLM call failed ({role}): {e}\")\n",
    "            return \"\"\n",
    "\n",
    "\n",
    "def llm_call(system: str, user: str, role: str = \"generate\",\n",
    "             temperature: float = 0.7, max_tokens: int = 512) -> str:\n",
    "    \"\"\"Sync LLM call.\"\"\"\n",
    "    call_counter[role] += 1\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": user},\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"LLM call failed ({role}): {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "GAME24_SYSTEM = \"\"\"You are a Game of 24 solver. Given 4 numbers, find an expression using +, -, *, / that equals 24.\n",
    "Each number must be used exactly once. Show your work step by step.\n",
    "\n",
    "Format your final answer as: ANSWER: <expression> = 24\n",
    "\n",
    "Example:\n",
    "Numbers: 1, 2, 3, 4\n",
    "Solution: 1 * 2 * 3 * 4 = 24\n",
    "ANSWER: 1 * 2 * 3 * 4 = 24\"\"\"\n",
    "\n",
    "\n",
    "async def get_llm_solutions(problem: Dict, n_samples: int = 5) -> List[str]:\n",
    "    \"\"\"Get N diverse solutions from LLM for a problem.\"\"\"\n",
    "    nums = problem[\"numbers\"]\n",
    "    user_prompt = f\"Numbers: {nums[0]}, {nums[1]}, {nums[2]}, {nums[3]}\\n\\nFind an expression that equals 24.\"\n",
    "    \n",
    "    tasks = [\n",
    "        llm_call_async(GAME24_SYSTEM, user_prompt, role=\"diversity\", temperature=0.9)\n",
    "        for _ in range(n_samples)\n",
    "    ]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Extract expressions\n",
    "    solutions = []\n",
    "    for resp in responses:\n",
    "        match = re.search(r\"ANSWER:\\s*(.+?)\\s*=\\s*24\", resp, re.IGNORECASE)\n",
    "        if match:\n",
    "            solutions.append(match.group(1).strip())\n",
    "        else:\n",
    "            # Fallback: look for any expression = 24\n",
    "            match2 = re.search(r\"([\\d\\s+\\-*/()]+)\\s*=\\s*24\", resp)\n",
    "            if match2:\n",
    "                solutions.append(match2.group(1).strip())\n",
    "    \n",
    "    return solutions\n",
    "\n",
    "\n",
    "# Compute diversity for candidate archetypes (top 3 per cluster by centrality)\n",
    "print(\"Computing solution diversity for archetype candidates...\")\n",
    "candidates = []\n",
    "for c in range(N_CLUSTERS):\n",
    "    cluster_probs = sorted([p for p in problems if p[\"cluster\"] == c],\n",
    "                           key=lambda p: -p[\"centrality\"])[:3]\n",
    "    candidates.extend(cluster_probs)\n",
    "\n",
    "print(f\"Evaluating {len(candidates)} candidates...\")\n",
    "\n",
    "async def compute_all_diversity():\n",
    "    for i, prob in enumerate(candidates):\n",
    "        llm_solutions = await get_llm_solutions(prob, n_samples=5)\n",
    "        # Diversity = number of unique valid expressions\n",
    "        unique_solutions = set(llm_solutions)\n",
    "        prob[\"llm_solutions\"] = llm_solutions\n",
    "        prob[\"diversity\"] = len(unique_solutions) / 5.0  # Normalize to [0, 1]\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(candidates)}\")\n",
    "\n",
    "asyncio.run(compute_all_diversity())\n",
    "print(f\"Diversity computed. LLM calls: {call_counter['diversity']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.3 Structural Simplicity ---\n",
    "\n",
    "def compute_solution_depth(expr: str) -> int:\n",
    "    \"\"\"Estimate the depth of a solution expression tree.\"\"\"\n",
    "    # Count nested parentheses as proxy for depth\n",
    "    max_depth = 0\n",
    "    current_depth = 0\n",
    "    for char in expr:\n",
    "        if char == '(':\n",
    "            current_depth += 1\n",
    "            max_depth = max(max_depth, current_depth)\n",
    "        elif char == ')':\n",
    "            current_depth -= 1\n",
    "    # Also count operators as contributing to depth\n",
    "    n_ops = sum(1 for c in expr if c in '+-*/')\n",
    "    return max_depth + n_ops // 2\n",
    "\n",
    "\n",
    "def compute_simplicity_score(problem: Dict) -> float:\n",
    "    \"\"\"\n",
    "    Simplicity = inverse of minimum solution tree depth.\n",
    "    Simpler problems (fewer operations) are better archetypes.\n",
    "    \"\"\"\n",
    "    # Use ground truth solutions if available\n",
    "    solutions = solve_24_exhaustive(problem[\"numbers\"])\n",
    "    if not solutions:\n",
    "        return 0.0\n",
    "    \n",
    "    min_depth = min(compute_solution_depth(s) for s in solutions[:10])  # Check up to 10\n",
    "    # Normalize: depth of 3 is simple (score=1), depth of 10 is complex (score~0.3)\n",
    "    return 1.0 / (1.0 + min_depth / 3.0)\n",
    "\n",
    "\n",
    "# Compute simplicity for candidates\n",
    "print(\"Computing simplicity scores...\")\n",
    "for prob in candidates:\n",
    "    prob[\"simplicity\"] = compute_simplicity_score(prob)\n",
    "\n",
    "print(f\"Simplicity computed.\")\n",
    "print(f\"Example: {candidates[0]['numbers']} simplicity = {candidates[0]['simplicity']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.4 Combined Ranking ---\n",
    "\n",
    "def compute_archetype_score(problem: Dict) -> float:\n",
    "    \"\"\"\n",
    "    Combined score = geometric mean of centrality * diversity * simplicity.\n",
    "    Geometric mean penalizes being weak in any dimension.\n",
    "    \"\"\"\n",
    "    centrality = problem.get(\"centrality\", 0.5)\n",
    "    diversity = problem.get(\"diversity\", 0.5)\n",
    "    simplicity = problem.get(\"simplicity\", 0.5)\n",
    "    \n",
    "    # Add small epsilon to avoid zero\n",
    "    eps = 0.01\n",
    "    return (max(centrality, eps) * max(diversity, eps) * max(simplicity, eps)) ** (1/3)\n",
    "\n",
    "\n",
    "# Compute combined scores\n",
    "for prob in candidates:\n",
    "    prob[\"archetype_score\"] = compute_archetype_score(prob)\n",
    "\n",
    "# Select top-10 archetypes (ensuring cluster diversity)\n",
    "archetypes = []\n",
    "used_clusters = set()\n",
    "\n",
    "# First pass: one per cluster\n",
    "sorted_candidates = sorted(candidates, key=lambda p: -p[\"archetype_score\"])\n",
    "for prob in sorted_candidates:\n",
    "    if prob[\"cluster\"] not in used_clusters and len(archetypes) < 10:\n",
    "        archetypes.append(prob)\n",
    "        used_clusters.add(prob[\"cluster\"])\n",
    "\n",
    "# Second pass: fill remaining slots with best overall\n",
    "for prob in sorted_candidates:\n",
    "    if prob not in archetypes and len(archetypes) < 10:\n",
    "        archetypes.append(prob)\n",
    "\n",
    "print(f\"\\nSelected {len(archetypes)} archetypes:\")\n",
    "print(\"-\" * 80)\n",
    "for i, arch in enumerate(archetypes):\n",
    "    print(f\"{i+1}. {arch['numbers']} | cluster={arch['cluster']} | \"\n",
    "          f\"score={arch['archetype_score']:.3f} \"\n",
    "          f\"(cent={arch['centrality']:.2f}, div={arch['diversity']:.2f}, simp={arch['simplicity']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verification Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.1 & 5.2 Multi-path Consistency & Execution Validation ---\n",
    "\n",
    "def safe_eval_expression(expr: str, numbers: Tuple[int, ...]) -> Tuple[bool, float, str]:\n",
    "    \"\"\"\n",
    "    Safely evaluate a math expression and verify it:\n",
    "    1. Uses all 4 numbers exactly once\n",
    "    2. Evaluates to 24\n",
    "    \n",
    "    Returns: (is_valid, result, error_msg)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clean expression\n",
    "        expr_clean = expr.replace('x', '*').replace('X', '*')\n",
    "        expr_clean = re.sub(r'\\s+', '', expr_clean)\n",
    "        \n",
    "        # Extract numbers from expression\n",
    "        expr_numbers = [int(n) for n in re.findall(r'\\d+', expr_clean)]\n",
    "        \n",
    "        # Check if all numbers are used exactly once\n",
    "        if sorted(expr_numbers) != sorted(numbers):\n",
    "            return False, 0.0, f\"Numbers mismatch: expected {sorted(numbers)}, got {sorted(expr_numbers)}\"\n",
    "        \n",
    "        # Safe eval with restricted namespace\n",
    "        allowed_chars = set('0123456789+-*/() .')\n",
    "        if not all(c in allowed_chars for c in expr_clean):\n",
    "            return False, 0.0, \"Invalid characters in expression\"\n",
    "        \n",
    "        result = eval(expr_clean)\n",
    "        \n",
    "        # Check if result is 24\n",
    "        if abs(result - 24) < 1e-6:\n",
    "            return True, result, \"\"\n",
    "        else:\n",
    "            return False, result, f\"Result is {result}, not 24\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return False, 0.0, str(e)\n",
    "\n",
    "\n",
    "async def verify_multipath_consistency(problem: Dict, n_paths: int = 5) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate N independent solutions and check consistency.\n",
    "    Returns verification results.\n",
    "    \"\"\"\n",
    "    nums = problem[\"numbers\"]\n",
    "    user_prompt = f\"Numbers: {nums[0]}, {nums[1]}, {nums[2]}, {nums[3]}\\n\\nFind an expression that equals 24.\"\n",
    "    \n",
    "    # Generate N solutions with different temperatures\n",
    "    tasks = [\n",
    "        llm_call_async(GAME24_SYSTEM, user_prompt, role=\"verify\", temperature=0.8 + 0.1 * i)\n",
    "        for i in range(n_paths)\n",
    "    ]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    \n",
    "    valid_count = 0\n",
    "    expressions = []\n",
    "    \n",
    "    for resp in responses:\n",
    "        # Extract expression\n",
    "        match = re.search(r\"ANSWER:\\s*(.+?)\\s*=\\s*24\", resp, re.IGNORECASE)\n",
    "        if match:\n",
    "            expr = match.group(1).strip()\n",
    "        else:\n",
    "            match2 = re.search(r\"([\\d\\s+\\-*/()]+)\\s*=\\s*24\", resp)\n",
    "            expr = match2.group(1).strip() if match2 else \"\"\n",
    "        \n",
    "        if expr:\n",
    "            is_valid, result, error = safe_eval_expression(expr, nums)\n",
    "            expressions.append({\"expr\": expr, \"valid\": is_valid, \"result\": result, \"error\": error})\n",
    "            if is_valid:\n",
    "                valid_count += 1\n",
    "        else:\n",
    "            expressions.append({\"expr\": \"\", \"valid\": False, \"result\": 0, \"error\": \"No expression found\"})\n",
    "    \n",
    "    consistency = valid_count / n_paths\n",
    "    execution_rate = sum(1 for e in expressions if e[\"valid\"] or e[\"result\"] != 0) / n_paths\n",
    "    \n",
    "    return {\n",
    "        \"consistency\": consistency,\n",
    "        \"execution_rate\": execution_rate,\n",
    "        \"valid_count\": valid_count,\n",
    "        \"total_paths\": n_paths,\n",
    "        \"expressions\": expressions,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.3 Perturbation Testing ---\n",
    "\n",
    "def generate_perturbations(numbers: Tuple[int, ...], n_perturb: int = 3) -> List[Tuple[int, ...]]:\n",
    "    \"\"\"\n",
    "    Generate perturbed versions of a problem by swapping numbers.\n",
    "    Only return perturbations that are solvable.\n",
    "    \"\"\"\n",
    "    perturbations = []\n",
    "    nums = list(numbers)\n",
    "    \n",
    "    # Try swapping one number at a time\n",
    "    for i in range(4):\n",
    "        for delta in [-1, 1, -2, 2]:\n",
    "            new_nums = nums.copy()\n",
    "            new_val = nums[i] + delta\n",
    "            if 1 <= new_val <= 13:\n",
    "                new_nums[i] = new_val\n",
    "                new_tuple = tuple(sorted(new_nums))\n",
    "                if new_tuple != numbers and is_solvable_24(new_tuple):\n",
    "                    perturbations.append(new_tuple)\n",
    "                    if len(perturbations) >= n_perturb:\n",
    "                        return perturbations\n",
    "    \n",
    "    return perturbations\n",
    "\n",
    "\n",
    "async def test_perturbation_robustness(problem: Dict, strategy: str, n_perturb: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Test if a strategy generalizes to perturbed problems.\n",
    "    \"\"\"\n",
    "    perturbations = generate_perturbations(problem[\"numbers\"], n_perturb)\n",
    "    \n",
    "    if not perturbations:\n",
    "        return {\"robustness\": 1.0, \"tested\": 0, \"passed\": 0}  # No perturbations possible\n",
    "    \n",
    "    system_with_strategy = f\"\"\"{GAME24_SYSTEM}\n",
    "\n",
    "Use this strategy: {strategy}\"\"\"\n",
    "    \n",
    "    tasks = []\n",
    "    for perturbed_nums in perturbations:\n",
    "        user_prompt = f\"Numbers: {perturbed_nums[0]}, {perturbed_nums[1]}, {perturbed_nums[2]}, {perturbed_nums[3]}\\n\\nFind an expression that equals 24.\"\n",
    "        tasks.append(llm_call_async(system_with_strategy, user_prompt, role=\"perturb\", temperature=0.7))\n",
    "    \n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    \n",
    "    passed = 0\n",
    "    for resp, perturbed_nums in zip(responses, perturbations):\n",
    "        match = re.search(r\"ANSWER:\\s*(.+?)\\s*=\\s*24\", resp, re.IGNORECASE)\n",
    "        if match:\n",
    "            expr = match.group(1).strip()\n",
    "            is_valid, _, _ = safe_eval_expression(expr, perturbed_nums)\n",
    "            if is_valid:\n",
    "                passed += 1\n",
    "    \n",
    "    robustness = passed / len(perturbations) if perturbations else 1.0\n",
    "    return {\"robustness\": robustness, \"tested\": len(perturbations), \"passed\": passed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.4 Adversarial Probing ---\n",
    "\n",
    "async def test_adversarial(strategy: str, hard_problems: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Test strategy against known-hard problems.\n",
    "    \"\"\"\n",
    "    system_with_strategy = f\"\"\"{GAME24_SYSTEM}\n",
    "\n",
    "Use this strategy: {strategy}\"\"\"\n",
    "    \n",
    "    tasks = []\n",
    "    for hp in hard_problems:\n",
    "        nums = hp[\"numbers\"]\n",
    "        user_prompt = f\"Numbers: {nums[0]}, {nums[1]}, {nums[2]}, {nums[3]}\\n\\nFind an expression that equals 24.\"\n",
    "        tasks.append(llm_call_async(system_with_strategy, user_prompt, role=\"adversarial\", temperature=0.7))\n",
    "    \n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    \n",
    "    solved = 0\n",
    "    results = []\n",
    "    for resp, hp in zip(responses, hard_problems):\n",
    "        match = re.search(r\"ANSWER:\\s*(.+?)\\s*=\\s*24\", resp, re.IGNORECASE)\n",
    "        if match:\n",
    "            expr = match.group(1).strip()\n",
    "            is_valid, result, error = safe_eval_expression(expr, hp[\"numbers\"])\n",
    "            results.append({\"numbers\": hp[\"numbers\"], \"valid\": is_valid, \"expr\": expr})\n",
    "            if is_valid:\n",
    "                solved += 1\n",
    "        else:\n",
    "            results.append({\"numbers\": hp[\"numbers\"], \"valid\": False, \"expr\": \"\"})\n",
    "    \n",
    "    adversarial_score = solved / len(hard_problems) if hard_problems else 0.0\n",
    "    return {\"adversarial_score\": adversarial_score, \"solved\": solved, \"total\": len(hard_problems), \"results\": results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.5 Confidence Scoring ---\n",
    "\n",
    "async def compute_full_verification(problem: Dict, strategy: str = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Run full verification suite and compute confidence score.\n",
    "    \n",
    "    Confidence = consistency * execution_rate * robustness * (1 + adversarial_bonus)\n",
    "    \"\"\"\n",
    "    # Multi-path consistency\n",
    "    multipath = await verify_multipath_consistency(problem, n_paths=5)\n",
    "    \n",
    "    # Extract strategy from valid solutions\n",
    "    if strategy is None:\n",
    "        valid_exprs = [e[\"expr\"] for e in multipath[\"expressions\"] if e[\"valid\"]]\n",
    "        if valid_exprs:\n",
    "            # Use the most common valid expression as the strategy\n",
    "            strategy = f\"For numbers like {problem['numbers']}, try: {valid_exprs[0]}\"\n",
    "        else:\n",
    "            # Fallback to ground truth\n",
    "            solutions = solve_24_exhaustive(problem[\"numbers\"])\n",
    "            if solutions:\n",
    "                strategy = f\"For numbers like {problem['numbers']}, try: {solutions[0]}\"\n",
    "            else:\n",
    "                strategy = \"Look for factor pairs that multiply to 24\"\n",
    "    \n",
    "    # Perturbation robustness\n",
    "    perturb = await test_perturbation_robustness(problem, strategy, n_perturb=3)\n",
    "    \n",
    "    # Adversarial testing (only on a subset)\n",
    "    adversarial = await test_adversarial(strategy, HARD_PROBLEMS[:5])\n",
    "    \n",
    "    # Compute confidence\n",
    "    consistency = multipath[\"consistency\"]\n",
    "    execution_rate = multipath[\"execution_rate\"]\n",
    "    robustness = perturb[\"robustness\"]\n",
    "    adversarial_bonus = adversarial[\"adversarial_score\"] * 0.2  # Up to 20% bonus\n",
    "    \n",
    "    confidence = consistency * execution_rate * robustness * (1 + adversarial_bonus)\n",
    "    \n",
    "    return {\n",
    "        \"confidence\": confidence,\n",
    "        \"consistency\": consistency,\n",
    "        \"execution_rate\": execution_rate,\n",
    "        \"robustness\": robustness,\n",
    "        \"adversarial_bonus\": adversarial_bonus,\n",
    "        \"strategy\": strategy,\n",
    "        \"multipath\": multipath,\n",
    "        \"perturbation\": perturb,\n",
    "        \"adversarial\": adversarial,\n",
    "    }\n",
    "\n",
    "\n",
    "# Run verification on all archetypes\n",
    "print(\"Running verification suite on archetypes...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "async def verify_all_archetypes():\n",
    "    for i, arch in enumerate(archetypes):\n",
    "        print(f\"\\nVerifying archetype {i+1}/{len(archetypes)}: {arch['numbers']}\")\n",
    "        verification = await compute_full_verification(arch)\n",
    "        arch[\"verification\"] = verification\n",
    "        print(f\"  Confidence: {verification['confidence']:.3f}\")\n",
    "        print(f\"  Consistency: {verification['consistency']:.3f}, Execution: {verification['execution_rate']:.3f}\")\n",
    "        print(f\"  Robustness: {verification['robustness']:.3f}, Adversarial: {verification['adversarial_bonus']:.3f}\")\n",
    "\n",
    "asyncio.run(verify_all_archetypes())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Verification complete. Total LLM calls: {sum(call_counter.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Playbook Bootstrap from Verified Archetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Playbook Data Structure ---\n",
    "\n",
    "@dataclass\n",
    "class PlaybookBullet:\n",
    "    id: str\n",
    "    content: str\n",
    "    confidence: float\n",
    "    source_problem: Tuple[int, ...]\n",
    "    helpful_count: int = 0\n",
    "    harmful_count: int = 0\n",
    "    \n",
    "    def to_str(self) -> str:\n",
    "        return f\"[{self.id}] confidence={self.confidence:.2f} helpful={self.helpful_count} harmful={self.harmful_count} :: {self.content}\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Playbook:\n",
    "    bullets: List[PlaybookBullet] = field(default_factory=list)\n",
    "    _next_id: int = 1\n",
    "    \n",
    "    def add(self, content: str, confidence: float, source_problem: Tuple[int, ...]) -> str:\n",
    "        bid = f\"arch-{self._next_id:05d}\"\n",
    "        self._next_id += 1\n",
    "        self.bullets.append(PlaybookBullet(\n",
    "            id=bid, content=content, confidence=confidence, source_problem=source_problem\n",
    "        ))\n",
    "        return bid\n",
    "    \n",
    "    def tag(self, bid: str, label: str):\n",
    "        for b in self.bullets:\n",
    "            if b.id == bid:\n",
    "                if label == \"helpful\":\n",
    "                    b.helpful_count += 1\n",
    "                elif label == \"harmful\":\n",
    "                    b.harmful_count += 1\n",
    "    \n",
    "    def update_confidence(self, bid: str, delta: float):\n",
    "        for b in self.bullets:\n",
    "            if b.id == bid:\n",
    "                b.confidence = max(0.0, min(1.0, b.confidence + delta))\n",
    "    \n",
    "    def to_str(self) -> str:\n",
    "        if not self.bullets:\n",
    "            return \"(empty playbook)\"\n",
    "        lines = [\"## GAME OF 24 STRATEGIES\"]\n",
    "        for b in sorted(self.bullets, key=lambda x: -x.confidence):\n",
    "            lines.append(b.to_str())\n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    def copy(self) -> \"Playbook\":\n",
    "        return copy.deepcopy(self)\n",
    "    \n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return len(self.bullets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Extract Strategies from Archetypes ---\n",
    "\n",
    "STRATEGY_EXTRACTION_SYSTEM = \"\"\"You are analyzing Game of 24 solutions to extract reusable strategies.\n",
    "\n",
    "Given a solved problem, extract 1-2 general strategies that could help solve similar problems.\n",
    "Focus on:\n",
    "- Patterns involving multiplication/division to reach 24\n",
    "- How to use specific numbers (like 6, 8, 4, 3, 2, 1)\n",
    "- When to use fractions vs whole number operations\n",
    "- Factor pairs that make 24: (1,24), (2,12), (3,8), (4,6)\n",
    "\n",
    "Format each strategy as a single line starting with \"STRATEGY:\" \n",
    "Make strategies general enough to apply to other problems.\n",
    "\n",
    "Example output:\n",
    "STRATEGY: When you have 6 and 4, try to make the other numbers equal 1 (e.g., 3/3=1) so 6*4*1=24\n",
    "STRATEGY: If you have 8, look for ways to make 3 from the other numbers since 8*3=24\"\"\"\n",
    "\n",
    "\n",
    "async def extract_strategies_from_archetype(arch: Dict) -> List[str]:\n",
    "    \"\"\"Extract general strategies from a verified archetype.\"\"\"\n",
    "    nums = arch[\"numbers\"]\n",
    "    solutions = solve_24_exhaustive(nums)[:3]  # Use up to 3 solutions\n",
    "    \n",
    "    user_prompt = f\"\"\"Problem: Use {nums[0]}, {nums[1]}, {nums[2]}, {nums[3]} to make 24\n",
    "\n",
    "Valid solutions:\n",
    "\"\"\"\n",
    "    for sol in solutions:\n",
    "        user_prompt += f\"- {sol}\\n\"\n",
    "    \n",
    "    user_prompt += \"\\nExtract 1-2 reusable strategies from these solutions.\"\n",
    "    \n",
    "    response = await llm_call_async(STRATEGY_EXTRACTION_SYSTEM, user_prompt, role=\"extract\", temperature=0.5)\n",
    "    \n",
    "    strategies = []\n",
    "    for line in response.split(\"\\n\"):\n",
    "        if line.strip().startswith(\"STRATEGY:\"):\n",
    "            strategy = line.replace(\"STRATEGY:\", \"\").strip()\n",
    "            if strategy:\n",
    "                strategies.append(strategy)\n",
    "    \n",
    "    return strategies\n",
    "\n",
    "\n",
    "# Bootstrap playbook from verified archetypes\n",
    "print(\"Bootstrapping playbook from verified archetypes...\")\n",
    "playbook = Playbook()\n",
    "\n",
    "# Add base strategies\n",
    "base_strategies = [\n",
    "    \"Look for factor pairs of 24: (1,24), (2,12), (3,8), (4,6)\",\n",
    "    \"If you have 8, try to make 3 from remaining numbers (8*3=24)\",\n",
    "    \"If you have 6, try to make 4 from remaining numbers (6*4=24)\",\n",
    "    \"Use division to create fractions when direct operations don't work\",\n",
    "    \"Try (a+b)*(c-d) or (a-b)*(c+d) patterns for 24\",\n",
    "]\n",
    "for strat in base_strategies:\n",
    "    playbook.add(strat, confidence=0.7, source_problem=(0, 0, 0, 0))\n",
    "\n",
    "async def bootstrap_playbook():\n",
    "    for arch in archetypes:\n",
    "        confidence = arch.get(\"verification\", {}).get(\"confidence\", 0.5)\n",
    "        if confidence < 0.3:\n",
    "            continue  # Skip low-confidence archetypes\n",
    "        \n",
    "        strategies = await extract_strategies_from_archetype(arch)\n",
    "        for strat in strategies:\n",
    "            playbook.add(strat, confidence=confidence, source_problem=arch[\"numbers\"])\n",
    "        \n",
    "        print(f\"  {arch['numbers']}: extracted {len(strategies)} strategies (conf={confidence:.2f})\")\n",
    "\n",
    "asyncio.run(bootstrap_playbook())\n",
    "\n",
    "print(f\"\\nPlaybook initialized with {playbook.size} strategies:\")\n",
    "print(\"-\" * 80)\n",
    "print(playbook.to_str())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LinUCB Contextual Bandit Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LinUCB Contextual Bandit ---\n",
    "\n",
    "class LinUCB:\n",
    "    \"\"\"\n",
    "    Linear Upper Confidence Bound algorithm for contextual bandits.\n",
    "    \n",
    "    Each arm maintains:\n",
    "    - A_k: d x d matrix (initialized to identity)\n",
    "    - b_k: d-dimensional vector (initialized to zero)\n",
    "    - theta_k = A_k^{-1} b_k (estimated coefficients)\n",
    "    \n",
    "    Selection: argmax_k (theta_k^T x_t + alpha * sqrt(x_t^T A_k^{-1} x_t))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms: int, d: int, alpha: float = 1.0):\n",
    "        self.n_arms = n_arms\n",
    "        self.d = d\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Initialize A and b for each arm\n",
    "        self.A = [np.eye(d) for _ in range(n_arms)]\n",
    "        self.b = [np.zeros(d) for _ in range(n_arms)]\n",
    "        \n",
    "        # Track statistics\n",
    "        self.arm_counts = np.zeros(n_arms)\n",
    "        self.arm_rewards = np.zeros(n_arms)\n",
    "    \n",
    "    def select(self, context: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Select arm with highest UCB value.\n",
    "        \n",
    "        Args:\n",
    "            context: d-dimensional feature vector\n",
    "        \n",
    "        Returns:\n",
    "            Selected arm index\n",
    "        \"\"\"\n",
    "        ucb_values = np.zeros(self.n_arms)\n",
    "        \n",
    "        for k in range(self.n_arms):\n",
    "            A_inv = np.linalg.inv(self.A[k])\n",
    "            theta_k = A_inv @ self.b[k]\n",
    "            \n",
    "            # UCB = theta^T x + alpha * sqrt(x^T A^{-1} x)\n",
    "            exploitation = theta_k @ context\n",
    "            exploration = self.alpha * np.sqrt(context @ A_inv @ context)\n",
    "            ucb_values[k] = exploitation + exploration\n",
    "        \n",
    "        return int(np.argmax(ucb_values))\n",
    "    \n",
    "    def update(self, arm: int, context: np.ndarray, reward: float):\n",
    "        \"\"\"\n",
    "        Update arm statistics after observing reward.\n",
    "        \n",
    "        A_k += x_t x_t^T\n",
    "        b_k += r_t x_t\n",
    "        \"\"\"\n",
    "        self.A[arm] += np.outer(context, context)\n",
    "        self.b[arm] += reward * context\n",
    "        self.arm_counts[arm] += 1\n",
    "        self.arm_rewards[arm] += reward\n",
    "    \n",
    "    def get_theta(self, arm: int) -> np.ndarray:\n",
    "        \"\"\"Get estimated coefficients for an arm.\"\"\"\n",
    "        A_inv = np.linalg.inv(self.A[arm])\n",
    "        return A_inv @ self.b[arm]\n",
    "    \n",
    "    def get_ucb_gap(self, context: np.ndarray) -> float:\n",
    "        \"\"\"Get the gap between best and second-best UCB values.\"\"\"\n",
    "        ucb_values = []\n",
    "        for k in range(self.n_arms):\n",
    "            A_inv = np.linalg.inv(self.A[k])\n",
    "            theta_k = A_inv @ self.b[k]\n",
    "            ucb = theta_k @ context + self.alpha * np.sqrt(context @ A_inv @ context)\n",
    "            ucb_values.append(ucb)\n",
    "        \n",
    "        sorted_ucb = sorted(ucb_values, reverse=True)\n",
    "        return sorted_ucb[0] - sorted_ucb[1] if len(sorted_ucb) > 1 else 0.0\n",
    "\n",
    "\n",
    "print(\"LinUCB implementation ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Problem Feature Extraction for LinUCB ---\n",
    "\n",
    "def extract_linucb_features(problem: Dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract feature vector for LinUCB context.\n",
    "    Uses the same numerical features as for clustering.\n",
    "    \"\"\"\n",
    "    nums = problem[\"numbers\"]\n",
    "    \n",
    "    features = [\n",
    "        sum(nums) / 52,  # sum normalized\n",
    "        np.prod(nums) / (13**4),  # product normalized\n",
    "        max(nums) / 13,  # max normalized\n",
    "        min(nums) / 13,  # min normalized\n",
    "        len(set(nums)) / 4,  # uniqueness ratio\n",
    "        (max(nums) - min(nums)) / 12,  # range normalized\n",
    "        np.std(nums) / 5,  # std normalized\n",
    "        sum(1 for n in nums if n % 2 == 0) / 4,  # even ratio\n",
    "        sum(1 for n in nums if 24 % n == 0) / 4,  # divisor of 24 ratio\n",
    "        1.0 if any(a * b == 24 for a, b in itertools.combinations(nums, 2)) else 0.0,  # has factor pair\n",
    "        # Additional features for curriculum learning\n",
    "        problem.get(\"n_solutions\", 5) / 20,  # solution count normalized\n",
    "        problem.get(\"centrality\", 0.5),  # cluster centrality\n",
    "        1.0,  # bias term\n",
    "    ]\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "# Verify feature extraction\n",
    "test_features = extract_linucb_features(problems[0])\n",
    "print(f\"Feature dimension: {len(test_features)}\")\n",
    "print(f\"Example features: {test_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Curriculum Loop (Archetype-first vs Random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core Evaluation Functions ---\n",
    "\n",
    "SOLVE_SYSTEM_TEMPLATE = \"\"\"You are a Game of 24 solver. Given 4 numbers, find an expression using +, -, *, / that equals 24.\n",
    "Each number must be used exactly once.\n",
    "\n",
    "{playbook}\n",
    "\n",
    "When you use a strategy from the playbook, mention its ID (e.g., [arch-00001]).\n",
    "Show your reasoning, then give the final answer as: ANSWER: <expression> = 24\"\"\"\n",
    "\n",
    "\n",
    "async def solve_with_playbook(problem: Dict, playbook: Playbook) -> Dict:\n",
    "    \"\"\"Solve a Game of 24 problem using the playbook.\"\"\"\n",
    "    nums = problem[\"numbers\"]\n",
    "    \n",
    "    system = SOLVE_SYSTEM_TEMPLATE.format(playbook=playbook.to_str())\n",
    "    user_prompt = f\"Numbers: {nums[0]}, {nums[1]}, {nums[2]}, {nums[3]}\\n\\nFind an expression that equals 24.\"\n",
    "    \n",
    "    response = await llm_call_async(system, user_prompt, role=\"solve\", temperature=0.7)\n",
    "    \n",
    "    # Extract answer\n",
    "    match = re.search(r\"ANSWER:\\s*(.+?)\\s*=\\s*24\", response, re.IGNORECASE)\n",
    "    if match:\n",
    "        expr = match.group(1).strip()\n",
    "    else:\n",
    "        match2 = re.search(r\"([\\d\\s+\\-*/()]+)\\s*=\\s*24\", response)\n",
    "        expr = match2.group(1).strip() if match2 else \"\"\n",
    "    \n",
    "    # Validate\n",
    "    is_correct, result, error = safe_eval_expression(expr, nums)\n",
    "    \n",
    "    # Extract used bullet IDs\n",
    "    bullets_used = re.findall(r\"\\[(arch-\\d+)\\]\", response)\n",
    "    \n",
    "    return {\n",
    "        \"correct\": is_correct,\n",
    "        \"expression\": expr,\n",
    "        \"result\": result,\n",
    "        \"error\": error,\n",
    "        \"bullets_used\": bullets_used,\n",
    "        \"response\": response,\n",
    "    }\n",
    "\n",
    "\n",
    "def update_playbook_from_result(playbook: Playbook, result: Dict):\n",
    "    \"\"\"Update playbook bullet statistics based on result.\"\"\"\n",
    "    label = \"helpful\" if result[\"correct\"] else \"harmful\"\n",
    "    for bid in result[\"bullets_used\"]:\n",
    "        playbook.tag(bid, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run Log ---\n",
    "\n",
    "@dataclass\n",
    "class CurriculumRunLog:\n",
    "    \"\"\"Tracks results for a curriculum condition.\"\"\"\n",
    "    correct: List[bool] = field(default_factory=list)\n",
    "    playbook_sizes: List[int] = field(default_factory=list)\n",
    "    confidence_history: List[List[float]] = field(default_factory=list)  # Per-problem bullet confidences\n",
    "    ucb_gaps: List[float] = field(default_factory=list)\n",
    "    problems_order: List[Tuple[int, ...]] = field(default_factory=list)\n",
    "    call_counts: Dict[str, int] = field(default_factory=dict)\n",
    "    final_playbook: Optional[Playbook] = None\n",
    "    \n",
    "    @property\n",
    "    def cumulative_accuracy(self) -> List[float]:\n",
    "        acc = []\n",
    "        total = 0\n",
    "        for i, c in enumerate(self.correct):\n",
    "            total += int(c)\n",
    "            acc.append(total / (i + 1))\n",
    "        return acc\n",
    "    \n",
    "    @property\n",
    "    def final_accuracy(self) -> float:\n",
    "        return sum(self.correct) / len(self.correct) if self.correct else 0.0\n",
    "    \n",
    "    def memory_precision(self) -> List[float]:\n",
    "        \"\"\"Fraction of playbook bullets that are net-helpful over time.\"\"\"\n",
    "        # This would need playbook snapshots - simplified version\n",
    "        return [1.0] * len(self.correct)  # Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Archetype-First Curriculum ---\n",
    "\n",
    "CONFIDENCE_UPDATE_INTERVAL = 10\n",
    "N_EVAL_PROBLEMS = 100\n",
    "\n",
    "async def run_archetype_curriculum(all_problems: List[Dict], archetypes: List[Dict],\n",
    "                                   initial_playbook: Playbook) -> CurriculumRunLog:\n",
    "    \"\"\"\n",
    "    Run archetype-first curriculum:\n",
    "    1. Phase 1: Process archetypes first (warm start)\n",
    "    2. Phase 2: LinUCB exploration-exploitation on remaining problems\n",
    "    \"\"\"\n",
    "    log = CurriculumRunLog()\n",
    "    pb = initial_playbook.copy()\n",
    "    \n",
    "    # Separate archetypes from other problems\n",
    "    archetype_nums = {a[\"numbers\"] for a in archetypes}\n",
    "    non_archetypes = [p for p in all_problems if p[\"numbers\"] not in archetype_nums]\n",
    "    \n",
    "    # Limit to N_EVAL_PROBLEMS total\n",
    "    eval_archetypes = archetypes[:min(10, N_EVAL_PROBLEMS // 10)]\n",
    "    remaining_budget = N_EVAL_PROBLEMS - len(eval_archetypes)\n",
    "    eval_non_archetypes = non_archetypes[:remaining_budget]\n",
    "    \n",
    "    # Initialize LinUCB (arms = different strategy emphasis levels)\n",
    "    # Arm 0: focus on factor pairs, Arm 1: focus on fractions, Arm 2: balanced\n",
    "    N_ARMS = 3\n",
    "    FEATURE_DIM = 13\n",
    "    linucb = LinUCB(n_arms=N_ARMS, d=FEATURE_DIM, alpha=1.5)\n",
    "    \n",
    "    print(f\"\\nPhase 1: Processing {len(eval_archetypes)} archetypes...\")\n",
    "    \n",
    "    # Phase 1: Archetypes (use balanced arm)\n",
    "    for i, arch in enumerate(eval_archetypes):\n",
    "        result = await solve_with_playbook(arch, pb)\n",
    "        log.correct.append(result[\"correct\"])\n",
    "        log.playbook_sizes.append(pb.size)\n",
    "        log.problems_order.append(arch[\"numbers\"])\n",
    "        log.confidence_history.append([b.confidence for b in pb.bullets])\n",
    "        \n",
    "        # Update playbook\n",
    "        update_playbook_from_result(pb, result)\n",
    "        \n",
    "        # Update LinUCB with archetype results (arm 2 = balanced)\n",
    "        context = extract_linucb_features(arch)\n",
    "        linucb.update(2, context, 1.0 if result[\"correct\"] else 0.0)\n",
    "        \n",
    "        if (i + 1) % 5 == 0:\n",
    "            acc = sum(log.correct) / len(log.correct)\n",
    "            print(f\"  Archetypes {i+1}/{len(eval_archetypes)}: accuracy={acc:.2%}\")\n",
    "    \n",
    "    print(f\"\\nPhase 2: LinUCB on {len(eval_non_archetypes)} remaining problems...\")\n",
    "    \n",
    "    # Phase 2: LinUCB exploration\n",
    "    for i, prob in enumerate(eval_non_archetypes):\n",
    "        context = extract_linucb_features(prob)\n",
    "        \n",
    "        # Select arm\n",
    "        arm = linucb.select(context)\n",
    "        log.ucb_gaps.append(linucb.get_ucb_gap(context))\n",
    "        \n",
    "        # Solve problem\n",
    "        result = await solve_with_playbook(prob, pb)\n",
    "        log.correct.append(result[\"correct\"])\n",
    "        log.playbook_sizes.append(pb.size)\n",
    "        log.problems_order.append(prob[\"numbers\"])\n",
    "        log.confidence_history.append([b.confidence for b in pb.bullets])\n",
    "        \n",
    "        # Update\n",
    "        reward = 1.0 if result[\"correct\"] else 0.0\n",
    "        linucb.update(arm, context, reward)\n",
    "        update_playbook_from_result(pb, result)\n",
    "        \n",
    "        # Retrospective confidence update\n",
    "        if (i + 1) % CONFIDENCE_UPDATE_INTERVAL == 0:\n",
    "            for b in pb.bullets:\n",
    "                if b.helpful_count + b.harmful_count > 0:\n",
    "                    ratio = b.helpful_count / (b.helpful_count + b.harmful_count)\n",
    "                    b.confidence = 0.7 * b.confidence + 0.3 * ratio\n",
    "        \n",
    "        total_done = len(eval_archetypes) + i + 1\n",
    "        if total_done % 20 == 0:\n",
    "            acc = sum(log.correct) / len(log.correct)\n",
    "            print(f\"  Problems {total_done}/{N_EVAL_PROBLEMS}: accuracy={acc:.2%}\")\n",
    "    \n",
    "    log.final_playbook = pb\n",
    "    log.call_counts = dict(call_counter)\n",
    "    \n",
    "    print(f\"\\nArchetype curriculum complete. Final accuracy: {log.final_accuracy:.2%}\")\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Random Curriculum (Baseline) ---\n",
    "\n",
    "async def run_random_curriculum(all_problems: List[Dict], \n",
    "                                initial_playbook: Playbook) -> CurriculumRunLog:\n",
    "    \"\"\"\n",
    "    Run random curriculum baseline:\n",
    "    Same problems in random order, same playbook updates.\n",
    "    \"\"\"\n",
    "    log = CurriculumRunLog()\n",
    "    pb = initial_playbook.copy()\n",
    "    \n",
    "    # Shuffle problems\n",
    "    eval_problems = all_problems[:N_EVAL_PROBLEMS]\n",
    "    rng = random.Random(SEED + 1)  # Different seed for random order\n",
    "    shuffled = eval_problems.copy()\n",
    "    rng.shuffle(shuffled)\n",
    "    \n",
    "    print(f\"\\nProcessing {len(shuffled)} problems in random order...\")\n",
    "    \n",
    "    for i, prob in enumerate(shuffled):\n",
    "        result = await solve_with_playbook(prob, pb)\n",
    "        log.correct.append(result[\"correct\"])\n",
    "        log.playbook_sizes.append(pb.size)\n",
    "        log.problems_order.append(prob[\"numbers\"])\n",
    "        log.confidence_history.append([b.confidence for b in pb.bullets])\n",
    "        \n",
    "        # Update playbook\n",
    "        update_playbook_from_result(pb, result)\n",
    "        \n",
    "        # Retrospective confidence update\n",
    "        if (i + 1) % CONFIDENCE_UPDATE_INTERVAL == 0:\n",
    "            for b in pb.bullets:\n",
    "                if b.helpful_count + b.harmful_count > 0:\n",
    "                    ratio = b.helpful_count / (b.helpful_count + b.harmful_count)\n",
    "                    b.confidence = 0.7 * b.confidence + 0.3 * ratio\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            acc = sum(log.correct) / len(log.correct)\n",
    "            print(f\"  Problems {i+1}/{len(shuffled)}: accuracy={acc:.2%}\")\n",
    "    \n",
    "    log.final_playbook = pb\n",
    "    log.call_counts = dict(call_counter)\n",
    "    \n",
    "    print(f\"\\nRandom curriculum complete. Final accuracy: {log.final_accuracy:.2%}\")\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run Both Conditions ---\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RUNNING ARCHETYPE-FIRST CURRICULUM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Reset call counter\n",
    "call_counter.clear()\n",
    "\n",
    "archetype_log = asyncio.run(run_archetype_curriculum(problems, archetypes, playbook))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING RANDOM CURRICULUM (BASELINE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Reset call counter\n",
    "call_counter.clear()\n",
    "\n",
    "random_log = asyncio.run(run_random_curriculum(problems, playbook))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Archetype-first accuracy: {archetype_log.final_accuracy:.2%}\")\n",
    "print(f\"Random baseline accuracy: {random_log.final_accuracy:.2%}\")\n",
    "print(f\"Improvement: {(archetype_log.final_accuracy - random_log.final_accuracy)*100:.1f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analysis & Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot 1: Accuracy Curves ---\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Cumulative accuracy over time\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(archetype_log.cumulative_accuracy, label='Archetype-first', color='blue', linewidth=2)\n",
    "ax1.plot(random_log.cumulative_accuracy, label='Random', color='red', linewidth=2, linestyle='--')\n",
    "ax1.axvline(x=len(archetypes), color='green', linestyle=':', label='End of archetype phase')\n",
    "ax1.set_xlabel('Problems Solved')\n",
    "ax1.set_ylabel('Cumulative Accuracy')\n",
    "ax1.set_title('Learning Curves: Archetype-first vs Random')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Plot 2: Memory Precision (Playbook Quality)\n",
    "ax2 = axes[0, 1]\n",
    "\n",
    "# Calculate precision as helpful/(helpful+harmful) for each bullet over time\n",
    "def calculate_precision_over_time(log: CurriculumRunLog) -> List[float]:\n",
    "    precisions = []\n",
    "    if log.final_playbook:\n",
    "        for conf_snapshot in log.confidence_history:\n",
    "            if conf_snapshot:\n",
    "                precisions.append(np.mean(conf_snapshot))\n",
    "            else:\n",
    "                precisions.append(0.5)\n",
    "    return precisions if precisions else [0.5] * len(log.correct)\n",
    "\n",
    "arch_precision = calculate_precision_over_time(archetype_log)\n",
    "rand_precision = calculate_precision_over_time(random_log)\n",
    "\n",
    "ax2.plot(arch_precision, label='Archetype-first', color='blue', linewidth=2)\n",
    "ax2.plot(rand_precision, label='Random', color='red', linewidth=2, linestyle='--')\n",
    "ax2.set_xlabel('Problems Solved')\n",
    "ax2.set_ylabel('Mean Strategy Confidence')\n",
    "ax2.set_title('Strategy Confidence Over Time')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Confidence Distribution (Final)\n",
    "ax3 = axes[1, 0]\n",
    "if archetype_log.final_playbook and random_log.final_playbook:\n",
    "    arch_confs = [b.confidence for b in archetype_log.final_playbook.bullets]\n",
    "    rand_confs = [b.confidence for b in random_log.final_playbook.bullets]\n",
    "    \n",
    "    ax3.hist(arch_confs, bins=10, alpha=0.6, label='Archetype-first', color='blue')\n",
    "    ax3.hist(rand_confs, bins=10, alpha=0.6, label='Random', color='red')\n",
    "    ax3.set_xlabel('Strategy Confidence')\n",
    "    ax3.set_ylabel('Count')\n",
    "    ax3.set_title('Final Strategy Confidence Distribution')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: UCB Gap (Exploration vs Exploitation)\n",
    "ax4 = axes[1, 1]\n",
    "if archetype_log.ucb_gaps:\n",
    "    ax4.plot(archetype_log.ucb_gaps, color='blue', linewidth=1, alpha=0.7)\n",
    "    ax4.axhline(y=np.mean(archetype_log.ucb_gaps), color='blue', linestyle='--', \n",
    "                label=f'Mean: {np.mean(archetype_log.ucb_gaps):.3f}')\n",
    "    ax4.set_xlabel('Problem Index (Phase 2)')\n",
    "    ax4.set_ylabel('UCB Gap')\n",
    "    ax4.set_title('LinUCB Exploration-Exploitation Gap')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('archetype_discovery_results.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Plots saved to archetype_discovery_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Results Summary & Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bootstrap Confidence Intervals ---\n",
    "\n",
    "def bootstrap_ci(data: List[bool], n_bootstrap: int = 1000, ci: float = 0.95) -> Tuple[float, float, float]:\n",
    "    \"\"\"Calculate bootstrap confidence interval for accuracy.\"\"\"\n",
    "    data = np.array(data, dtype=float)\n",
    "    n = len(data)\n",
    "    \n",
    "    bootstrap_means = []\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    for _ in range(n_bootstrap):\n",
    "        sample = rng.choice(data, size=n, replace=True)\n",
    "        bootstrap_means.append(np.mean(sample))\n",
    "    \n",
    "    alpha = 1 - ci\n",
    "    lower = np.percentile(bootstrap_means, alpha/2 * 100)\n",
    "    upper = np.percentile(bootstrap_means, (1 - alpha/2) * 100)\n",
    "    mean = np.mean(data)\n",
    "    \n",
    "    return mean, lower, upper\n",
    "\n",
    "\n",
    "# --- Wilcoxon Signed-Rank Test ---\n",
    "\n",
    "def paired_comparison(arch_correct: List[bool], rand_correct: List[bool]) -> Dict:\n",
    "    \"\"\"\n",
    "    Compare two conditions using Wilcoxon signed-rank test.\n",
    "    Note: This requires paired data (same problems in different order).\n",
    "    \"\"\"\n",
    "    # Use sliding window accuracy for comparison\n",
    "    window = 10\n",
    "    arch_windows = [np.mean(arch_correct[max(0,i-window):i+1]) for i in range(len(arch_correct))]\n",
    "    rand_windows = [np.mean(rand_correct[max(0,i-window):i+1]) for i in range(len(rand_correct))]\n",
    "    \n",
    "    # Truncate to same length\n",
    "    min_len = min(len(arch_windows), len(rand_windows))\n",
    "    arch_windows = arch_windows[:min_len]\n",
    "    rand_windows = rand_windows[:min_len]\n",
    "    \n",
    "    # Wilcoxon test\n",
    "    try:\n",
    "        stat, p_value = stats.wilcoxon(arch_windows, rand_windows, alternative='greater')\n",
    "    except ValueError:\n",
    "        # All differences are zero\n",
    "        stat, p_value = 0, 1.0\n",
    "    \n",
    "    return {\n",
    "        \"statistic\": stat,\n",
    "        \"p_value\": p_value,\n",
    "        \"significant\": p_value < 0.05,\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Results Summary ---\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STATISTICAL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Bootstrap CIs\n",
    "arch_mean, arch_lower, arch_upper = bootstrap_ci(archetype_log.correct)\n",
    "rand_mean, rand_lower, rand_upper = bootstrap_ci(random_log.correct)\n",
    "\n",
    "print(f\"\\nArchetype-first Accuracy: {arch_mean:.2%} (95% CI: [{arch_lower:.2%}, {arch_upper:.2%}])\")\n",
    "print(f\"Random Baseline Accuracy: {rand_mean:.2%} (95% CI: [{rand_lower:.2%}, {rand_upper:.2%}])\")\n",
    "\n",
    "# Wilcoxon test\n",
    "wilcoxon_result = paired_comparison(archetype_log.correct, random_log.correct)\n",
    "print(f\"\\nWilcoxon Signed-Rank Test:\")\n",
    "print(f\"  Statistic: {wilcoxon_result['statistic']:.2f}\")\n",
    "print(f\"  P-value: {wilcoxon_result['p_value']:.4f}\")\n",
    "print(f\"  Significant (p < 0.05): {wilcoxon_result['significant']}\")\n",
    "\n",
    "# Effect size (Cohen's h for proportions)\n",
    "def cohens_h(p1, p2):\n",
    "    return 2 * (np.arcsin(np.sqrt(p1)) - np.arcsin(np.sqrt(p2)))\n",
    "\n",
    "effect = cohens_h(arch_mean, rand_mean)\n",
    "print(f\"\\nEffect Size (Cohen's h): {effect:.3f}\")\n",
    "if abs(effect) < 0.2:\n",
    "    print(\"  Interpretation: Small effect\")\n",
    "elif abs(effect) < 0.5:\n",
    "    print(\"  Interpretation: Medium effect\")\n",
    "else:\n",
    "    print(\"  Interpretation: Large effect\")\n",
    "\n",
    "# Coverage efficiency\n",
    "print(f\"\\nCoverage Efficiency:\")\n",
    "print(f\"  Archetype-first reached 50% acc after: {next((i for i, a in enumerate(archetype_log.cumulative_accuracy) if a >= 0.5), 'N/A')} problems\")\n",
    "print(f\"  Random reached 50% acc after: {next((i for i, a in enumerate(random_log.cumulative_accuracy) if a >= 0.5), 'N/A')} problems\")\n",
    "\n",
    "# Strategy churn (how much playbook changed)\n",
    "print(f\"\\nFinal Playbook Stats:\")\n",
    "if archetype_log.final_playbook:\n",
    "    total_tags = sum(b.helpful_count + b.harmful_count for b in archetype_log.final_playbook.bullets)\n",
    "    print(f\"  Archetype-first: {archetype_log.final_playbook.size} strategies, {total_tags} total tags\")\n",
    "if random_log.final_playbook:\n",
    "    total_tags = sum(b.helpful_count + b.harmful_count for b in random_log.final_playbook.bullets)\n",
    "    print(f\"  Random: {random_log.final_playbook.size} strategies, {total_tags} total tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Summary ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "Verified Archetype Discovery PoC Results\n",
    "-----------------------------------------\n",
    "\n",
    "Task: Game of 24 (mathematical reasoning)\n",
    "Model: Qwen2.5-7B-Instruct via vLLM\n",
    "Problems: {N_EVAL_PROBLEMS} total ({len(archetypes)} archetypes + {N_EVAL_PROBLEMS - len(archetypes)} others)\n",
    "\n",
    "Archetype Selection:\n",
    "- Clustered {len(problems)} problems into {N_CLUSTERS} clusters\n",
    "- Selected top-10 archetypes by: centrality * diversity * simplicity\n",
    "- Verified strategies with multi-path consistency, perturbation testing, adversarial probing\n",
    "\n",
    "Results:\n",
    "- Archetype-first accuracy: {archetype_log.final_accuracy:.2%}\n",
    "- Random baseline accuracy: {random_log.final_accuracy:.2%}\n",
    "- Improvement: {(archetype_log.final_accuracy - random_log.final_accuracy)*100:+.1f} percentage points\n",
    "- Statistical significance: p = {wilcoxon_result['p_value']:.4f} {'(significant)' if wilcoxon_result['significant'] else '(not significant)'}\n",
    "\n",
    "Key Findings:\n",
    "1. Archetype-first curriculum {'outperforms' if arch_mean > rand_mean else 'underperforms compared to'} random baseline\n",
    "2. Verified strategies from archetypes provide {'reliable' if arch_mean > 0.5 else 'limited'} transfer\n",
    "3. LinUCB contextual bandit {'effectively' if np.mean(archetype_log.ucb_gaps) > 0.1 else 'minimally'} balances exploration/exploitation\n",
    "\n",
    "LLM Budget:\n",
    "- Total calls: ~{sum(archetype_log.call_counts.values()) + sum(random_log.call_counts.values())}\n",
    "- Verification calls: ~{call_counter.get('verify', 0) + call_counter.get('perturb', 0) + call_counter.get('adversarial', 0)}\n",
    "\"\"\")\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    \"archetype_log\": {\n",
    "        \"correct\": archetype_log.correct,\n",
    "        \"final_accuracy\": archetype_log.final_accuracy,\n",
    "        \"cumulative_accuracy\": archetype_log.cumulative_accuracy,\n",
    "    },\n",
    "    \"random_log\": {\n",
    "        \"correct\": random_log.correct,\n",
    "        \"final_accuracy\": random_log.final_accuracy,\n",
    "        \"cumulative_accuracy\": random_log.cumulative_accuracy,\n",
    "    },\n",
    "    \"statistics\": {\n",
    "        \"arch_ci\": (arch_mean, arch_lower, arch_upper),\n",
    "        \"rand_ci\": (rand_mean, rand_lower, rand_upper),\n",
    "        \"wilcoxon\": wilcoxon_result,\n",
    "        \"effect_size\": effect,\n",
    "    },\n",
    "    \"archetypes\": [a[\"numbers\"] for a in archetypes],\n",
    "}\n",
    "\n",
    "with open(CHECKPOINT_DIR / \"final_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "print(f\"\\nResults saved to {CHECKPOINT_DIR / 'final_results.pkl'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cleanup ---\n",
    "\n",
    "print(\"Shutting down vLLM server...\")\n",
    "if vllm_proc:\n",
    "    vllm_proc.terminate()\n",
    "    try:\n",
    "        vllm_proc.wait(timeout=10)\n",
    "    except subprocess.TimeoutExpired:\n",
    "        vllm_proc.kill()\n",
    "    print(f\"vLLM server (PID {vllm_proc.pid}) terminated.\")\n",
    "\n",
    "print(\"\\nNotebook complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
