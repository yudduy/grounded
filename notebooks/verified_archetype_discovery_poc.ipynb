{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8XCqsSn4SXO"
      },
      "source": [
        "# Verified Archetype Discovery PoC\n",
        "\n",
        "**Hypothesis**: Identifying \"archetype\" problems first improves learning compared to random curriculum. The key innovation is rigorous verification of strategies before trusting them.\n",
        "\n",
        "**Game of 24**: Given 4 numbers, use +, -, *, / to make 24 (each number used exactly once).\n",
        "\n",
        "**Approach**:\n",
        "1. Embed problems and cluster to find representative archetypes\n",
        "2. Rigorously verify strategies extracted from archetypes\n",
        "3. Bootstrap playbook from verified strategies\n",
        "4. Use LinUCB contextual bandit for curriculum selection\n",
        "5. Compare archetype-first vs random curriculum\n",
        "\n",
        "**Setup**: Qwen2.5-7B-Instruct via vLLM on A100 (bfloat16, prefix caching, async parallel eval)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sepafYUm4SXP"
      },
      "source": [
        "## 1. Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qz98iHLT9Va8",
        "outputId": "821879a6-2974-4801-8b0b-9c08343a107b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"numpy<2.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hRO-bvh4SXP",
        "outputId": "6d452f96-4b1c-48e9-90a8-7f63f44464d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vllm==0.6.6\n",
            "  Using cached vllm-0.6.6-cp38-abi3-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "Collecting openai==1.58.1\n",
            "  Using cached openai-1.58.1-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting datasets==3.2.0\n",
            "  Using cached datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting matplotlib==3.9.3\n",
            "  Using cached matplotlib-3.9.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: nest_asyncio==1.6.0 in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.2)\n",
            "Collecting scikit-learn==1.4.0\n",
            "  Using cached scikit_learn-1.4.0-1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting scipy==1.12.0\n",
            "  Using cached scipy-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (0.2.1)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (4.67.1)\n",
            "Collecting blake3 (from vllm==0.6.6)\n",
            "  Using cached blake3-1.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.45.2 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (5.0.0)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (0.22.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (5.29.5)\n",
            "Requirement already satisfied: fastapi!=0.113.*,!=0.114.0,>=0.107.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (0.123.10)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (3.13.3)\n",
            "Requirement already satisfied: uvicorn[standard] in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (0.40.0)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (2.12.3)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (0.24.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (11.3.0)\n",
            "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm==0.6.6)\n",
            "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (0.12.0)\n",
            "Collecting lm-format-enforcer<0.11,>=0.10.9 (from vllm==0.6.6)\n",
            "  Downloading lm_format_enforcer-0.10.12-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting outlines==0.1.11 (from vllm==0.6.6)\n",
            "  Downloading outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting lark==1.2.2 (from vllm==0.6.6)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xgrammar>=0.1.6 (from vllm==0.6.6)\n",
            "  Downloading xgrammar-0.1.31-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (4.15.0)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (3.20.3)\n",
            "Collecting partial-json-parser (from vllm==0.6.6)\n",
            "  Downloading partial_json_parser-0.2.1.1.post7-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (26.2.1)\n",
            "Collecting msgspec (from vllm==0.6.6)\n",
            "  Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting gguf==0.10.0 (from vllm==0.6.6)\n",
            "  Downloading gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (8.7.1)\n",
            "Collecting mistral_common>=1.5.0 (from mistral_common[opencv]>=1.5.0->vllm==0.6.6)\n",
            "  Downloading mistral_common-1.9.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (6.0.3)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (1.17.0)\n",
            "Requirement already satisfied: setuptools>=74.1.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (75.2.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (0.8.2)\n",
            "Collecting compressed-tensors==0.8.1 (from vllm==0.6.6)\n",
            "  Downloading compressed_tensors-0.8.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting depyf==0.18.0 (from vllm==0.6.6)\n",
            "  Downloading depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.6) (3.1.2)\n",
            "Collecting ray>=2.9 (from ray[default]>=2.9->vllm==0.6.6)\n",
            "  Downloading ray-2.53.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting nvidia-ml-py>=12.560.30 (from vllm==0.6.6)\n",
            "  Downloading nvidia_ml_py-13.590.48-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting torch==2.5.1 (from vllm==0.6.6)\n",
            "  Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision==0.20.1 (from vllm==0.6.6)\n",
            "  Downloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting xformers==0.0.28.post3 (from vllm==0.6.6)\n",
            "  Downloading xformers-0.0.28.post3-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.58.1) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.58.1) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.58.1) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.58.1) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai==1.58.1) (1.3.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.2.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.2.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==3.2.0) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==3.2.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==3.2.0) (0.70.16)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets==3.2.0)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.2.0) (1.3.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==3.2.0) (25.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.9.3) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.9.3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.9.3) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.9.3) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.9.3) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.9.3) (2.9.0.post0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.4.0) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.4.0) (3.6.0)\n",
            "Collecting astor (from depyf==0.18.0->vllm==0.6.6)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting interegular (from outlines==0.1.11->vllm==0.6.6)\n",
            "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from outlines==0.1.11->vllm==0.6.6) (3.1.6)\n",
            "Collecting diskcache (from outlines==0.1.11->vllm==0.6.6)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.12/dist-packages (from outlines==0.1.11->vllm==0.6.6) (0.37.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from outlines==0.1.11->vllm==0.6.6) (4.26.0)\n",
            "Collecting pycountry (from outlines==0.1.11->vllm==0.6.6)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting airportsdata (from outlines==0.1.11->vllm==0.6.6)\n",
            "  Downloading airportsdata-20260205-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting outlines_core==0.1.26 (from outlines==0.1.11->vllm==0.6.6)\n",
            "  Downloading outlines_core-0.1.26-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1->vllm==0.6.6) (3.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->vllm==0.6.6)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->vllm==0.6.6)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->vllm==0.6.6)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->vllm==0.6.6)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->vllm==0.6.6)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->vllm==0.6.6)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->vllm==0.6.6)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->vllm==0.6.6)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->vllm==0.6.6)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1->vllm==0.6.6)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.1->vllm==0.6.6)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->vllm==0.6.6)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch==2.5.1->vllm==0.6.6)\n",
            "  Downloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting sympy==1.13.1 (from torch==2.5.1->vllm==0.6.6)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch==2.5.1->vllm==0.6.6) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai==1.58.1) (3.11)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm==0.6.6) (0.50.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm==0.6.6) (0.0.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.6.6) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.6.6) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.6.6) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.6.6) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.6.6) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.6.6) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.6.6) (1.22.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==1.58.1) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==1.58.1) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.58.1) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.23.0->datasets==3.2.0) (1.2.0)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.23.0->datasets==3.2.0) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.23.0->datasets==3.2.0) (0.21.1)\n",
            "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.5.0->mistral_common[opencv]>=1.5.0->vllm==0.6.6)\n",
            "  Downloading pydantic_extra_types-2.11.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: opencv-python-headless>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common[opencv]>=1.5.0->vllm==0.6.6) (4.13.0.90)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->vllm==0.6.6) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->vllm==0.6.6) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->vllm==0.6.6) (0.4.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.9->ray[default]>=2.9->vllm==0.6.6) (8.3.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.9->ray[default]>=2.9->vllm==0.6.6) (1.1.2)\n",
            "Collecting aiohttp_cors (from ray[default]>=2.9->vllm==0.6.6)\n",
            "  Downloading aiohttp_cors-0.8.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting colorful (from ray[default]>=2.9->vllm==0.6.6)\n",
            "  Downloading colorful-0.5.8-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting py-spy>=0.4.0 (from ray[default]>=2.9->vllm==0.6.6)\n",
            "  Downloading py_spy-0.4.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (510 bytes)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.12/dist-packages (from ray[default]>=2.9->vllm==0.6.6) (1.76.0)\n",
            "Collecting opencensus (from ray[default]>=2.9->vllm==0.6.6)\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.30.0 in /usr/local/lib/python3.12/dist-packages (from ray[default]>=2.9->vllm==0.6.6) (1.37.0)\n",
            "Collecting opentelemetry-exporter-prometheus (from ray[default]>=2.9->vllm==0.6.6)\n",
            "  Downloading opentelemetry_exporter_prometheus-0.60b1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: opentelemetry-proto in /usr/local/lib/python3.12/dist-packages (from ray[default]>=2.9->vllm==0.6.6) (1.37.0)\n",
            "Requirement already satisfied: smart_open in /usr/local/lib/python3.12/dist-packages (from ray[default]>=2.9->vllm==0.6.6) (7.5.0)\n",
            "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[default]>=2.9->vllm==0.6.6)\n",
            "  Downloading virtualenv-20.36.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.6.6) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.6.6) (2.5.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.6.0->vllm==0.6.6) (2025.11.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.2->vllm==0.6.6) (0.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->vllm==0.6.6) (3.23.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.2.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.2.0) (2025.3)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]->vllm==0.6.6) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]->vllm==0.6.6) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]->vllm==0.6.6) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]->vllm==0.6.6) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]->vllm==0.6.6) (15.0.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->outlines==0.1.11->vllm==0.6.6) (2025.9.1)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema->outlines==0.1.11->vllm==0.6.6) (0.30.0)\n",
            "INFO: pip is looking at multiple versions of opencv-python-headless to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-python-headless>=4.0.0 (from mistral_common[opencv]>=1.5.0->vllm==0.6.6)\n",
            "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
            "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opentelemetry-api==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.30.0->ray[default]>=2.9->vllm==0.6.6) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.30.0->ray[default]>=2.9->vllm==0.6.6) (0.58b0)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default]>=2.9->vllm==0.6.6)\n",
            "  Downloading distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.12/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default]>=2.9->vllm==0.6.6) (4.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->outlines==0.1.11->vllm==0.6.6) (3.0.3)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default]>=2.9->vllm==0.6.6)\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from opencensus->ray[default]>=2.9->vllm==0.6.6) (2.29.0)\n",
            "Collecting opentelemetry-sdk>=1.30.0 (from ray[default]>=2.9->vllm==0.6.6)\n",
            "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "INFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opentelemetry-exporter-prometheus (from ray[default]>=2.9->vllm==0.6.6)\n",
            "  Downloading opentelemetry_exporter_prometheus-0.60b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-sdk>=1.30.0 (from ray[default]>=2.9->vllm==0.6.6)\n",
            "  Downloading opentelemetry_sdk-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-prometheus (from ray[default]>=2.9->vllm==0.6.6)\n",
            "  Downloading opentelemetry_exporter_prometheus-0.59b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-sdk>=1.30.0 (from ray[default]>=2.9->vllm==0.6.6)\n",
            "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-prometheus (from ray[default]>=2.9->vllm==0.6.6)\n",
            "  Downloading opentelemetry_exporter_prometheus-0.58b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open->ray[default]>=2.9->vllm==0.6.6) (2.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm==0.6.6) (1.72.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm==0.6.6) (1.27.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm==0.6.6) (2.47.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm==0.6.6) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm==0.6.6) (4.9.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm==0.6.6) (0.6.2)\n",
            "Downloading vllm-0.6.6-cp38-abi3-manylinux1_x86_64.whl (201.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.1/201.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.58.1-py3-none-any.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.9.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.4.0-1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading compressed_tensors-0.8.1-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading depyf-0.18.0-py3-none-any.whl (38 kB)\n",
            "Downloading gguf-0.10.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outlines-0.1.11-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m875.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.28.post3-cp312-cp312-manylinux_2_28_x86_64.whl (16.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outlines_core-0.1.26-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.2/343.2 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_format_enforcer-0.10.12-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistral_common-1.9.0-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_ml_py-13.590.48-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading ray-2.53.0-cp312-cp312-manylinux2014_x86_64.whl (72.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xgrammar-0.1.31-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (33.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.2/33.2 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blake3-1.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (388 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.0/388.0 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post7-py3-none-any.whl (10 kB)\n",
            "Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
            "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py_spy-0.4.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_extra_types-2.11.0-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading virtualenv-20.36.1-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_cors-0.8.1-py3-none-any.whl (25 kB)\n",
            "Downloading airportsdata-20260205-py3-none-any.whl (914 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m914.0/914.0 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading colorful-0.5.8-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_prometheus-0.58b0-py3-none-any.whl (13 kB)\n",
            "Downloading distlib-0.4.0-py2.py3-none-any.whl (469 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Installing collected packages: py-spy, opencensus-context, nvidia-ml-py, distlib, colorful, virtualenv, triton, sympy, pycountry, partial-json-parser, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, msgspec, lark, interegular, fsspec, diskcache, blake3, astor, airportsdata, scipy, opencv-python-headless, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gguf, depyf, scikit-learn, pydantic-extra-types, prometheus-fastapi-instrumentator, openai, nvidia-cusolver-cu12, matplotlib, lm-format-enforcer, aiohttp_cors, torch, ray, outlines_core, opencensus, datasets, xformers, torchvision, outlines, opentelemetry-exporter-prometheus, mistral_common, xgrammar, compressed-tensors, vllm\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.14.0\n",
            "    Uninstalling sympy-1.14.0:\n",
            "      Successfully uninstalled sympy-1.14.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.29.2\n",
            "    Uninstalling nvidia-nccl-cu12-2.29.2:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.29.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: lark\n",
            "    Found existing installation: lark 1.3.1\n",
            "    Uninstalling lark-1.3.1:\n",
            "      Successfully uninstalled lark-1.3.1\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.3\n",
            "    Uninstalling scipy-1.16.3:\n",
            "      Successfully uninstalled scipy-1.16.3\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.13.0.90\n",
            "    Uninstalling opencv-python-headless-4.13.0.90:\n",
            "      Successfully uninstalled opencv-python-headless-4.13.0.90\n"
          ]
        }
      ],
      "source": [
        "!pip install vllm==0.6.6 openai==1.58.1 datasets==3.2.0 matplotlib==3.9.3 numpy==1.26.4 nest_asyncio==1.6.0 sentence-transformers scikit-learn==1.4.0 scipy==1.12.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mqzp46F34SXP"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "import signal\n",
        "import json\n",
        "import re\n",
        "import copy\n",
        "import math\n",
        "import random\n",
        "import pickle\n",
        "import asyncio\n",
        "import itertools\n",
        "import nest_asyncio\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Tuple, Optional, Set, Any\n",
        "from collections import defaultdict, Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from scipy import stats\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from openai import OpenAI, AsyncOpenAI\n",
        "\n",
        "# Allow nested event loops (required for Colab/Jupyter)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Checkpoint directory\n",
        "CHECKPOINT_DIR = Path(\"checkpoints_archetype\")\n",
        "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Concurrency for async LLM calls\n",
        "MAX_CONCURRENT_LLM = 128\n",
        "\n",
        "print(f\"Setup complete. Seed={SEED}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAkRsvot4SXP"
      },
      "outputs": [],
      "source": [
        "# Launch vLLM server in background\n",
        "VLLM_PORT = 8000\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "print(\"Launching vLLM server...\")\n",
        "vllm_log = open('/tmp/vllm_server.log', 'w')\n",
        "\n",
        "vllm_proc = subprocess.Popen(\n",
        "    [\n",
        "        \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
        "        \"--model\", MODEL_NAME,\n",
        "        \"--port\", str(VLLM_PORT),\n",
        "        \"--max-model-len\", \"8192\",\n",
        "        \"--gpu-memory-utilization\", \"0.95\",\n",
        "        \"--dtype\", \"bfloat16\",\n",
        "        \"--max-num-seqs\", \"1024\",\n",
        "        \"--max-num-batched-tokens\", \"16384\",\n",
        "        \"--enable-prefix-caching\",\n",
        "        \"--disable-log-requests\",\n",
        "    ],\n",
        "    stdout=vllm_log,\n",
        "    stderr=subprocess.STDOUT,\n",
        ")\n",
        "\n",
        "# Wait for server to be ready\n",
        "client = OpenAI(base_url=f\"http://localhost:{VLLM_PORT}/v1\", api_key=\"dummy\")\n",
        "aclient = AsyncOpenAI(base_url=f\"http://localhost:{VLLM_PORT}/v1\", api_key=\"dummy\")\n",
        "print(\"Waiting for vLLM server to be ready...\")\n",
        "\n",
        "for attempt in range(600):\n",
        "    try:\n",
        "        client.models.list()\n",
        "        print(f\"vLLM ready after {attempt + 1}s\")\n",
        "        break\n",
        "    except Exception:\n",
        "        time.sleep(1)\n",
        "else:\n",
        "    raise RuntimeError(\"vLLM server failed to start within 180s\")\n",
        "\n",
        "# Warmup: trigger CUDA graph compilation for various batch sizes\n",
        "# CUDA best practice: Pre-compile graphs for expected batch sizes to avoid\n",
        "# JIT compilation overhead during actual inference\n",
        "print(\"Warming up vLLM (CUDA graph compilation)...\")\n",
        "\n",
        "async def warmup_vllm():\n",
        "    \"\"\"Warmup with varied batch sizes to pre-compile CUDA graphs.\"\"\"\n",
        "    warmup_prompt = \"What is 2+2? Answer briefly.\"\n",
        "\n",
        "    for batch_size in [1, 4, 16, 32]:\n",
        "        print(f\"  Warming up batch size {batch_size}...\")\n",
        "        tasks = [\n",
        "            aclient.chat.completions.create(\n",
        "                model=MODEL_NAME,\n",
        "                messages=[{\"role\": \"user\", \"content\": warmup_prompt}],\n",
        "                max_tokens=16,\n",
        "                temperature=0.0,\n",
        "            )\n",
        "            for _ in range(batch_size)\n",
        "        ]\n",
        "        try:\n",
        "            await asyncio.gather(*tasks)\n",
        "        except Exception as e:\n",
        "            print(f\"  Warmup batch {batch_size} failed (non-fatal): {e}\")\n",
        "\n",
        "    print(\"Warmup complete.\")\n",
        "\n",
        "asyncio.run(warmup_vllm())\n",
        "\n",
        "print(f\"vLLM server running on port {VLLM_PORT} (PID: {vllm_proc.pid})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RIlMaW74SXQ"
      },
      "source": [
        "## 2. Game of 24 Data Loading & Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TC9moaLH4SXQ"
      },
      "outputs": [],
      "source": [
        "# --- Game of 24 Problem Generation & Validation ---\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "\n",
        "def generate_all_game24_problems(max_num: int = 13) -> List[Tuple[int, int, int, int]]:\n",
        "    \"\"\"Generate all unique Game of 24 problems with numbers 1-max_num.\"\"\"\n",
        "    problems = set()\n",
        "    for a in range(1, max_num + 1):\n",
        "        for b in range(a, max_num + 1):\n",
        "            for c in range(b, max_num + 1):\n",
        "                for d in range(c, max_num + 1):\n",
        "                    problems.add((a, b, c, d))\n",
        "    return list(problems)\n",
        "\n",
        "\n",
        "def solve_24_exhaustive(numbers: Tuple[int, ...], target: float = 24.0) -> List[str]:\n",
        "    \"\"\"\n",
        "    Find all distinct solutions to Game of 24 using exhaustive search.\n",
        "    Returns list of expression strings that evaluate to target.\n",
        "    \"\"\"\n",
        "    if len(numbers) == 1:\n",
        "        if abs(numbers[0] - target) < 1e-9:\n",
        "            return [str(int(numbers[0])) if numbers[0] == int(numbers[0]) else str(numbers[0])]\n",
        "        return []\n",
        "\n",
        "    solutions = set()\n",
        "    ops = [('+', lambda a, b: a + b),\n",
        "           ('-', lambda a, b: a - b),\n",
        "           ('*', lambda a, b: a * b),\n",
        "           ('/', lambda a, b: a / b if b != 0 else float('inf'))]\n",
        "\n",
        "    # Try all pairs of numbers\n",
        "    for i in range(len(numbers)):\n",
        "        for j in range(len(numbers)):\n",
        "            if i == j:\n",
        "                continue\n",
        "            a, b = numbers[i], numbers[j]\n",
        "            remaining = tuple(numbers[k] for k in range(len(numbers)) if k != i and k != j)\n",
        "\n",
        "            for op_str, op_func in ops:\n",
        "                try:\n",
        "                    result = op_func(a, b)\n",
        "                    if result == float('inf') or result == float('-inf'):\n",
        "                        continue\n",
        "\n",
        "                    # Format expression part\n",
        "                    a_str = str(int(a)) if a == int(a) else str(a)\n",
        "                    b_str = str(int(b)) if b == int(b) else str(b)\n",
        "                    expr_part = f\"({a_str} {op_str} {b_str})\"\n",
        "\n",
        "                    # Recurse with new number list\n",
        "                    new_numbers = remaining + (result,)\n",
        "                    sub_solutions = solve_24_exhaustive(new_numbers, target)\n",
        "\n",
        "                    for sub_sol in sub_solutions:\n",
        "                        # Replace the result placeholder with the expression\n",
        "                        result_str = str(int(result)) if result == int(result) else str(result)\n",
        "                        if result_str in sub_sol:\n",
        "                            full_expr = sub_sol.replace(result_str, expr_part, 1)\n",
        "                            solutions.add(full_expr)\n",
        "                        else:\n",
        "                            solutions.add(f\"{expr_part} -> {sub_sol}\")\n",
        "                except (ZeroDivisionError, OverflowError):\n",
        "                    continue\n",
        "\n",
        "    return list(solutions)\n",
        "\n",
        "\n",
        "def is_solvable_24(numbers: Tuple[int, ...]) -> bool:\n",
        "    \"\"\"Check if a Game of 24 problem is solvable.\"\"\"\n",
        "    return len(solve_24_exhaustive(numbers)) > 0\n",
        "\n",
        "\n",
        "def count_solutions_24(numbers: Tuple[int, ...]) -> int:\n",
        "    \"\"\"Count the number of distinct solutions.\"\"\"\n",
        "    return len(solve_24_exhaustive(numbers))\n",
        "\n",
        "\n",
        "def _check_problem(nums: Tuple[int, ...]) -> Optional[Dict]:\n",
        "    \"\"\"Worker function for parallel solvability check.\"\"\"\n",
        "    solutions = solve_24_exhaustive(nums)\n",
        "    if solutions:\n",
        "        return {\n",
        "            \"numbers\": nums,\n",
        "            \"text\": f\"Use {nums[0]}, {nums[1]}, {nums[2]}, {nums[3]} to make 24\",\n",
        "            \"n_solutions\": len(solutions),\n",
        "        }\n",
        "    return None\n",
        "\n",
        "\n",
        "# Generate solvable problems\n",
        "print(\"Generating Game of 24 problems...\")\n",
        "all_problems = generate_all_game24_problems(max_num=13)\n",
        "print(f\"Total candidate problems: {len(all_problems)}\")\n",
        "\n",
        "# Filter to solvable problems using parallel processing\n",
        "# CUDA best practice: Parallelize sequential code (CPU-side)\n",
        "print(\"Filtering to solvable problems (parallel)...\")\n",
        "solvable_problems = []\n",
        "\n",
        "# Use ProcessPoolExecutor for CPU-bound exhaustive search\n",
        "N_WORKERS = min(8, os.cpu_count() or 4)\n",
        "print(f\"  Using {N_WORKERS} worker processes...\")\n",
        "\n",
        "with ProcessPoolExecutor(max_workers=N_WORKERS) as executor:\n",
        "    futures = {executor.submit(_check_problem, nums): nums for nums in all_problems}\n",
        "    done_count = 0\n",
        "    for future in as_completed(futures):\n",
        "        done_count += 1\n",
        "        result = future.result()\n",
        "        if result is not None:\n",
        "            solvable_problems.append(result)\n",
        "        if done_count % 200 == 0:\n",
        "            print(f\"  Processed {done_count}/{len(all_problems)}...\")\n",
        "\n",
        "print(f\"Solvable problems: {len(solvable_problems)}\")\n",
        "\n",
        "# Shuffle and take subset for experiment\n",
        "rng = random.Random(SEED)\n",
        "rng.shuffle(solvable_problems)\n",
        "problems = solvable_problems[:200]  # Use 200 for clustering, 100 for eval\n",
        "\n",
        "print(f\"\\nUsing {len(problems)} problems for experiment\")\n",
        "print(f\"Example: {problems[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCq1qZSW4SXQ"
      },
      "outputs": [],
      "source": [
        "# --- Known Hard Game of 24 Problems (for adversarial testing) ---\n",
        "# These require non-obvious operations like fractions or specific orderings\n",
        "\n",
        "HARD_PROBLEMS = [\n",
        "    # Requires fractions\n",
        "    {\"numbers\": (1, 5, 5, 5), \"text\": \"Use 1, 5, 5, 5 to make 24\", \"solution\": \"5 * (5 - 1/5) = 24\", \"difficulty\": \"fraction\"},\n",
        "    {\"numbers\": (3, 3, 8, 8), \"text\": \"Use 3, 3, 8, 8 to make 24\", \"solution\": \"8 / (3 - 8/3) = 24\", \"difficulty\": \"fraction\"},\n",
        "    {\"numbers\": (1, 3, 4, 6), \"text\": \"Use 1, 3, 4, 6 to make 24\", \"solution\": \"6 / (1 - 3/4) = 24\", \"difficulty\": \"fraction\"},\n",
        "    {\"numbers\": (1, 4, 5, 6), \"text\": \"Use 1, 4, 5, 6 to make 24\", \"solution\": \"4 / (1 - 5/6) = 24\", \"difficulty\": \"fraction\"},\n",
        "    {\"numbers\": (1, 6, 6, 8), \"text\": \"Use 1, 6, 6, 8 to make 24\", \"solution\": \"8 / (1 - 6/6) - nope, (6 - 1) * 6 - 8 = 22 - no, 8 * 6 / (6 - 1) = 48/5\", \"difficulty\": \"tricky\"},\n",
        "    # Requires specific ordering\n",
        "    {\"numbers\": (2, 3, 5, 12), \"text\": \"Use 2, 3, 5, 12 to make 24\", \"solution\": \"(5 - 3 + 2) * 12 / 2 = 24 - check\", \"difficulty\": \"ordering\"},\n",
        "    {\"numbers\": (1, 2, 7, 7), \"text\": \"Use 1, 2, 7, 7 to make 24\", \"solution\": \"(7 + 1) * (7 - 2) / ... = (7 - 1) * (7 - 2) = 30 - no, 7 * 7 / 2 - 1 = 23.5\", \"difficulty\": \"tricky\"},\n",
        "    {\"numbers\": (4, 4, 7, 7), \"text\": \"Use 4, 4, 7, 7 to make 24\", \"solution\": \"(7 - 4) * (7 + 4/4) = 3 * 8 = 24? - no 4/4=1, 7+1=8, 7-4=3, 3*8=24!\", \"difficulty\": \"tricky\"},\n",
        "    {\"numbers\": (3, 3, 7, 7), \"text\": \"Use 3, 3, 7, 7 to make 24\", \"solution\": \"(7 + 7) * (3 - 3) = 0 - no, (3 + 3/7) * 7 = 24\", \"difficulty\": \"fraction\"},\n",
        "    {\"numbers\": (2, 5, 5, 10), \"text\": \"Use 2, 5, 5, 10 to make 24\", \"solution\": \"(5 - 5/10) * 2 = 9 - no, 10 * 5 / 2 - 5 = 20\", \"difficulty\": \"tricky\"},\n",
        "]\n",
        "\n",
        "# Verify and correct hard problems\n",
        "verified_hard = []\n",
        "for hp in HARD_PROBLEMS:\n",
        "    nums = hp[\"numbers\"]\n",
        "    solutions = solve_24_exhaustive(nums)\n",
        "    if solutions:\n",
        "        hp[\"verified_solutions\"] = solutions[:3]  # Keep up to 3\n",
        "        hp[\"n_solutions\"] = len(solutions)\n",
        "        verified_hard.append(hp)\n",
        "        print(f\"{nums}: {len(solutions)} solutions - {solutions[0][:50]}...\")\n",
        "    else:\n",
        "        print(f\"{nums}: NO SOLUTION (removing from hard set)\")\n",
        "\n",
        "HARD_PROBLEMS = verified_hard\n",
        "print(f\"\\nVerified {len(HARD_PROBLEMS)} hard problems\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwH_vrXw4SXQ"
      },
      "source": [
        "## 3. Problem Embedding & Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_heEjJL4SXQ"
      },
      "outputs": [],
      "source": [
        "# Load sentence transformer for embeddings\n",
        "# GPU optimization: Use CUDA with bfloat16 for A100 (matches vLLM dtype)\n",
        "print(\"Loading sentence transformer model (GPU, bfloat16)...\")\n",
        "embed_model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda', model_kwargs={\"torch_dtype\": \"bfloat16\"})\n",
        "print(f\"Model loaded on {embed_model.device}\")\n",
        "\n",
        "\n",
        "def compute_numerical_features(nums: Tuple[int, ...]) -> np.ndarray:\n",
        "    \"\"\"Compute numerical features for a Game of 24 problem.\"\"\"\n",
        "    return np.array([\n",
        "        sum(nums) / 52,  # sum normalized by max possible (13*4)\n",
        "        np.prod(nums) / (13**4),  # product normalized\n",
        "        max(nums) / 13,  # max normalized\n",
        "        min(nums) / 13,  # min normalized\n",
        "        len(set(nums)) / 4,  # uniqueness ratio\n",
        "        (max(nums) - min(nums)) / 12,  # range normalized\n",
        "        np.std(nums) / 5,  # std normalized\n",
        "        sum(1 for n in nums if n % 2 == 0) / 4,  # even ratio\n",
        "        sum(1 for n in nums if 24 % n == 0) / 4,  # divisor of 24 ratio\n",
        "        1.0 if any(a * b == 24 for a, b in itertools.combinations(nums, 2)) else 0.0,  # has factor pair\n",
        "    ]) * 2.0  # Scale up numerical importance\n",
        "\n",
        "\n",
        "def compute_problem_features_batch(problems_list: List[Dict], batch_size: int = 128) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute feature vectors for all problems using batched encoding.\n",
        "\n",
        "    CUDA optimization: Batched GPU inference maximizes device utilization\n",
        "    and minimizes kernel launch overhead. Increased batch_size for A100.\n",
        "    \"\"\"\n",
        "    # Batch encode all texts at once (GPU-efficient)\n",
        "    texts = [p[\"text\"] for p in problems_list]\n",
        "    print(f\"  Batch encoding {len(texts)} texts...\")\n",
        "    text_embeddings = embed_model.encode(\n",
        "        texts,\n",
        "        batch_size=batch_size,\n",
        "        convert_to_numpy=True,\n",
        "        show_progress_bar=True,\n",
        "    )\n",
        "\n",
        "    # Compute numerical features (CPU, vectorized where possible)\n",
        "    print(\"  Computing numerical features...\")\n",
        "    num_features = np.array([\n",
        "        compute_numerical_features(p[\"numbers\"]) for p in problems_list\n",
        "    ])\n",
        "\n",
        "    # Concatenate: [text_embedding, scaled_num_features]\n",
        "    combined = np.concatenate([text_embeddings, num_features], axis=1)\n",
        "    return combined\n",
        "\n",
        "\n",
        "# Compute embeddings for all problems (batched for GPU efficiency)\n",
        "print(\"Computing problem embeddings (batched)...\")\n",
        "embeddings = compute_problem_features_batch(problems, batch_size=128)\n",
        "\n",
        "# Assign embeddings back to problems\n",
        "for i, prob in enumerate(problems):\n",
        "    prob[\"embedding\"] = embeddings[i]\n",
        "\n",
        "print(f\"Embeddings shape: {embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DLB7obr4SXR"
      },
      "outputs": [],
      "source": [
        "# K-means clustering\n",
        "N_CLUSTERS = 10\n",
        "\n",
        "print(f\"Clustering into {N_CLUSTERS} clusters...\")\n",
        "kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=SEED, n_init=10)\n",
        "cluster_labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "# Assign cluster info to problems\n",
        "for i, prob in enumerate(problems):\n",
        "    prob[\"cluster\"] = cluster_labels[i]\n",
        "    # Distance to cluster center (for centrality)\n",
        "    center = kmeans.cluster_centers_[cluster_labels[i]]\n",
        "    prob[\"dist_to_center\"] = np.linalg.norm(prob[\"embedding\"] - center)\n",
        "\n",
        "# Cluster statistics\n",
        "cluster_sizes = Counter(cluster_labels)\n",
        "print(f\"\\nCluster sizes: {dict(cluster_sizes)}\")\n",
        "\n",
        "# Show example from each cluster\n",
        "print(\"\\nCluster examples:\")\n",
        "for c in range(N_CLUSTERS):\n",
        "    cluster_probs = [p for p in problems if p[\"cluster\"] == c]\n",
        "    if cluster_probs:\n",
        "        ex = min(cluster_probs, key=lambda p: p[\"dist_to_center\"])\n",
        "        print(f\"  Cluster {c}: {ex['numbers']} (size={len(cluster_probs)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YuBQe_w4SXR"
      },
      "source": [
        "## 4. Archetype Identification Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sTUwTMM4SXR"
      },
      "outputs": [],
      "source": [
        "# --- 4.1 Embedding Centrality ---\n",
        "\n",
        "def compute_centrality_score(problem: Dict, cluster_probs: List[Dict]) -> float:\n",
        "    \"\"\"\n",
        "    Centrality = negative distance to cluster center.\n",
        "    Higher is better (closer to center).\n",
        "    \"\"\"\n",
        "    # Normalize by max distance in cluster\n",
        "    max_dist = max(p[\"dist_to_center\"] for p in cluster_probs) + 1e-6\n",
        "    return 1.0 - (problem[\"dist_to_center\"] / max_dist)\n",
        "\n",
        "\n",
        "# Compute centrality for all problems\n",
        "for c in range(N_CLUSTERS):\n",
        "    cluster_probs = [p for p in problems if p[\"cluster\"] == c]\n",
        "    for prob in cluster_probs:\n",
        "        prob[\"centrality\"] = compute_centrality_score(prob, cluster_probs)\n",
        "\n",
        "print(\"Centrality scores computed.\")\n",
        "print(f\"Example: {problems[0]['numbers']} centrality = {problems[0]['centrality']:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7xAuavD4SXR"
      },
      "outputs": [],
      "source": [
        "# --- 4.2 Solution Diversity (via LLM) ---\n",
        "\n",
        "call_counter = defaultdict(int)\n",
        "_llm_semaphore = asyncio.Semaphore(MAX_CONCURRENT_LLM)\n",
        "\n",
        "# Retry configuration\n",
        "MAX_RETRIES = 3\n",
        "RETRY_DELAY = 1.0\n",
        "\n",
        "\n",
        "async def llm_call_async(\n",
        "    system: str,\n",
        "    user: str,\n",
        "    role: str = \"generate\",\n",
        "    temperature: float = 0.7,\n",
        "    max_tokens: int = 512,  # CUDA optimization: reduced from 512 to minimize decoding steps\n",
        "    retries: int = MAX_RETRIES,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Async LLM call with semaphore-based concurrency control and retry logic.\n",
        "\n",
        "    Args:\n",
        "        system: System prompt.\n",
        "        user: User prompt.\n",
        "        role: Role tag for call counting.\n",
        "        temperature: Sampling temperature.\n",
        "        max_tokens: Maximum tokens to generate.\n",
        "        retries: Number of retries on failure.\n",
        "\n",
        "    Returns:\n",
        "        Generated text, or empty string on failure.\n",
        "    \"\"\"\n",
        "    call_counter[role] += 1\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        async with _llm_semaphore:\n",
        "            try:\n",
        "                resp = await aclient.chat.completions.create(\n",
        "                    model=MODEL_NAME,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": system},\n",
        "                        {\"role\": \"user\", \"content\": user},\n",
        "                    ],\n",
        "                    temperature=temperature,\n",
        "                    max_tokens=max_tokens,\n",
        "                )\n",
        "                content = resp.choices[0].message.content\n",
        "                return content.strip() if content else \"\"\n",
        "            except Exception as e:\n",
        "                if attempt < retries - 1:\n",
        "                    await asyncio.sleep(RETRY_DELAY * (attempt + 1))\n",
        "                else:\n",
        "                    print(f\"Async LLM call failed after {retries} attempts ({role}): {e}\")\n",
        "                    return \"\"\n",
        "\n",
        "\n",
        "def llm_call(\n",
        "    system: str,\n",
        "    user: str,\n",
        "    role: str = \"generate\",\n",
        "    temperature: float = 0.7,\n",
        "    max_tokens: int = 512,  # CUDA optimization: reduced from 512\n",
        "    retries: int = MAX_RETRIES,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Sync LLM call with retry logic.\n",
        "\n",
        "    Args:\n",
        "        system: System prompt.\n",
        "        user: User prompt.\n",
        "        role: Role tag for call counting.\n",
        "        temperature: Sampling temperature.\n",
        "        max_tokens: Maximum tokens to generate.\n",
        "        retries: Number of retries on failure.\n",
        "\n",
        "    Returns:\n",
        "        Generated text, or empty string on failure.\n",
        "    \"\"\"\n",
        "    call_counter[role] += 1\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=MODEL_NAME,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system},\n",
        "                    {\"role\": \"user\", \"content\": user},\n",
        "                ],\n",
        "                temperature=temperature,\n",
        "                max_tokens=max_tokens,\n",
        "            )\n",
        "            content = resp.choices[0].message.content\n",
        "            return content.strip() if content else \"\"\n",
        "        except Exception as e:\n",
        "            if attempt < retries - 1:\n",
        "                time.sleep(RETRY_DELAY * (attempt + 1))\n",
        "            else:\n",
        "                print(f\"LLM call failed after {retries} attempts ({role}): {e}\")\n",
        "                return \"\"\n",
        "\n",
        "\n",
        "GAME24_SYSTEM = \"\"\"You are a Game of 24 solver. Given 4 numbers, find an expression using +, -, *, / that equals 24.\n",
        "Each number must be used exactly once. Show your work step by step.\n",
        "\n",
        "Format your final answer as: ANSWER: <expression> = 24\n",
        "\n",
        "Example:\n",
        "Numbers: 1, 2, 3, 4\n",
        "Solution: 1 * 2 * 3 * 4 = 24\n",
        "ANSWER: 1 * 2 * 3 * 4 = 24\"\"\"\n",
        "\n",
        "\n",
        "async def get_llm_solutions(problem: Dict, n_samples: int = 5) -> List[str]:\n",
        "    \"\"\"Get N diverse solutions from LLM for a problem.\"\"\"\n",
        "    nums = problem[\"numbers\"]\n",
        "    user_prompt = f\"Numbers: {nums[0]}, {nums[1]}, {nums[2]}, {nums[3]}\\n\\nFind an expression that equals 24.\"\n",
        "\n",
        "    # CUDA optimization: max_tokens=512 since Game of 24 answers are short (~50 tokens)\n",
        "    tasks = [\n",
        "        llm_call_async(GAME24_SYSTEM, user_prompt, role=\"diversity\", temperature=0.9, max_tokens=512)\n",
        "        for _ in range(n_samples)\n",
        "    ]\n",
        "    responses = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "    # Extract expressions\n",
        "    solutions = []\n",
        "    for resp in responses:\n",
        "        if isinstance(resp, Exception) or not resp:\n",
        "            continue\n",
        "        match = re.search(r\"ANSWER:\\s*(.+?)\\s*=\\s*24\", resp, re.IGNORECASE)\n",
        "        if match:\n",
        "            solutions.append(match.group(1).strip())\n",
        "        else:\n",
        "            # Fallback: look for any expression = 24\n",
        "            match2 = re.search(r\"([\\d\\s+\\-*/()]+)\\s*=\\s*24\", resp)\n",
        "            if match2:\n",
        "                solutions.append(match2.group(1).strip())\n",
        "\n",
        "    return solutions\n",
        "\n",
        "\n",
        "async def _compute_diversity_for_problem(prob: Dict) -> Dict:\n",
        "    \"\"\"Compute diversity for a single problem. Returns updated problem dict.\"\"\"\n",
        "    llm_solutions = await get_llm_solutions(prob, n_samples=5)\n",
        "    unique_solutions = set(llm_solutions)\n",
        "    return {\n",
        "        \"numbers\": prob[\"numbers\"],\n",
        "        \"llm_solutions\": llm_solutions,\n",
        "        \"diversity\": len(unique_solutions) / 5.0,\n",
        "    }\n",
        "\n",
        "\n",
        "# Compute diversity for candidate archetypes (top 3 per cluster by centrality)\n",
        "print(\"Computing solution diversity for archetype candidates...\")\n",
        "candidates = []\n",
        "for c in range(N_CLUSTERS):\n",
        "    cluster_probs = sorted(\n",
        "        [p for p in problems if p[\"cluster\"] == c],\n",
        "        key=lambda p: -p[\"centrality\"]\n",
        "    )[:3]\n",
        "    candidates.extend(cluster_probs)\n",
        "\n",
        "print(f\"Evaluating {len(candidates)} candidates...\")\n",
        "\n",
        "\n",
        "async def compute_all_diversity():\n",
        "    \"\"\"Compute diversity for all candidates IN PARALLEL.\"\"\"\n",
        "    # CUDA best practice: Launch all independent work concurrently\n",
        "    tasks = [_compute_diversity_for_problem(prob) for prob in candidates]\n",
        "    print(f\"  Launching {len(tasks)} parallel diversity evaluations...\")\n",
        "\n",
        "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "    # Assign results back to candidates\n",
        "    for prob, result in zip(candidates, results):\n",
        "        if isinstance(result, Exception):\n",
        "            print(f\"  Warning: diversity computation failed for {prob['numbers']}: {result}\")\n",
        "            prob[\"llm_solutions\"] = []\n",
        "            prob[\"diversity\"] = 0.0\n",
        "        else:\n",
        "            prob[\"llm_solutions\"] = result[\"llm_solutions\"]\n",
        "            prob[\"diversity\"] = result[\"diversity\"]\n",
        "\n",
        "\n",
        "asyncio.run(compute_all_diversity())\n",
        "print(f\"Diversity computed. LLM calls: {call_counter['diversity']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cQ9T-7I4SXR"
      },
      "outputs": [],
      "source": [
        "# --- 4.3 Structural Simplicity ---\n",
        "\n",
        "def compute_solution_depth(expr: str) -> int:\n",
        "    \"\"\"Estimate the depth of a solution expression tree.\"\"\"\n",
        "    # Count nested parentheses as proxy for depth\n",
        "    max_depth = 0\n",
        "    current_depth = 0\n",
        "    for char in expr:\n",
        "        if char == '(':\n",
        "            current_depth += 1\n",
        "            max_depth = max(max_depth, current_depth)\n",
        "        elif char == ')':\n",
        "            current_depth -= 1\n",
        "    # Also count operators as contributing to depth\n",
        "    n_ops = sum(1 for c in expr if c in '+-*/')\n",
        "    return max_depth + n_ops // 2\n",
        "\n",
        "\n",
        "def compute_simplicity_score(problem: Dict) -> float:\n",
        "    \"\"\"\n",
        "    Simplicity = inverse of minimum solution tree depth.\n",
        "    Simpler problems (fewer operations) are better archetypes.\n",
        "    \"\"\"\n",
        "    # Use ground truth solutions if available\n",
        "    solutions = solve_24_exhaustive(problem[\"numbers\"])\n",
        "    if not solutions:\n",
        "        return 0.0\n",
        "\n",
        "    min_depth = min(compute_solution_depth(s) for s in solutions[:10])  # Check up to 10\n",
        "    # Normalize: depth of 3 is simple (score=1), depth of 10 is complex (score~0.3)\n",
        "    return 1.0 / (1.0 + min_depth / 3.0)\n",
        "\n",
        "\n",
        "# Compute simplicity for candidates\n",
        "print(\"Computing simplicity scores...\")\n",
        "for prob in candidates:\n",
        "    prob[\"simplicity\"] = compute_simplicity_score(prob)\n",
        "\n",
        "print(f\"Simplicity computed.\")\n",
        "print(f\"Example: {candidates[0]['numbers']} simplicity = {candidates[0]['simplicity']:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNl13d7M4SXR"
      },
      "outputs": [],
      "source": [
        "# --- 4.4 Combined Ranking ---\n",
        "\n",
        "def compute_archetype_score(problem: Dict) -> float:\n",
        "    \"\"\"\n",
        "    Combined score = geometric mean of centrality * diversity * simplicity.\n",
        "    Geometric mean penalizes being weak in any dimension.\n",
        "    \"\"\"\n",
        "    centrality = problem.get(\"centrality\", 0.5)\n",
        "    diversity = problem.get(\"diversity\", 0.5)\n",
        "    simplicity = problem.get(\"simplicity\", 0.5)\n",
        "\n",
        "    # Add small epsilon to avoid zero\n",
        "    eps = 0.01\n",
        "    return (max(centrality, eps) * max(diversity, eps) * max(simplicity, eps)) ** (1/3)\n",
        "\n",
        "\n",
        "# Compute combined scores\n",
        "for prob in candidates:\n",
        "    prob[\"archetype_score\"] = compute_archetype_score(prob)\n",
        "\n",
        "# Select top-10 archetypes (ensuring cluster diversity)\n",
        "archetypes = []\n",
        "used_clusters = set()\n",
        "\n",
        "# First pass: one per cluster\n",
        "sorted_candidates = sorted(candidates, key=lambda p: -p[\"archetype_score\"])\n",
        "for prob in sorted_candidates:\n",
        "    if prob[\"cluster\"] not in used_clusters and len(archetypes) < 10:\n",
        "        archetypes.append(prob)\n",
        "        used_clusters.add(prob[\"cluster\"])\n",
        "\n",
        "# Second pass: fill remaining slots with best overall\n",
        "for prob in sorted_candidates:\n",
        "    if prob not in archetypes and len(archetypes) < 10:\n",
        "        archetypes.append(prob)\n",
        "\n",
        "print(f\"\\nSelected {len(archetypes)} archetypes:\")\n",
        "print(\"-\" * 80)\n",
        "for i, arch in enumerate(archetypes):\n",
        "    print(f\"{i+1}. {arch['numbers']} | cluster={arch['cluster']} | \"\n",
        "          f\"score={arch['archetype_score']:.3f} \"\n",
        "          f\"(cent={arch['centrality']:.2f}, div={arch['diversity']:.2f}, simp={arch['simplicity']:.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsMxfUCk4SXS"
      },
      "source": [
        "## 5. Verification Suite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLidEDVP4SXS"
      },
      "outputs": [],
      "source": [
        "# --- 5.1 & 5.2 Multi-path Consistency & Execution Validation ---\n",
        "\n",
        "# Allowed tokens for safe expression evaluation\n",
        "_SAFE_EXPR_PATTERN = re.compile(r'^[\\d\\s+\\-*/().]+$')\n",
        "\n",
        "\n",
        "def safe_eval_expression(expr: str, numbers: Tuple[int, ...]) -> Tuple[bool, float, str]:\n",
        "    \"\"\"\n",
        "    Safely evaluate a math expression and verify it:\n",
        "    1. Uses all 4 numbers exactly once\n",
        "    2. Evaluates to 24\n",
        "\n",
        "    Security: Uses AST parsing to ensure only arithmetic operations are performed.\n",
        "    No function calls, attribute access, or arbitrary code execution allowed.\n",
        "\n",
        "    Returns: (is_valid, result, error_msg)\n",
        "    \"\"\"\n",
        "    import ast\n",
        "    import operator\n",
        "\n",
        "    try:\n",
        "        # Clean expression\n",
        "        expr_clean = expr.replace('x', '*').replace('X', '*')\n",
        "        expr_clean = re.sub(r'\\s+', '', expr_clean)\n",
        "\n",
        "        # First check: only allowed characters\n",
        "        if not _SAFE_EXPR_PATTERN.match(expr_clean):\n",
        "            return False, 0.0, \"Invalid characters in expression\"\n",
        "\n",
        "        # Extract numbers from expression\n",
        "        expr_numbers = [int(n) for n in re.findall(r'\\d+', expr_clean)]\n",
        "\n",
        "        # Check if all numbers are used exactly once\n",
        "        if sorted(expr_numbers) != sorted(numbers):\n",
        "            return False, 0.0, f\"Numbers mismatch: expected {sorted(numbers)}, got {sorted(expr_numbers)}\"\n",
        "\n",
        "        # Safe AST-based evaluation\n",
        "        # Only allow: numbers, binary ops (+, -, *, /), unary minus, parentheses\n",
        "        ALLOWED_OPS = {\n",
        "            ast.Add: operator.add,\n",
        "            ast.Sub: operator.sub,\n",
        "            ast.Mult: operator.mul,\n",
        "            ast.Div: operator.truediv,\n",
        "            ast.USub: operator.neg,\n",
        "        }\n",
        "\n",
        "        def _eval_node(node):\n",
        "            if isinstance(node, ast.Expression):\n",
        "                return _eval_node(node.body)\n",
        "            elif isinstance(node, ast.Constant):\n",
        "                if isinstance(node.value, (int, float)):\n",
        "                    return float(node.value)\n",
        "                raise ValueError(f\"Unsupported constant type: {type(node.value)}\")\n",
        "            elif isinstance(node, ast.Num):  # Python 3.7 compatibility\n",
        "                return float(node.n)\n",
        "            elif isinstance(node, ast.BinOp):\n",
        "                op_type = type(node.op)\n",
        "                if op_type not in ALLOWED_OPS:\n",
        "                    raise ValueError(f\"Unsupported operator: {op_type.__name__}\")\n",
        "                left = _eval_node(node.left)\n",
        "                right = _eval_node(node.right)\n",
        "                if op_type == ast.Div and right == 0:\n",
        "                    raise ZeroDivisionError(\"Division by zero\")\n",
        "                return ALLOWED_OPS[op_type](left, right)\n",
        "            elif isinstance(node, ast.UnaryOp):\n",
        "                op_type = type(node.op)\n",
        "                if op_type not in ALLOWED_OPS:\n",
        "                    raise ValueError(f\"Unsupported unary operator: {op_type.__name__}\")\n",
        "                return ALLOWED_OPS[op_type](_eval_node(node.operand))\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported AST node: {type(node).__name__}\")\n",
        "\n",
        "        tree = ast.parse(expr_clean, mode='eval')\n",
        "        result = _eval_node(tree)\n",
        "\n",
        "        # Check if result is 24\n",
        "        if abs(result - 24) < 1e-6:\n",
        "            return True, result, \"\"\n",
        "        else:\n",
        "            return False, result, f\"Result is {result}, not 24\"\n",
        "\n",
        "    except ZeroDivisionError:\n",
        "        return False, 0.0, \"Division by zero\"\n",
        "    except (SyntaxError, ValueError) as e:\n",
        "        return False, 0.0, str(e)\n",
        "    except Exception as e:\n",
        "        return False, 0.0, f\"Evaluation error: {e}\"\n",
        "\n",
        "\n",
        "async def verify_multipath_consistency(problem: Dict, n_paths: int = 5) -> Dict:\n",
        "    \"\"\"\n",
        "    Generate N independent solutions and check consistency.\n",
        "    Returns verification results.\n",
        "    \"\"\"\n",
        "    nums = problem[\"numbers\"]\n",
        "    user_prompt = f\"Numbers: {nums[0]}, {nums[1]}, {nums[2]}, {nums[3]}\\n\\nFind an expression that equals 24.\"\n",
        "\n",
        "    # Generate N solutions with different temperatures\n",
        "    # CUDA optimization: max_tokens=512 for short Game of 24 answers\n",
        "    tasks = [\n",
        "        llm_call_async(GAME24_SYSTEM, user_prompt, role=\"verify\", temperature=0.8 + 0.1 * i, max_tokens=512)\n",
        "        for i in range(n_paths)\n",
        "    ]\n",
        "    responses = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "    valid_count = 0\n",
        "    expressions = []\n",
        "\n",
        "    for resp in responses:\n",
        "        # Handle exceptions from asyncio.gather\n",
        "        if isinstance(resp, Exception):\n",
        "            expressions.append({\"expr\": \"\", \"valid\": False, \"result\": 0, \"error\": str(resp)})\n",
        "            continue\n",
        "        if not resp:\n",
        "            expressions.append({\"expr\": \"\", \"valid\": False, \"result\": 0, \"error\": \"Empty response\"})\n",
        "            continue\n",
        "\n",
        "        # Extract expression\n",
        "        match = re.search(r\"ANSWER:\\s*(.+?)\\s*=\\s*24\", resp, re.IGNORECASE)\n",
        "        if match:\n",
        "            expr = match.group(1).strip()\n",
        "        else:\n",
        "            match2 = re.search(r\"([\\d\\s+\\-*/()]+)\\s*=\\s*24\", resp)\n",
        "            expr = match2.group(1).strip() if match2 else \"\"\n",
        "\n",
        "        if expr:\n",
        "            is_valid, result, error = safe_eval_expression(expr, nums)\n",
        "            expressions.append({\"expr\": expr, \"valid\": is_valid, \"result\": result, \"error\": error})\n",
        "            if is_valid:\n",
        "                valid_count += 1\n",
        "        else:\n",
        "            expressions.append({\"expr\": \"\", \"valid\": False, \"result\": 0, \"error\": \"No expression found\"})\n",
        "\n",
        "    consistency = valid_count / n_paths\n",
        "    execution_rate = sum(1 for e in expressions if e[\"valid\"] or e[\"result\"] != 0) / n_paths\n",
        "\n",
        "    return {\n",
        "        \"consistency\": consistency,\n",
        "        \"execution_rate\": execution_rate,\n",
        "        \"valid_count\": valid_count,\n",
        "        \"total_paths\": n_paths,\n",
        "        \"expressions\": expressions,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVqiQxWJ4SXS"
      },
      "outputs": [],
      "source": [
        "# --- 5.3 Perturbation Testing ---\n",
        "\n",
        "def generate_perturbations(numbers: Tuple[int, ...], n_perturb: int = 3) -> List[Tuple[int, ...]]:\n",
        "    \"\"\"\n",
        "    Generate perturbed versions of a problem by swapping numbers.\n",
        "    Only return perturbations that are solvable.\n",
        "    \"\"\"\n",
        "    perturbations = []\n",
        "    nums = list(numbers)\n",
        "\n",
        "    # Try swapping one number at a time\n",
        "    for i in range(4):\n",
        "        for delta in [-1, 1, -2, 2]:\n",
        "            new_nums = nums.copy()\n",
        "            new_val = nums[i] + delta\n",
        "            if 1 <= new_val <= 13:\n",
        "                new_nums[i] = new_val\n",
        "                new_tuple = tuple(sorted(new_nums))\n",
        "                if new_tuple != numbers and is_solvable_24(new_tuple):\n",
        "                    perturbations.append(new_tuple)\n",
        "                    if len(perturbations) >= n_perturb:\n",
        "                        return perturbations\n",
        "\n",
        "    return perturbations\n",
        "\n",
        "\n",
        "async def test_perturbation_robustness(problem: Dict, strategy: str, n_perturb: int = 3) -> Dict:\n",
        "    \"\"\"\n",
        "    Test if a strategy generalizes to perturbed problems.\n",
        "    \"\"\"\n",
        "    perturbations = generate_perturbations(problem[\"numbers\"], n_perturb)\n",
        "\n",
        "    if not perturbations:\n",
        "        return {\"robustness\": 1.0, \"tested\": 0, \"passed\": 0}  # No perturbations possible\n",
        "\n",
        "    system_with_strategy = f\"\"\"{GAME24_SYSTEM}\n",
        "\n",
        "Use this strategy: {strategy}\"\"\"\n",
        "\n",
        "    # CUDA optimization: max_tokens=512 for short Game of 24 answers\n",
        "    tasks = []\n",
        "    for perturbed_nums in perturbations:\n",
        "        user_prompt = f\"Numbers: {perturbed_nums[0]}, {perturbed_nums[1]}, {perturbed_nums[2]}, {perturbed_nums[3]}\\n\\nFind an expression that equals 24.\"\n",
        "        tasks.append(llm_call_async(system_with_strategy, user_prompt, role=\"perturb\", temperature=0.7, max_tokens=512))\n",
        "\n",
        "    responses = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "    passed = 0\n",
        "    for resp, perturbed_nums in zip(responses, perturbations):\n",
        "        if isinstance(resp, Exception) or not resp:\n",
        "            continue\n",
        "        match = re.search(r\"ANSWER:\\s*(.+?)\\s*=\\s*24\", resp, re.IGNORECASE)\n",
        "        if match:\n",
        "            expr = match.group(1).strip()\n",
        "            is_valid, _, _ = safe_eval_expression(expr, perturbed_nums)\n",
        "            if is_valid:\n",
        "                passed += 1\n",
        "\n",
        "    robustness = passed / len(perturbations) if perturbations else 1.0\n",
        "    return {\"robustness\": robustness, \"tested\": len(perturbations), \"passed\": passed}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn143b2C4SXS"
      },
      "outputs": [],
      "source": [
        "# --- 5.4 Adversarial Probing ---\n",
        "\n",
        "async def test_adversarial(strategy: str, hard_problems: List[Dict]) -> Dict:\n",
        "    \"\"\"\n",
        "    Test strategy against known-hard problems.\n",
        "    \"\"\"\n",
        "    if not hard_problems:\n",
        "        return {\"adversarial_score\": 0.0, \"solved\": 0, \"total\": 0, \"results\": []}\n",
        "\n",
        "    system_with_strategy = f\"\"\"{GAME24_SYSTEM}\n",
        "\n",
        "Use this strategy: {strategy}\"\"\"\n",
        "\n",
        "    # CUDA optimization: max_tokens=512 for short Game of 24 answers\n",
        "    tasks = []\n",
        "    for hp in hard_problems:\n",
        "        nums = hp[\"numbers\"]\n",
        "        user_prompt = f\"Numbers: {nums[0]}, {nums[1]}, {nums[2]}, {nums[3]}\\n\\nFind an expression that equals 24.\"\n",
        "        tasks.append(llm_call_async(system_with_strategy, user_prompt, role=\"adversarial\", temperature=0.7, max_tokens=512))\n",
        "\n",
        "    responses = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "    solved = 0\n",
        "    results = []\n",
        "    for resp, hp in zip(responses, hard_problems):\n",
        "        if isinstance(resp, Exception) or not resp:\n",
        "            results.append({\"numbers\": hp[\"numbers\"], \"valid\": False, \"expr\": \"\", \"error\": str(resp) if isinstance(resp, Exception) else \"Empty response\"})\n",
        "            continue\n",
        "\n",
        "        match = re.search(r\"ANSWER:\\s*(.+?)\\s*=\\s*24\", resp, re.IGNORECASE)\n",
        "        if match:\n",
        "            expr = match.group(1).strip()\n",
        "            is_valid, result, error = safe_eval_expression(expr, hp[\"numbers\"])\n",
        "            results.append({\"numbers\": hp[\"numbers\"], \"valid\": is_valid, \"expr\": expr})\n",
        "            if is_valid:\n",
        "                solved += 1\n",
        "        else:\n",
        "            results.append({\"numbers\": hp[\"numbers\"], \"valid\": False, \"expr\": \"\"})\n",
        "\n",
        "    adversarial_score = solved / len(hard_problems)\n",
        "    return {\"adversarial_score\": adversarial_score, \"solved\": solved, \"total\": len(hard_problems), \"results\": results}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Zu0TuGQ4SXS"
      },
      "outputs": [],
      "source": [
        "# --- 5.5 Confidence Scoring ---\n",
        "\n",
        "async def compute_full_verification(problem: Dict, strategy: str = None) -> Dict:\n",
        "    \"\"\"\n",
        "    Run full verification suite and compute confidence score.\n",
        "\n",
        "    Confidence = consistency * execution_rate * robustness * (1 + adversarial_bonus)\n",
        "    \"\"\"\n",
        "    # Multi-path consistency\n",
        "    multipath = await verify_multipath_consistency(problem, n_paths=5)\n",
        "\n",
        "    # Extract strategy from valid solutions\n",
        "    if strategy is None:\n",
        "        valid_exprs = [e[\"expr\"] for e in multipath[\"expressions\"] if e[\"valid\"]]\n",
        "        if valid_exprs:\n",
        "            # Use the most common valid expression as the strategy\n",
        "            strategy = f\"For numbers like {problem['numbers']}, try: {valid_exprs[0]}\"\n",
        "        else:\n",
        "            # Fallback to ground truth\n",
        "            solutions = solve_24_exhaustive(problem[\"numbers\"])\n",
        "            if solutions:\n",
        "                strategy = f\"For numbers like {problem['numbers']}, try: {solutions[0]}\"\n",
        "            else:\n",
        "                strategy = \"Look for factor pairs that multiply to 24\"\n",
        "\n",
        "    # Perturbation robustness\n",
        "    perturb = await test_perturbation_robustness(problem, strategy, n_perturb=3)\n",
        "\n",
        "    # Adversarial testing (only on a subset)\n",
        "    adversarial = await test_adversarial(strategy, HARD_PROBLEMS[:5])\n",
        "\n",
        "    # Compute confidence\n",
        "    consistency = multipath[\"consistency\"]\n",
        "    execution_rate = multipath[\"execution_rate\"]\n",
        "    robustness = perturb[\"robustness\"]\n",
        "    adversarial_bonus = adversarial[\"adversarial_score\"] * 0.2  # Up to 20% bonus\n",
        "\n",
        "    confidence = consistency * execution_rate * robustness * (1 + adversarial_bonus)\n",
        "\n",
        "    return {\n",
        "        \"confidence\": confidence,\n",
        "        \"consistency\": consistency,\n",
        "        \"execution_rate\": execution_rate,\n",
        "        \"robustness\": robustness,\n",
        "        \"adversarial_bonus\": adversarial_bonus,\n",
        "        \"strategy\": strategy,\n",
        "        \"multipath\": multipath,\n",
        "        \"perturbation\": perturb,\n",
        "        \"adversarial\": adversarial,\n",
        "    }\n",
        "\n",
        "\n",
        "# Run verification on all archetypes IN PARALLEL\n",
        "# CUDA best practice: Parallelize independent work to maximize throughput\n",
        "print(\"Running verification suite on archetypes (parallel)...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "async def verify_all_archetypes():\n",
        "    \"\"\"Verify all archetypes concurrently.\"\"\"\n",
        "    # Launch all verifications in parallel\n",
        "    tasks = [compute_full_verification(arch) for arch in archetypes]\n",
        "    print(f\"Launching {len(tasks)} parallel verification tasks...\")\n",
        "\n",
        "    verifications = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "    # Assign results back to archetypes\n",
        "    for i, (arch, verification) in enumerate(zip(archetypes, verifications)):\n",
        "        if isinstance(verification, Exception):\n",
        "            print(f\"  Archetype {i+1} ({arch['numbers']}): FAILED - {verification}\")\n",
        "            arch[\"verification\"] = {\"confidence\": 0.0, \"error\": str(verification)}\n",
        "        else:\n",
        "            arch[\"verification\"] = verification\n",
        "            print(f\"  Archetype {i+1} ({arch['numbers']}): conf={verification['confidence']:.3f} \"\n",
        "                  f\"(cons={verification['consistency']:.2f}, exec={verification['execution_rate']:.2f}, \"\n",
        "                  f\"rob={verification['robustness']:.2f}, adv={verification['adversarial_bonus']:.2f})\")\n",
        "\n",
        "asyncio.run(verify_all_archetypes())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"Verification complete. Total LLM calls: {sum(call_counter.values())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjcVGkDj4SXS"
      },
      "source": [
        "## 6. Playbook Bootstrap from Verified Archetypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdflwbRJ4SXS"
      },
      "outputs": [],
      "source": [
        "# --- Playbook Data Structure ---\n",
        "\n",
        "@dataclass\n",
        "class PlaybookBullet:\n",
        "    \"\"\"A single strategy in the playbook.\"\"\"\n",
        "    id: str\n",
        "    content: str\n",
        "    confidence: float\n",
        "    source_problem: Tuple[int, ...]\n",
        "    helpful_count: int = 0\n",
        "    harmful_count: int = 0\n",
        "\n",
        "    def to_str(self) -> str:\n",
        "        return f\"[{self.id}] confidence={self.confidence:.2f} helpful={self.helpful_count} harmful={self.harmful_count} :: {self.content}\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Playbook:\n",
        "    \"\"\"Collection of strategies for solving Game of 24.\"\"\"\n",
        "    bullets: List[PlaybookBullet] = field(default_factory=list)\n",
        "    _next_id: int = 1\n",
        "\n",
        "    def add(self, content: str, confidence: float, source_problem: Tuple[int, ...]) -> str:\n",
        "        \"\"\"Add a new bullet and return its ID.\"\"\"\n",
        "        bid = f\"arch-{self._next_id:05d}\"\n",
        "        self._next_id += 1\n",
        "        self.bullets.append(PlaybookBullet(\n",
        "            id=bid, content=content, confidence=confidence, source_problem=source_problem\n",
        "        ))\n",
        "        return bid\n",
        "\n",
        "    def tag(self, bid: str, label: str):\n",
        "        \"\"\"Tag a bullet as helpful or harmful.\"\"\"\n",
        "        for b in self.bullets:\n",
        "            if b.id == bid:\n",
        "                if label == \"helpful\":\n",
        "                    b.helpful_count += 1\n",
        "                elif label == \"harmful\":\n",
        "                    b.harmful_count += 1\n",
        "                break\n",
        "\n",
        "    def update_confidence(self, bid: str, delta: float):\n",
        "        \"\"\"Adjust confidence for a bullet.\"\"\"\n",
        "        for b in self.bullets:\n",
        "            if b.id == bid:\n",
        "                b.confidence = max(0.0, min(1.0, b.confidence + delta))\n",
        "                break\n",
        "\n",
        "    def to_str(self) -> str:\n",
        "        \"\"\"Format playbook for LLM prompt.\"\"\"\n",
        "        if not self.bullets:\n",
        "            return \"(empty playbook)\"\n",
        "        lines = [\"## GAME OF 24 STRATEGIES\"]\n",
        "        for b in sorted(self.bullets, key=lambda x: -x.confidence):\n",
        "            lines.append(b.to_str())\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    def copy(self) -> \"Playbook\":\n",
        "        \"\"\"Create a deep copy of the playbook.\"\"\"\n",
        "        return copy.deepcopy(self)\n",
        "\n",
        "    @property\n",
        "    def size(self) -> int:\n",
        "        \"\"\"Number of bullets in the playbook.\"\"\"\n",
        "        return len(self.bullets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsZiN-IU4SXS"
      },
      "outputs": [],
      "source": [
        "# --- Extract Strategies from Archetypes ---\n",
        "\n",
        "STRATEGY_EXTRACTION_SYSTEM = \"\"\"You are analyzing Game of 24 solutions to extract reusable strategies.\n",
        "\n",
        "Given a solved problem, extract 1-2 general strategies that could help solve similar problems.\n",
        "Focus on:\n",
        "- Patterns involving multiplication/division to reach 24\n",
        "- How to use specific numbers (like 6, 8, 4, 3, 2, 1)\n",
        "- When to use fractions vs whole number operations\n",
        "- Factor pairs that make 24: (1,24), (2,12), (3,8), (4,6)\n",
        "\n",
        "Format each strategy as a single line starting with \"STRATEGY:\"\n",
        "Make strategies general enough to apply to other problems.\n",
        "\n",
        "Example output:\n",
        "STRATEGY: When you have 6 and 4, try to make the other numbers equal 1 (e.g., 3/3=1) so 6*4*1=24\n",
        "STRATEGY: If you have 8, look for ways to make 3 from the other numbers since 8*3=24\"\"\"\n",
        "\n",
        "\n",
        "async def extract_strategies_from_archetype(arch: Dict) -> List[str]:\n",
        "    \"\"\"Extract general strategies from a verified archetype.\"\"\"\n",
        "    nums = arch[\"numbers\"]\n",
        "    solutions = solve_24_exhaustive(nums)[:3]  # Use up to 3 solutions\n",
        "\n",
        "    if not solutions:\n",
        "        print(f\"WARNING: No valid solution found for archetype {nums}, skipping\")\n",
        "        return []\n",
        "\n",
        "    user_prompt = f\"\"\"Problem: Use {nums[0]}, {nums[1]}, {nums[2]}, {nums[3]} to make 24\n",
        "\n",
        "Valid solutions:\n",
        "\"\"\"\n",
        "    for sol in solutions:\n",
        "        user_prompt += f\"- {sol}\\n\"\n",
        "\n",
        "    user_prompt += \"\\nExtract 1-2 reusable strategies from these solutions.\"\n",
        "\n",
        "    response = await llm_call_async(STRATEGY_EXTRACTION_SYSTEM, user_prompt, role=\"extract\", temperature=0.5)\n",
        "\n",
        "    if not response:\n",
        "        return []\n",
        "\n",
        "    strategies = []\n",
        "    for line in response.split(\"\\n\"):\n",
        "        line = line.strip()\n",
        "        if line.startswith(\"STRATEGY:\"):\n",
        "            strategy = line[len(\"STRATEGY:\"):].strip()\n",
        "            if strategy:\n",
        "                strategies.append(strategy)\n",
        "\n",
        "    return strategies\n",
        "\n",
        "\n",
        "# Bootstrap playbook from verified archetypes\n",
        "print(\"Bootstrapping playbook from verified archetypes...\")\n",
        "\n",
        "# Check if any archetypes passed verification (confidence >= 0.3)\n",
        "verified_archetypes = [a for a in archetypes if a.get(\"verification\", {}).get(\"confidence\", 0) >= 0.3]\n",
        "USE_RANDOM_CURRICULUM_FALLBACK = False\n",
        "if not verified_archetypes:\n",
        "    print(\"WARNING: All archetypes failed verification. Falling back to random curriculum.\")\n",
        "    USE_RANDOM_CURRICULUM_FALLBACK = True\n",
        "\n",
        "playbook = Playbook()\n",
        "\n",
        "# Add base strategies\n",
        "BASE_STRATEGIES = [\n",
        "    \"Look for factor pairs of 24: (1,24), (2,12), (3,8), (4,6)\",\n",
        "    \"If you have 8, try to make 3 from remaining numbers (8*3=24)\",\n",
        "    \"If you have 6, try to make 4 from remaining numbers (6*4=24)\",\n",
        "    \"Use division to create fractions when direct operations don't work\",\n",
        "    \"Try (a+b)*(c-d) or (a-b)*(c+d) patterns for 24\",\n",
        "]\n",
        "for strat in BASE_STRATEGIES:\n",
        "    playbook.add(strat, confidence=0.7, source_problem=(0, 0, 0, 0))\n",
        "\n",
        "\n",
        "async def bootstrap_playbook():\n",
        "    for arch in archetypes:\n",
        "        confidence = arch.get(\"verification\", {}).get(\"confidence\", 0.5)\n",
        "        if confidence < 0.3:\n",
        "            print(f\"  {arch['numbers']}: skipped (confidence {confidence:.2f} < 0.3)\")\n",
        "            continue\n",
        "\n",
        "        # REQ-VER-1: Enforce consistency threshold of 0.6\n",
        "        consistency = arch.get(\"verification\", {}).get(\"consistency\", 0)\n",
        "        if consistency < 0.6:\n",
        "            print(f\"WARNING: Archetype {arch['numbers']} failed consistency check ({consistency:.2f} < 0.6)\")\n",
        "            continue\n",
        "\n",
        "        strategies = await extract_strategies_from_archetype(arch)\n",
        "        for strat in strategies:\n",
        "            playbook.add(strat, confidence=confidence, source_problem=arch[\"numbers\"])\n",
        "\n",
        "        print(f\"  {arch['numbers']}: extracted {len(strategies)} strategies (conf={confidence:.2f})\")\n",
        "\n",
        "\n",
        "asyncio.run(bootstrap_playbook())\n",
        "\n",
        "print(f\"\\nPlaybook initialized with {playbook.size} strategies:\")\n",
        "print(\"-\" * 80)\n",
        "print(playbook.to_str())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_xihOay4SXS"
      },
      "source": [
        "## 7. LinUCB Contextual Bandit Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiAFR11L4SXS"
      },
      "outputs": [],
      "source": [
        "# --- LinUCB Contextual Bandit ---\n",
        "\n",
        "class LinUCB:\n",
        "    \"\"\"\n",
        "    Linear Upper Confidence Bound algorithm for contextual bandits.\n",
        "\n",
        "    Each arm maintains:\n",
        "    - A_k: d x d matrix (initialized to identity)\n",
        "    - b_k: d-dimensional vector (initialized to zero)\n",
        "    - theta_k = A_k^{-1} b_k (estimated coefficients)\n",
        "\n",
        "    Selection: argmax_k (theta_k^T x_t + alpha * sqrt(x_t^T A_k^{-1} x_t))\n",
        "\n",
        "    Regularization: Uses lambda * I for numerical stability in matrix inversion.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_arms: int, d: int, alpha: float = 1.0, reg_lambda: float = 1.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            n_arms: Number of arms.\n",
        "            d: Feature dimension.\n",
        "            alpha: Exploration parameter (higher = more exploration).\n",
        "            reg_lambda: Regularization parameter for matrix inversion.\n",
        "        \"\"\"\n",
        "        self.n_arms = n_arms\n",
        "        self.d = d\n",
        "        self.alpha = alpha\n",
        "        self.reg_lambda = reg_lambda\n",
        "\n",
        "        # Initialize A with regularization (lambda * I) and b to zero for each arm\n",
        "        self.A = [reg_lambda * np.eye(d) for _ in range(n_arms)]\n",
        "        self.b = [np.zeros(d) for _ in range(n_arms)]\n",
        "\n",
        "        # Track statistics\n",
        "        self.arm_counts = np.zeros(n_arms)\n",
        "        self.arm_rewards = np.zeros(n_arms)\n",
        "\n",
        "    def _safe_inv(self, matrix: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Compute matrix inverse with regularization fallback.\"\"\"\n",
        "        try:\n",
        "            return np.linalg.inv(matrix)\n",
        "        except np.linalg.LinAlgError:\n",
        "            # Add extra regularization if singular\n",
        "            return np.linalg.inv(matrix + self.reg_lambda * np.eye(self.d))\n",
        "\n",
        "    def select(self, context: np.ndarray) -> int:\n",
        "        \"\"\"\n",
        "        Select arm with highest UCB value.\n",
        "\n",
        "        Args:\n",
        "            context: d-dimensional feature vector\n",
        "\n",
        "        Returns:\n",
        "            Selected arm index\n",
        "        \"\"\"\n",
        "        ucb_values = np.zeros(self.n_arms)\n",
        "\n",
        "        for k in range(self.n_arms):\n",
        "            A_inv = self._safe_inv(self.A[k])\n",
        "            theta_k = A_inv @ self.b[k]\n",
        "\n",
        "            # UCB = theta^T x + alpha * sqrt(x^T A^{-1} x)\n",
        "            exploitation = theta_k @ context\n",
        "            variance_term = context @ A_inv @ context\n",
        "            # Clamp variance term to avoid numerical issues\n",
        "            variance_term = max(0.0, variance_term)\n",
        "            exploration = self.alpha * np.sqrt(variance_term)\n",
        "            ucb_values[k] = exploitation + exploration\n",
        "\n",
        "        return int(np.argmax(ucb_values))\n",
        "\n",
        "    def update(self, arm: int, context: np.ndarray, reward: float):\n",
        "        \"\"\"\n",
        "        Update arm statistics after observing reward.\n",
        "\n",
        "        A_k += x_t x_t^T\n",
        "        b_k += r_t x_t\n",
        "        \"\"\"\n",
        "        self.A[arm] += np.outer(context, context)\n",
        "        self.b[arm] += reward * context\n",
        "        self.arm_counts[arm] += 1\n",
        "        self.arm_rewards[arm] += reward\n",
        "\n",
        "    def get_theta(self, arm: int) -> np.ndarray:\n",
        "        \"\"\"Get estimated coefficients for an arm.\"\"\"\n",
        "        A_inv = self._safe_inv(self.A[arm])\n",
        "        return A_inv @ self.b[arm]\n",
        "\n",
        "    def get_ucb_gap(self, context: np.ndarray) -> float:\n",
        "        \"\"\"Get the gap between best and second-best UCB values.\"\"\"\n",
        "        ucb_values = []\n",
        "        for k in range(self.n_arms):\n",
        "            A_inv = self._safe_inv(self.A[k])\n",
        "            theta_k = A_inv @ self.b[k]\n",
        "            variance_term = max(0.0, context @ A_inv @ context)\n",
        "            ucb = theta_k @ context + self.alpha * np.sqrt(variance_term)\n",
        "            ucb_values.append(ucb)\n",
        "\n",
        "        sorted_ucb = sorted(ucb_values, reverse=True)\n",
        "        if len(sorted_ucb) > 1:\n",
        "            return sorted_ucb[0] - sorted_ucb[1]\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "print(\"LinUCB implementation ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WfRH_7w4SXT"
      },
      "outputs": [],
      "source": [
        "# --- Problem Feature Extraction for LinUCB ---\n",
        "\n",
        "def extract_linucb_features(problem: Dict) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Extract feature vector for LinUCB context.\n",
        "    Uses the same numerical features as for clustering.\n",
        "    \"\"\"\n",
        "    nums = problem[\"numbers\"]\n",
        "\n",
        "    features = [\n",
        "        sum(nums) / 52,  # sum normalized\n",
        "        np.prod(nums) / (13**4),  # product normalized\n",
        "        max(nums) / 13,  # max normalized\n",
        "        min(nums) / 13,  # min normalized\n",
        "        len(set(nums)) / 4,  # uniqueness ratio\n",
        "        (max(nums) - min(nums)) / 12,  # range normalized\n",
        "        np.std(nums) / 5,  # std normalized\n",
        "        sum(1 for n in nums if n % 2 == 0) / 4,  # even ratio\n",
        "        sum(1 for n in nums if 24 % n == 0) / 4,  # divisor of 24 ratio\n",
        "        1.0 if any(a * b == 24 for a, b in itertools.combinations(nums, 2)) else 0.0,  # has factor pair\n",
        "        # Additional features for curriculum learning\n",
        "        problem.get(\"n_solutions\", 5) / 20,  # solution count normalized\n",
        "        problem.get(\"centrality\", 0.5),  # cluster centrality\n",
        "        1.0,  # bias term\n",
        "    ]\n",
        "\n",
        "    return np.array(features)\n",
        "\n",
        "\n",
        "# Verify feature extraction\n",
        "test_features = extract_linucb_features(problems[0])\n",
        "print(f\"Feature dimension: {len(test_features)}\")\n",
        "print(f\"Example features: {test_features}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvR9styt4SXT"
      },
      "source": [
        "## 8. Curriculum Loop (Archetype-first vs Random)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPJ5q_ho4SXT"
      },
      "outputs": [],
      "source": [
        "# --- Core Evaluation Functions ---\n",
        "\n",
        "SOLVE_SYSTEM_TEMPLATE = \"\"\"You are a Game of 24 solver. Given 4 numbers, find an expression using +, -, *, / that equals 24.\n",
        "Each number must be used exactly once.\n",
        "\n",
        "{playbook}\n",
        "\n",
        "When you use a strategy from the playbook, mention its ID (e.g., [arch-00001]).\n",
        "Show your reasoning, then give the final answer as: ANSWER: <expression> = 24\"\"\"\n",
        "\n",
        "\n",
        "async def solve_with_playbook(problem: Dict, playbook: Playbook) -> Dict:\n",
        "    \"\"\"\n",
        "    Solve a Game of 24 problem using the playbook.\n",
        "\n",
        "    Returns dict with: correct, expression, result, error, bullets_used, response\n",
        "    \"\"\"\n",
        "    nums = problem[\"numbers\"]\n",
        "\n",
        "    system = SOLVE_SYSTEM_TEMPLATE.format(playbook=playbook.to_str())\n",
        "    user_prompt = f\"Numbers: {nums[0]}, {nums[1]}, {nums[2]}, {nums[3]}\\n\\nFind an expression that equals 24.\"\n",
        "    response = await llm_call_async(system, user_prompt, role=\"solve\", temperature=0.7, max_tokens=512)\n",
        "\n",
        "    # Handle empty response\n",
        "    if not response:\n",
        "        return {\n",
        "            \"correct\": False,\n",
        "            \"expression\": \"\",\n",
        "            \"result\": 0.0,\n",
        "            \"error\": \"Empty LLM response\",\n",
        "            \"bullets_used\": [],\n",
        "            \"response\": \"\",\n",
        "        }\n",
        "\n",
        "    # Extract answer\n",
        "    match = re.search(r\"ANSWER:\\s*(.+?)\\s*=\\s*24\", response, re.IGNORECASE)\n",
        "    if match:\n",
        "        expr = match.group(1).strip()\n",
        "    else:\n",
        "        match2 = re.search(r\"([\\d\\s+\\-*/()]+)\\s*=\\s*24\", response)\n",
        "        expr = match2.group(1).strip() if match2 else \"\"\n",
        "\n",
        "    # Validate\n",
        "    if expr:\n",
        "        is_correct, result, error = safe_eval_expression(expr, nums)\n",
        "    else:\n",
        "        is_correct, result, error = False, 0.0, \"No expression found in response\"\n",
        "\n",
        "    # Extract used bullet IDs\n",
        "    bullets_used = re.findall(r\"\\[(arch-\\d+)\\]\", response)\n",
        "\n",
        "    return {\n",
        "        \"correct\": is_correct,\n",
        "        \"expression\": expr,\n",
        "        \"result\": result,\n",
        "        \"error\": error,\n",
        "        \"bullets_used\": bullets_used,\n",
        "        \"response\": response,\n",
        "    }\n",
        "\n",
        "\n",
        "def update_playbook_from_result(playbook: Playbook, result: Dict):\n",
        "    \"\"\"Update playbook bullet statistics based on result.\"\"\"\n",
        "    label = \"helpful\" if result[\"correct\"] else \"harmful\"\n",
        "    for bid in result[\"bullets_used\"]:\n",
        "        playbook.tag(bid, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEPaEbVv4SXT"
      },
      "outputs": [],
      "source": [
        "# --- Run Log ---\n",
        "\n",
        "@dataclass\n",
        "class CurriculumRunLog:\n",
        "    \"\"\"Tracks results for a curriculum condition.\"\"\"\n",
        "    correct: List[bool] = field(default_factory=list)\n",
        "    playbook_sizes: List[int] = field(default_factory=list)\n",
        "    confidence_history: List[List[float]] = field(default_factory=list)\n",
        "    ucb_gaps: List[float] = field(default_factory=list)\n",
        "    problems_order: List[Tuple[int, ...]] = field(default_factory=list)\n",
        "    call_counts: Dict[str, int] = field(default_factory=dict)\n",
        "    final_playbook: Optional[Playbook] = None\n",
        "    # Strategy churn tracking (REQ-EVAL-2)\n",
        "    strategies_added: List[int] = field(default_factory=list)\n",
        "    strategies_removed: List[int] = field(default_factory=list)\n",
        "\n",
        "    @property\n",
        "    def cumulative_accuracy(self) -> List[float]:\n",
        "        \"\"\"Compute cumulative accuracy at each step.\"\"\"\n",
        "        if not self.correct:\n",
        "            return []\n",
        "        acc = []\n",
        "        total = 0\n",
        "        for i, c in enumerate(self.correct):\n",
        "            total += int(c)\n",
        "            acc.append(total / (i + 1))\n",
        "        return acc\n",
        "\n",
        "    @property\n",
        "    def final_accuracy(self) -> float:\n",
        "        \"\"\"Final accuracy over all problems.\"\"\"\n",
        "        if not self.correct:\n",
        "            return 0.0\n",
        "        return sum(self.correct) / len(self.correct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pcn4UO4G4SXT"
      },
      "outputs": [],
      "source": [
        "# --- Archetype-First Curriculum ---\n",
        "\n",
        "CONFIDENCE_UPDATE_INTERVAL = 10\n",
        "N_EVAL_PROBLEMS = 100\n",
        "BATCH_SIZE = 128  # Match MAX_CONCURRENT_LLM for full GPU utilization  # Mini-batch size for parallel LLM calls\n",
        "\n",
        "\n",
        "def _chunked(iterable, size):\n",
        "    \"\"\"Yield successive chunks of specified size.\"\"\"\n",
        "    for i in range(0, len(iterable), size):\n",
        "        yield iterable[i:i + size]\n",
        "\n",
        "\n",
        "def _update_bullet_confidences(playbook: Playbook, decay: float = 0.7) -> int:\n",
        "    \"\"\"\n",
        "    Update bullet confidences based on helpful/harmful counts.\n",
        "    Returns number of strategies removed (pruned).\n",
        "    \"\"\"\n",
        "    removed_count = 0\n",
        "    bullets_to_keep = []\n",
        "    for b in playbook.bullets:\n",
        "        total = b.helpful_count + b.harmful_count\n",
        "        if total > 0:\n",
        "            ratio = b.helpful_count / total\n",
        "            b.confidence = decay * b.confidence + (1 - decay) * ratio\n",
        "            # Prune strategies with very low confidence after sufficient feedback\n",
        "            if b.confidence < 0.1 and total >= 5:\n",
        "                removed_count += 1\n",
        "                continue\n",
        "        bullets_to_keep.append(b)\n",
        "    playbook.bullets = bullets_to_keep\n",
        "    return removed_count\n",
        "\n",
        "\n",
        "async def run_archetype_curriculum(\n",
        "    all_problems: List[Dict],\n",
        "    archetypes: List[Dict],\n",
        "    initial_playbook: Playbook,\n",
        ") -> CurriculumRunLog:\n",
        "    \"\"\"\n",
        "    Run archetype-first curriculum with mini-batching for GPU efficiency.\n",
        "\n",
        "    Mini-batching optimization:\n",
        "    - Process BATCH_SIZE problems in parallel via asyncio.gather\n",
        "    - Update playbook/bandit after each batch (not each problem)\n",
        "    - Enables prefix caching (constant system prompt within batch)\n",
        "    - LinUCB is robust to delayed feedback\n",
        "\n",
        "    Phases:\n",
        "    1. Phase 1: Process archetypes first (warm start)\n",
        "    2. Phase 2: LinUCB exploration-exploitation on remaining problems\n",
        "    \"\"\"\n",
        "    log = CurriculumRunLog()\n",
        "    pb = initial_playbook.copy()\n",
        "    initial_strategy_count = pb.size\n",
        "\n",
        "    # Track strategies added during bootstrap (already done before this function)\n",
        "    log.strategies_added.append(initial_strategy_count)\n",
        "\n",
        "    # Separate archetypes from other problems\n",
        "    archetype_nums = {a[\"numbers\"] for a in archetypes}\n",
        "    non_archetypes = [p for p in all_problems if p[\"numbers\"] not in archetype_nums]\n",
        "\n",
        "    # Limit to N_EVAL_PROBLEMS total\n",
        "    n_archetypes = min(10, N_EVAL_PROBLEMS // 10)\n",
        "    eval_archetypes = archetypes[:n_archetypes]\n",
        "    remaining_budget = N_EVAL_PROBLEMS - len(eval_archetypes)\n",
        "    eval_non_archetypes = non_archetypes[:remaining_budget]\n",
        "\n",
        "    # Initialize LinUCB\n",
        "    N_ARMS = 3\n",
        "    FEATURE_DIM = 13\n",
        "    linucb = LinUCB(n_arms=N_ARMS, d=FEATURE_DIM, alpha=1.5, reg_lambda=1.0)\n",
        "\n",
        "    print(f\"\\nPhase 1: Processing {len(eval_archetypes)} archetypes (batch_size={BATCH_SIZE})...\")\n",
        "\n",
        "    # Phase 1: Archetypes (batched)\n",
        "    for batch in _chunked(eval_archetypes, BATCH_SIZE):\n",
        "        # Solve batch in parallel (same playbook for all - enables prefix caching)\n",
        "        tasks = [solve_with_playbook(arch, pb) for arch in batch]\n",
        "        results = await asyncio.gather(*tasks)\n",
        "\n",
        "        # Update logs and playbook after batch\n",
        "        for arch, result in zip(batch, results):\n",
        "            log.correct.append(result[\"correct\"])\n",
        "            log.playbook_sizes.append(pb.size)\n",
        "            log.problems_order.append(arch[\"numbers\"])\n",
        "            log.confidence_history.append([b.confidence for b in pb.bullets])\n",
        "\n",
        "            # Update playbook bullet counts\n",
        "            update_playbook_from_result(pb, result)\n",
        "\n",
        "            # Update LinUCB with archetype results (arm 2 = balanced)\n",
        "            context = extract_linucb_features(arch)\n",
        "            linucb.update(2, context, 1.0 if result[\"correct\"] else 0.0)\n",
        "\n",
        "        acc = sum(log.correct) / len(log.correct)\n",
        "        print(f\"  Archetypes {len(log.correct)}/{len(eval_archetypes)}: accuracy={acc:.2%}\")\n",
        "\n",
        "    print(f\"\\nPhase 2: LinUCB on {len(eval_non_archetypes)} problems (batch_size={BATCH_SIZE})...\")\n",
        "\n",
        "    # Phase 2: LinUCB exploration (batched)\n",
        "    problems_done = 0\n",
        "    for batch in _chunked(eval_non_archetypes, BATCH_SIZE):\n",
        "        # Pre-compute arm selections for batch (before solving)\n",
        "        batch_contexts = [extract_linucb_features(prob) for prob in batch]\n",
        "        batch_arms = [linucb.select(ctx) for ctx in batch_contexts]\n",
        "\n",
        "        # Record UCB gaps\n",
        "        for ctx in batch_contexts:\n",
        "            log.ucb_gaps.append(linucb.get_ucb_gap(ctx))\n",
        "\n",
        "        # Solve batch in parallel (same playbook - enables prefix caching)\n",
        "        tasks = [solve_with_playbook(prob, pb) for prob in batch]\n",
        "        results = await asyncio.gather(*tasks)\n",
        "\n",
        "        # Update logs, playbook, and bandit after batch\n",
        "        for prob, result, ctx, arm in zip(batch, results, batch_contexts, batch_arms):\n",
        "            log.correct.append(result[\"correct\"])\n",
        "            log.playbook_sizes.append(pb.size)\n",
        "            log.problems_order.append(prob[\"numbers\"])\n",
        "            log.confidence_history.append([b.confidence for b in pb.bullets])\n",
        "\n",
        "            # Update playbook and bandit\n",
        "            update_playbook_from_result(pb, result)\n",
        "            reward = 1.0 if result[\"correct\"] else 0.0\n",
        "            linucb.update(arm, ctx, reward)\n",
        "\n",
        "        problems_done += len(batch)\n",
        "\n",
        "        # Retrospective confidence update and pruning (every CONFIDENCE_UPDATE_INTERVAL)\n",
        "        if problems_done % CONFIDENCE_UPDATE_INTERVAL == 0:\n",
        "            removed = _update_bullet_confidences(pb)\n",
        "            if removed > 0:\n",
        "                log.strategies_removed.append(removed)\n",
        "                print(f\"  Pruned {removed} low-confidence strategies\")\n",
        "\n",
        "        total_done = len(eval_archetypes) + problems_done\n",
        "        if total_done % 20 == 0:\n",
        "            acc = sum(log.correct) / len(log.correct)\n",
        "            print(f\"  Problems {total_done}/{N_EVAL_PROBLEMS}: accuracy={acc:.2%}\")\n",
        "\n",
        "    log.final_playbook = pb\n",
        "    log.call_counts = dict(call_counter)\n",
        "\n",
        "    print(f\"\\nArchetype curriculum complete. Final accuracy: {log.final_accuracy:.2%}\")\n",
        "    print(f\"Strategy churn: +{sum(log.strategies_added)} added, -{sum(log.strategies_removed)} removed\")\n",
        "    return log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQURLmV14SXT"
      },
      "outputs": [],
      "source": [
        "# --- Random Curriculum (Baseline) ---\n",
        "\n",
        "async def run_random_curriculum(\n",
        "    all_problems: List[Dict],\n",
        "    initial_playbook: Playbook,\n",
        ") -> CurriculumRunLog:\n",
        "    \"\"\"\n",
        "    Run random curriculum baseline with mini-batching for GPU efficiency.\n",
        "\n",
        "    Mini-batching optimization:\n",
        "    - Process BATCH_SIZE problems in parallel via asyncio.gather\n",
        "    - Update playbook after each batch (not each problem)\n",
        "    - Enables prefix caching (constant system prompt within batch)\n",
        "    \"\"\"\n",
        "    log = CurriculumRunLog()\n",
        "    pb = initial_playbook.copy()\n",
        "    initial_strategy_count = pb.size\n",
        "\n",
        "    # Track initial strategies\n",
        "    log.strategies_added.append(initial_strategy_count)\n",
        "\n",
        "    # Shuffle problems\n",
        "    eval_problems = all_problems[:N_EVAL_PROBLEMS]\n",
        "    rng = random.Random(SEED + 1)  # Different seed for random order\n",
        "    shuffled = eval_problems.copy()\n",
        "    rng.shuffle(shuffled)\n",
        "\n",
        "    print(f\"\\nProcessing {len(shuffled)} problems in random order (batch_size={BATCH_SIZE})...\")\n",
        "\n",
        "    problems_done = 0\n",
        "    for batch in _chunked(shuffled, BATCH_SIZE):\n",
        "        # Solve batch in parallel (same playbook - enables prefix caching)\n",
        "        tasks = [solve_with_playbook(prob, pb) for prob in batch]\n",
        "        results = await asyncio.gather(*tasks)\n",
        "\n",
        "        # Update logs and playbook after batch\n",
        "        for prob, result in zip(batch, results):\n",
        "            log.correct.append(result[\"correct\"])\n",
        "            log.playbook_sizes.append(pb.size)\n",
        "            log.problems_order.append(prob[\"numbers\"])\n",
        "            log.confidence_history.append([b.confidence for b in pb.bullets])\n",
        "\n",
        "            # Update playbook\n",
        "            update_playbook_from_result(pb, result)\n",
        "\n",
        "        problems_done += len(batch)\n",
        "\n",
        "        # Retrospective confidence update and pruning (every CONFIDENCE_UPDATE_INTERVAL)\n",
        "        if problems_done % CONFIDENCE_UPDATE_INTERVAL == 0:\n",
        "            removed = _update_bullet_confidences(pb)\n",
        "            if removed > 0:\n",
        "                log.strategies_removed.append(removed)\n",
        "                print(f\"  Pruned {removed} low-confidence strategies\")\n",
        "\n",
        "        if problems_done % 20 == 0:\n",
        "            acc = sum(log.correct) / len(log.correct)\n",
        "            print(f\"  Problems {problems_done}/{len(shuffled)}: accuracy={acc:.2%}\")\n",
        "\n",
        "    log.final_playbook = pb\n",
        "    log.call_counts = dict(call_counter)\n",
        "\n",
        "    print(f\"\\nRandom curriculum complete. Final accuracy: {log.final_accuracy:.2%}\")\n",
        "    print(f\"Strategy churn: +{sum(log.strategies_added)} added, -{sum(log.strategies_removed)} removed\")\n",
        "    return log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Qg8e22i4SXT"
      },
      "outputs": [],
      "source": [
        "# --- Run Both Conditions ---\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"RUNNING ARCHETYPE-FIRST CURRICULUM\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Reset call counter\n",
        "call_counter.clear()\n",
        "\n",
        "# Check for fallback mode (set in cell-23 if all archetypes failed verification)\n",
        "if USE_RANDOM_CURRICULUM_FALLBACK:\n",
        "    print(\"FALLBACK MODE: All archetypes failed verification, using random curriculum for both conditions\")\n",
        "    archetype_log = asyncio.run(run_random_curriculum(problems, playbook))\n",
        "else:\n",
        "    archetype_log = asyncio.run(run_archetype_curriculum(problems, archetypes, playbook))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RUNNING RANDOM CURRICULUM (BASELINE)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Reset call counter\n",
        "call_counter.clear()\n",
        "\n",
        "random_log = asyncio.run(run_random_curriculum(problems, playbook))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARISON SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Archetype-first accuracy: {archetype_log.final_accuracy:.2%}\")\n",
        "print(f\"Random baseline accuracy: {random_log.final_accuracy:.2%}\")\n",
        "print(f\"Improvement: {(archetype_log.final_accuracy - random_log.final_accuracy)*100:.1f} percentage points\")\n",
        "print(f\"\\nStrategy churn (archetype): +{sum(archetype_log.strategies_added)} added, -{sum(archetype_log.strategies_removed)} removed\")\n",
        "print(f\"Strategy churn (random): +{sum(random_log.strategies_added)} added, -{sum(random_log.strategies_removed)} removed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5XSrFwZ4SXT"
      },
      "source": [
        "## 8.5 Coverage Efficiency Ablation\n",
        "\n",
        "Measure how many verified archetypes are needed for effective learning.\n",
        "This answers the proposal question: \"how many verified archetypes are needed?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_D-3_In4SXT"
      },
      "outputs": [],
      "source": [
        "# --- Coverage Efficiency: Ablation on Number of Archetypes ---\n",
        "\n",
        "ARCHETYPE_COUNTS = [0, 2, 4, 6, 8, 10]  # Vary number of archetypes used\n",
        "\n",
        "async def run_archetype_ablation(\n",
        "    all_problems: List[Dict],\n",
        "    archetypes: List[Dict],\n",
        "    initial_playbook: Playbook,\n",
        "    n_archetypes: int,\n",
        ") -> CurriculumRunLog:\n",
        "    \"\"\"\n",
        "    Run curriculum with exactly n_archetypes in Phase 1.\n",
        "    n_archetypes=0 is equivalent to random curriculum (no archetype warm-start).\n",
        "    \"\"\"\n",
        "    log = CurriculumRunLog()\n",
        "    pb = initial_playbook.copy()\n",
        "    log.strategies_added.append(pb.size)\n",
        "\n",
        "    # Select subset of archetypes\n",
        "    eval_archetypes = archetypes[:n_archetypes]\n",
        "\n",
        "    # Remaining problems (exclude used archetypes)\n",
        "    archetype_nums = {a[\"numbers\"] for a in eval_archetypes}\n",
        "    non_archetypes = [p for p in all_problems if p[\"numbers\"] not in archetype_nums]\n",
        "    remaining_budget = N_EVAL_PROBLEMS - len(eval_archetypes)\n",
        "    eval_non_archetypes = non_archetypes[:remaining_budget]\n",
        "\n",
        "    # Initialize LinUCB\n",
        "    N_ARMS = 3\n",
        "    FEATURE_DIM = 13\n",
        "    linucb = LinUCB(n_arms=N_ARMS, d=FEATURE_DIM, alpha=1.5, reg_lambda=1.0)\n",
        "\n",
        "    # Phase 1: Archetypes (if any)\n",
        "    if eval_archetypes:\n",
        "        for batch in _chunked(eval_archetypes, BATCH_SIZE):\n",
        "            tasks = [solve_with_playbook(arch, pb) for arch in batch]\n",
        "            results = await asyncio.gather(*tasks)\n",
        "            for arch, result in zip(batch, results):\n",
        "                log.correct.append(result[\"correct\"])\n",
        "                log.playbook_sizes.append(pb.size)\n",
        "                log.problems_order.append(arch[\"numbers\"])\n",
        "                update_playbook_from_result(pb, result)\n",
        "                context = extract_linucb_features(arch)\n",
        "                linucb.update(2, context, 1.0 if result[\"correct\"] else 0.0)\n",
        "\n",
        "    # Phase 2: Remaining problems with LinUCB\n",
        "    problems_done = 0\n",
        "    for batch in _chunked(eval_non_archetypes, BATCH_SIZE):\n",
        "        batch_contexts = [extract_linucb_features(prob) for prob in batch]\n",
        "        batch_arms = [linucb.select(ctx) for ctx in batch_contexts]\n",
        "\n",
        "        tasks = [solve_with_playbook(prob, pb) for prob in batch]\n",
        "        results = await asyncio.gather(*tasks)\n",
        "\n",
        "        for prob, result, ctx, arm in zip(batch, results, batch_contexts, batch_arms):\n",
        "            log.correct.append(result[\"correct\"])\n",
        "            log.playbook_sizes.append(pb.size)\n",
        "            log.problems_order.append(prob[\"numbers\"])\n",
        "            update_playbook_from_result(pb, result)\n",
        "            linucb.update(arm, ctx, 1.0 if result[\"correct\"] else 0.0)\n",
        "\n",
        "        problems_done += len(batch)\n",
        "        if problems_done % CONFIDENCE_UPDATE_INTERVAL == 0:\n",
        "            _update_bullet_confidences(pb)\n",
        "\n",
        "    log.final_playbook = pb\n",
        "    return log\n",
        "\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"COVERAGE EFFICIENCY ABLATION\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Testing archetype counts: {ARCHETYPE_COUNTS}\")\n",
        "\n",
        "# Reset call counter\n",
        "call_counter.clear()\n",
        "\n",
        "ablation_results = {}\n",
        "\n",
        "async def run_ablation():\n",
        "    \"\"\"Run all ablation conditions IN PARALLEL for GPU efficiency.\"\"\"\n",
        "    # Launch all ablation runs concurrently - GPU semaphore manages load\n",
        "    tasks = [\n",
        "        run_archetype_ablation(problems, archetypes, playbook, n_arch)\n",
        "        for n_arch in ARCHETYPE_COUNTS\n",
        "    ]\n",
        "    print(f\"  Launching {len(tasks)} ablation conditions in parallel...\")\n",
        "    results = await asyncio.gather(*tasks)\n",
        "\n",
        "    # Collect results\n",
        "    for n_arch, log in zip(ARCHETYPE_COUNTS, results):\n",
        "        ablation_results[n_arch] = {\n",
        "            \"final_accuracy\": log.final_accuracy,\n",
        "            \"cumulative_accuracy\": log.cumulative_accuracy,\n",
        "            \"correct\": log.correct,\n",
        "        }\n",
        "        print(f\"  {n_arch} archetypes: {log.final_accuracy:.2%}\")\n",
        "\n",
        "asyncio.run(run_ablation())\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COVERAGE EFFICIENCY SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'# Archetypes':<15} {'Final Accuracy':<15} {'Δ vs 0':<15}\")\n",
        "print(\"-\" * 45)\n",
        "baseline_acc = ablation_results[0][\"final_accuracy\"]\n",
        "for n_arch in ARCHETYPE_COUNTS:\n",
        "    acc = ablation_results[n_arch][\"final_accuracy\"]\n",
        "    delta = acc - baseline_acc\n",
        "    print(f\"{n_arch:<15} {acc:<15.2%} {delta:+.1%}\")\n",
        "\n",
        "# Find diminishing returns point\n",
        "best_n = max(ARCHETYPE_COUNTS, key=lambda n: ablation_results[n][\"final_accuracy\"])\n",
        "print(f\"\\nOptimal archetype count: {best_n}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePRDYzuf4SXU"
      },
      "source": [
        "## 9. Analysis & Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsR2b3sn4SXU"
      },
      "outputs": [],
      "source": [
        "# --- Plot 1: Accuracy Curves ---\n",
        "\n",
        "def calculate_mean_confidence_over_time(log: CurriculumRunLog) -> List[float]:\n",
        "    \"\"\"Calculate mean bullet confidence at each step.\"\"\"\n",
        "    precisions = []\n",
        "    for conf_snapshot in log.confidence_history:\n",
        "        if conf_snapshot:\n",
        "            precisions.append(np.mean(conf_snapshot))\n",
        "        else:\n",
        "            precisions.append(0.5)\n",
        "    return precisions if precisions else [0.5] * len(log.correct)\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# Plot 1: Cumulative accuracy over time\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(archetype_log.cumulative_accuracy, label='Archetype-first', color='blue', linewidth=2)\n",
        "ax1.plot(random_log.cumulative_accuracy, label='Random', color='red', linewidth=2, linestyle='--')\n",
        "ax1.axvline(x=len(archetypes), color='green', linestyle=':', label='End of archetype phase')\n",
        "ax1.set_xlabel('Problems Solved')\n",
        "ax1.set_ylabel('Cumulative Accuracy')\n",
        "ax1.set_title('Learning Curves: Archetype-first vs Random')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_ylim(0, 1)\n",
        "\n",
        "# Plot 2: Strategy Confidence Over Time\n",
        "ax2 = axes[0, 1]\n",
        "arch_precision = calculate_mean_confidence_over_time(archetype_log)\n",
        "rand_precision = calculate_mean_confidence_over_time(random_log)\n",
        "ax2.plot(arch_precision, label='Archetype-first', color='blue', linewidth=2)\n",
        "ax2.plot(rand_precision, label='Random', color='red', linewidth=2, linestyle='--')\n",
        "ax2.set_xlabel('Problems Solved')\n",
        "ax2.set_ylabel('Mean Strategy Confidence')\n",
        "ax2.set_title('Strategy Confidence Over Time')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Coverage Efficiency (Ablation)\n",
        "ax3 = axes[0, 2]\n",
        "if ablation_results:\n",
        "    n_archs = list(ablation_results.keys())\n",
        "    accuracies = [ablation_results[n][\"final_accuracy\"] for n in n_archs]\n",
        "    ax3.plot(n_archs, accuracies, 'bo-', linewidth=2, markersize=8)\n",
        "    ax3.axhline(y=ablation_results[0][\"final_accuracy\"], color='red', linestyle='--',\n",
        "                label=f'Random baseline: {ablation_results[0][\"final_accuracy\"]:.1%}')\n",
        "    ax3.set_xlabel('Number of Archetypes')\n",
        "    ax3.set_ylabel('Final Accuracy')\n",
        "    ax3.set_title('Coverage Efficiency: How Many Archetypes Needed?')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    ax3.set_xticks(n_archs)\n",
        "\n",
        "# Plot 4: Final Confidence Distribution\n",
        "ax4 = axes[1, 0]\n",
        "if archetype_log.final_playbook and random_log.final_playbook:\n",
        "    arch_confs = [b.confidence for b in archetype_log.final_playbook.bullets]\n",
        "    rand_confs = [b.confidence for b in random_log.final_playbook.bullets]\n",
        "    ax4.hist(arch_confs, bins=10, alpha=0.6, label='Archetype-first', color='blue')\n",
        "    ax4.hist(rand_confs, bins=10, alpha=0.6, label='Random', color='red')\n",
        "    ax4.set_xlabel('Strategy Confidence')\n",
        "    ax4.set_ylabel('Count')\n",
        "    ax4.set_title('Final Strategy Confidence Distribution')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 5: UCB Gap (Exploration vs Exploitation)\n",
        "ax5 = axes[1, 1]\n",
        "if archetype_log.ucb_gaps:\n",
        "    ax5.plot(archetype_log.ucb_gaps, color='blue', linewidth=1, alpha=0.7)\n",
        "    mean_gap = np.mean(archetype_log.ucb_gaps)\n",
        "    ax5.axhline(y=mean_gap, color='blue', linestyle='--', label=f'Mean: {mean_gap:.3f}')\n",
        "    ax5.set_xlabel('Problem Index (Phase 2)')\n",
        "    ax5.set_ylabel('UCB Gap')\n",
        "    ax5.set_title('LinUCB Exploration-Exploitation Gap')\n",
        "    ax5.legend()\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 6: Ablation Learning Curves\n",
        "ax6 = axes[1, 2]\n",
        "if ablation_results:\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, len(ARCHETYPE_COUNTS)))\n",
        "    for n_arch, color in zip(ARCHETYPE_COUNTS, colors):\n",
        "        cum_acc = ablation_results[n_arch][\"cumulative_accuracy\"]\n",
        "        ax6.plot(cum_acc, label=f'{n_arch} archetypes', color=color, linewidth=1.5, alpha=0.8)\n",
        "    ax6.set_xlabel('Problems Solved')\n",
        "    ax6.set_ylabel('Cumulative Accuracy')\n",
        "    ax6.set_title('Learning Curves by Archetype Count')\n",
        "    ax6.legend(fontsize=8)\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "    ax6.set_ylim(0, 1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('archetype_discovery_results.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Plots saved to archetype_discovery_results.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci5oA16p4SXU"
      },
      "source": [
        "## 10. Results Summary & Statistical Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihKXU9ds4SXU"
      },
      "outputs": [],
      "source": [
        "# --- Bootstrap Confidence Intervals ---\n",
        "\n",
        "def bootstrap_ci(\n",
        "    data: List[bool],\n",
        "    n_bootstrap: int = 1000,\n",
        "    ci: float = 0.95,\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Calculate bootstrap confidence interval for accuracy.\n",
        "\n",
        "    Returns: (mean, lower_bound, upper_bound)\n",
        "    \"\"\"\n",
        "    data_arr = np.array(data, dtype=float)\n",
        "    n = len(data_arr)\n",
        "\n",
        "    rng = np.random.RandomState(SEED)\n",
        "    bootstrap_means = [\n",
        "        np.mean(rng.choice(data_arr, size=n, replace=True))\n",
        "        for _ in range(n_bootstrap)\n",
        "    ]\n",
        "\n",
        "    alpha = 1 - ci\n",
        "    lower = np.percentile(bootstrap_means, alpha / 2 * 100)\n",
        "    upper = np.percentile(bootstrap_means, (1 - alpha / 2) * 100)\n",
        "    mean = np.mean(data_arr)\n",
        "\n",
        "    return mean, lower, upper\n",
        "\n",
        "\n",
        "def paired_comparison(arch_correct: List[bool], rand_correct: List[bool]) -> Dict:\n",
        "    \"\"\"\n",
        "    Compare two conditions using Wilcoxon signed-rank test.\n",
        "    Uses sliding window accuracy for paired comparison.\n",
        "    \"\"\"\n",
        "    window = 10\n",
        "\n",
        "    def windowed_accuracy(correct_list):\n",
        "        return [\n",
        "            np.mean(correct_list[max(0, i - window):i + 1])\n",
        "            for i in range(len(correct_list))\n",
        "        ]\n",
        "\n",
        "    arch_windows = windowed_accuracy(arch_correct)\n",
        "    rand_windows = windowed_accuracy(rand_correct)\n",
        "\n",
        "    # Truncate to same length\n",
        "    min_len = min(len(arch_windows), len(rand_windows))\n",
        "    arch_windows = arch_windows[:min_len]\n",
        "    rand_windows = rand_windows[:min_len]\n",
        "\n",
        "    # Wilcoxon test\n",
        "    try:\n",
        "        stat, p_value = stats.wilcoxon(arch_windows, rand_windows, alternative='greater')\n",
        "    except ValueError:\n",
        "        # All differences are zero\n",
        "        stat, p_value = 0.0, 1.0\n",
        "\n",
        "    return {\n",
        "        \"statistic\": stat,\n",
        "        \"p_value\": p_value,\n",
        "        \"significant\": p_value < 0.05,\n",
        "    }\n",
        "\n",
        "\n",
        "def cohens_h(p1: float, p2: float) -> float:\n",
        "    \"\"\"Cohen's h effect size for comparing two proportions.\"\"\"\n",
        "    return 2 * (np.arcsin(np.sqrt(p1)) - np.arcsin(np.sqrt(p2)))\n",
        "\n",
        "\n",
        "# --- Results Summary ---\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STATISTICAL ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Bootstrap CIs\n",
        "arch_mean, arch_lower, arch_upper = bootstrap_ci(archetype_log.correct)\n",
        "rand_mean, rand_lower, rand_upper = bootstrap_ci(random_log.correct)\n",
        "\n",
        "print(f\"\\nArchetype-first Accuracy: {arch_mean:.2%} (95% CI: [{arch_lower:.2%}, {arch_upper:.2%}])\")\n",
        "print(f\"Random Baseline Accuracy: {rand_mean:.2%} (95% CI: [{rand_lower:.2%}, {rand_upper:.2%}])\")\n",
        "\n",
        "# Wilcoxon test\n",
        "wilcoxon_result = paired_comparison(archetype_log.correct, random_log.correct)\n",
        "print(f\"\\nWilcoxon Signed-Rank Test:\")\n",
        "print(f\"  Statistic: {wilcoxon_result['statistic']:.2f}\")\n",
        "print(f\"  P-value: {wilcoxon_result['p_value']:.4f}\")\n",
        "print(f\"  Significant (p < 0.05): {wilcoxon_result['significant']}\")\n",
        "\n",
        "# Effect size\n",
        "effect = cohens_h(arch_mean, rand_mean)\n",
        "print(f\"\\nEffect Size (Cohen's h): {effect:.3f}\")\n",
        "if abs(effect) < 0.2:\n",
        "    effect_interpretation = \"Small effect\"\n",
        "elif abs(effect) < 0.5:\n",
        "    effect_interpretation = \"Medium effect\"\n",
        "else:\n",
        "    effect_interpretation = \"Large effect\"\n",
        "print(f\"  Interpretation: {effect_interpretation}\")\n",
        "\n",
        "# Coverage efficiency\n",
        "def find_threshold_index(acc_list: List[float], threshold: float) -> Optional[int]:\n",
        "    \"\"\"Find first index where accuracy reaches threshold.\"\"\"\n",
        "    for i, a in enumerate(acc_list):\n",
        "        if a >= threshold:\n",
        "            return i\n",
        "    return None\n",
        "\n",
        "arch_50_idx = find_threshold_index(archetype_log.cumulative_accuracy, 0.5)\n",
        "rand_50_idx = find_threshold_index(random_log.cumulative_accuracy, 0.5)\n",
        "\n",
        "print(f\"\\nCoverage Efficiency:\")\n",
        "print(f\"  Archetype-first reached 50% acc after: {arch_50_idx if arch_50_idx is not None else 'N/A'} problems\")\n",
        "print(f\"  Random reached 50% acc after: {rand_50_idx if rand_50_idx is not None else 'N/A'} problems\")\n",
        "\n",
        "# Strategy churn\n",
        "print(f\"\\nFinal Playbook Stats:\")\n",
        "if archetype_log.final_playbook:\n",
        "    total_tags = sum(b.helpful_count + b.harmful_count for b in archetype_log.final_playbook.bullets)\n",
        "    print(f\"  Archetype-first: {archetype_log.final_playbook.size} strategies, {total_tags} total tags\")\n",
        "if random_log.final_playbook:\n",
        "    total_tags = sum(b.helpful_count + b.harmful_count for b in random_log.final_playbook.bullets)\n",
        "    print(f\"  Random: {random_log.final_playbook.size} strategies, {total_tags} total tags\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uU_kr8Wr4SXU"
      },
      "outputs": [],
      "source": [
        "# --- Final Summary ---\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EXPERIMENT SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Determine comparison outcomes\n",
        "outperforms = \"outperforms\" if arch_mean > rand_mean else \"underperforms compared to\"\n",
        "transfer_quality = \"reliable\" if arch_mean > 0.5 else \"limited\"\n",
        "ucb_effectiveness = \"effectively\" if archetype_log.ucb_gaps and np.mean(archetype_log.ucb_gaps) > 0.1 else \"minimally\"\n",
        "significance_note = \"(significant)\" if wilcoxon_result['significant'] else \"(not significant)\"\n",
        "\n",
        "total_llm_calls = sum(archetype_log.call_counts.values()) + sum(random_log.call_counts.values())\n",
        "verification_calls = call_counter.get('verify', 0) + call_counter.get('perturb', 0) + call_counter.get('adversarial', 0)\n",
        "\n",
        "print(f\"\"\"\n",
        "Verified Archetype Discovery PoC Results\n",
        "-----------------------------------------\n",
        "\n",
        "Task: Game of 24 (mathematical reasoning)\n",
        "Model: Qwen2.5-7B-Instruct via vLLM\n",
        "Problems: {N_EVAL_PROBLEMS} total ({len(archetypes)} archetypes + {N_EVAL_PROBLEMS - len(archetypes)} others)\n",
        "\n",
        "Archetype Selection:\n",
        "- Clustered {len(problems)} problems into {N_CLUSTERS} clusters\n",
        "- Selected top-10 archetypes by: centrality * diversity * simplicity\n",
        "- Verified strategies with multi-path consistency, perturbation testing, adversarial probing\n",
        "\n",
        "Results:\n",
        "- Archetype-first accuracy: {archetype_log.final_accuracy:.2%}\n",
        "- Random baseline accuracy: {random_log.final_accuracy:.2%}\n",
        "- Improvement: {(archetype_log.final_accuracy - random_log.final_accuracy)*100:+.1f} percentage points\n",
        "- Statistical significance: p = {wilcoxon_result['p_value']:.4f} {significance_note}\n",
        "\n",
        "Key Findings:\n",
        "1. Archetype-first curriculum {outperforms} random baseline\n",
        "2. Verified strategies from archetypes provide {transfer_quality} transfer\n",
        "3. LinUCB contextual bandit {ucb_effectiveness} balances exploration/exploitation\n",
        "\n",
        "LLM Budget:\n",
        "- Total calls: ~{total_llm_calls}\n",
        "- Verification calls: ~{verification_calls}\n",
        "\"\"\")\n",
        "\n",
        "# Save results\n",
        "results = {\n",
        "    \"archetype_log\": {\n",
        "        \"correct\": archetype_log.correct,\n",
        "        \"final_accuracy\": archetype_log.final_accuracy,\n",
        "        \"cumulative_accuracy\": archetype_log.cumulative_accuracy,\n",
        "    },\n",
        "    \"random_log\": {\n",
        "        \"correct\": random_log.correct,\n",
        "        \"final_accuracy\": random_log.final_accuracy,\n",
        "        \"cumulative_accuracy\": random_log.cumulative_accuracy,\n",
        "    },\n",
        "    \"statistics\": {\n",
        "        \"arch_ci\": (arch_mean, arch_lower, arch_upper),\n",
        "        \"rand_ci\": (rand_mean, rand_lower, rand_upper),\n",
        "        \"wilcoxon\": wilcoxon_result,\n",
        "        \"effect_size\": effect,\n",
        "    },\n",
        "    \"archetypes\": [a[\"numbers\"] for a in archetypes],\n",
        "}\n",
        "\n",
        "with open(CHECKPOINT_DIR / \"final_results.pkl\", \"wb\") as f:\n",
        "    pickle.dump(results, f)\n",
        "\n",
        "print(f\"Results saved to {CHECKPOINT_DIR / 'final_results.pkl'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEtK-YI04SXU"
      },
      "outputs": [],
      "source": [
        "# --- Cleanup ---\n",
        "\n",
        "print(\"Shutting down vLLM server...\")\n",
        "if vllm_proc:\n",
        "    vllm_proc.terminate()\n",
        "    try:\n",
        "        vllm_proc.wait(timeout=10)\n",
        "    except subprocess.TimeoutExpired:\n",
        "        vllm_proc.kill()\n",
        "    print(f\"vLLM server (PID {vllm_proc.pid}) terminated.\")\n",
        "\n",
        "print(\"\\nNotebook complete.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}